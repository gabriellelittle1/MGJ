{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from portia.cli import CLIExecutionHooks\n",
    "from portia import *\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Generic, TypeVar, List, Literal\n",
    "import os\n",
    "from notion_client import Client\n",
    "from my_custom_tools.registry import custom_tool_registry\n",
    "import shutil\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Fetch the Notion API key and set up client\n",
    "notion_api_key = os.getenv(\"NOTION_API_KEY\")\n",
    "notion_parent_id = os.getenv(\"NOTION_PARENT_ID\")\n",
    "\n",
    "# Initialize the Notion client\n",
    "notion = Client(auth=notion_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = Config.from_default()\n",
    "complete_tool_registry = PortiaToolRegistry(my_config) + custom_tool_registry\n",
    "\n",
    "portia = Portia(config = my_config,\n",
    "                tools = complete_tool_registry,\n",
    "                execution_hooks=CLIExecutionHooks(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 08:40:22.242\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;39mportia.portia\u001b[0m:\u001b[38;5;39mplan\u001b[0m:\u001b[38;5;39m197\u001b[0m - \u001b[1mRunning planning_agent for query - You are a research assistant running these tasks: \n",
      "                      - Find and download 1 paper on the topic of Protein Modelling using the ArXivTool. \n",
      "                      - Run the PDFReaderTool to extract the full text from the pdfs in the local folder.\n",
      "                      - Use PSTool to create and populate the Page Summary subpage.\n",
      "                      - From the text, extract the core mathematical and scientific concepts required \n",
      "                        to understand the paper. Focus only on generalizable topics that could be included \n",
      "                        in a learning pathway or curriculum—avoid content specific to the study's location, \n",
      "                        data, or outcomes. List only the overarching topics, with no explanations or extra text.\n",
      "                      - Then use the TopicSelectorTool on these topics. \n",
      "                      - Then use the Notion Tool to create Notion pages for these topics.\n",
      "\n",
      "                      - Use the YouTubeTool to find videos on each topic.\n",
      "                      - Use the RecReadTool to find resources on each topic.\n",
      "                      - Use the QuizTool to create quizzes on each topic.\n",
      "\n",
      "\n",
      "                        Take into account these constraints: []\n",
      "                      \u001b[0m\n",
      "\u001b[32m2025-04-13 08:40:37.524\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;39mportia.portia\u001b[0m:\u001b[38;5;39mplan\u001b[0m:\u001b[38;5;39m222\u001b[0m - \u001b[1mPlan created with 10 steps\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee'}\n",
      "\n",
      "Here are the steps in the generated plan:\n",
      "{\n",
      "  \"task\": \"Use the Arxiv tool to retrieve the most relevant paper on the topic 'Protein Modelling'. The query is provided as 'Protein Modelling'.\",\n",
      "  \"inputs\": [],\n",
      "  \"tool_id\": \"arxiv_tool\",\n",
      "  \"output\": \"$paper_info\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Download the paper provided in the previous step by creating a list with the paper information and passing it to the download function.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$paper_info\",\n",
      "      \"description\": \"Information about the paper on Protein Modelling from the Arxiv tool.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"download_tool\",\n",
      "  \"output\": \"$downloaded_papers\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Run the PDF Reader tool to extract the full text from all PDFs in the local 'papers' folder.\",\n",
      "  \"inputs\": [],\n",
      "  \"tool_id\": \"pdf_reader_tool\",\n",
      "  \"output\": \"$pdf_texts\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Create and populate the Page Summary subpage by using the previously downloaded paper information and the extracted full text from the PDFs.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$paper_info\",\n",
      "      \"description\": \"Paper information for creating the summary.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"$pdf_texts\",\n",
      "      \"description\": \"The full text extracted from the downloaded PDFs.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"pstool\",\n",
      "  \"output\": \"$page_summary\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Extract the core mathematical and scientific concepts from the full text output to determine the generalizable topics for a learning pathway. The extraction should focus solely on overarching mathematical and scientific topics with no additional explanation or study-specific details.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$pdf_texts\",\n",
      "      \"description\": \"The full text of the paper from which topics should be extracted.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"llm_tool\",\n",
      "  \"output\": \"$extracted_topics\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Use the Topic Selector tool by providing the extracted topics as the raw topics list to allow the selection of topics.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$extracted_topics\",\n",
      "      \"description\": \"List of raw topics extracted from the paper text.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"topic_selector_tool\",\n",
      "  \"output\": \"$selected_topics\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Create Notion pages for each of the selected topics by passing the topics list to the creation process.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$selected_topics\",\n",
      "      \"description\": \"The list of topics selected by the Topic Selector tool.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"notion_tool\",\n",
      "  \"output\": \"$notion_pages\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Find relevant YouTube videos for the selected topics by supplying the topics list to the video search function.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$selected_topics\",\n",
      "      \"description\": \"The list of selected topics for which to find related YouTube videos.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"youtube_tool\",\n",
      "  \"output\": \"$youtube_videos\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Find recommended reading resources for each selected topic by using the reading resources tool with the topics list as input.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$selected_topics\",\n",
      "      \"description\": \"The selected topics for which recommended readings should be found.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"RecRead_tool\",\n",
      "  \"output\": \"$reading_resources\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Create quizzes on each of the selected topics by passing the topics list to the quiz creation function.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$selected_topics\",\n",
      "      \"description\": \"The list of topics on which quizzes will be created.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"Quiz_tool\",\n",
      "  \"output\": \"$quizzes\",\n",
      "  \"condition\": null\n",
      "}\n",
      "\n",
      "The plan will now be executed. Please wait...\n",
      "\u001b[32m2025-04-13 08:40:44.179\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m528\u001b[0m - \u001b[1mPlan Run State is updated to PlanRunState.IN_PROGRESS. View in your Portia AI dashboard: https://app.portialabs.ai/dashboard/plan-runs?plan_run_id=prun-a587cefc-258f-40f9-8b45-8d66356c24e2\u001b[0m\n",
      "\u001b[32m2025-04-13 08:40:44.180\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 0: Use the Arxiv tool to retrieve the most relevant paper on the topic 'Protein Modelling'. The query is provided as 'Protein Modelling'.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:40:46.745\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking arXiv Tool with args: {'topic': 'Protein Modelling'}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:40:50.445\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [{\"title\": \"Develop machine learning based predictive models for engineering protein\\n  solubility\", \"summary\": \"Protein activity is a significant characteristic for recombinant proteins\\nwhich can be used as biocatalysts. High activity of proteins reduces the cost\\nof biocatalysts. A model that can predict protein activity from amino acid\\nsequence is highly desired, as it aids experimental improvement of proteins.\\nHowever, only limited data for protein activity are currently available, which\\nprevents the development of such models. Since protein activity and solubility\\nare correlated for some proteins, the publicly available solubility dataset may\\nbe adopted to develop models that can predict protein solubility from sequence.\\nThe models could serve as a tool to indirectly predict protein activity from\\nsequence. In literature, predicting protein solubility from sequence has been\\nintensively explored, but the predicted solubility represented in binary values\\nfrom all the developed models was not suitable for guiding experimental designs\\nto improve protein solubility. Here we propose new machine learning models for\\nimproving protein solubility in vivo. We first implemented a novel approach\\nthat predicted protein solubility in continuous numerical values instead of\\nbinary ones. After combing it with various machine learning algorithms, we\\nachieved a prediction accuracy of 76.28 percent when Support Vector Machine\\nalgorithm was used. Continuous values of solubility are more meaningful in\\nprotein engineering, as they enable researchers to choose proteins with higher\\npredicted solubility for experimental validation, while binary values fail to\\ndistinguish proteins with the same value. There are only two possible values so\\nmany proteins have the same one.\", \"link\": \"https://arxiv.org/pdf/1806.11369v2.pdf\"}]\u001b[0m\n",
      "\u001b[32m2025-04-13 08:40:51.860\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 1: Download the paper provided in the previous step by creating a list with the paper information and passing it to the download function.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:41:02.305\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Download Tool with args: {'papers': [{'title': 'Develop machine learning based predictive models for engineering protein\\n  solubility', 'link': 'https://arxiv.org/pdf/1806.11369v2.pdf', 'summary': 'Protein activity is a significant characteristic for recombinant proteins\\nwhich can be used as biocatalysts. High activity of proteins reduces the cost\\nof biocatalysts. A model that can predict protein activity from amino acid\\nsequence is highly desired, as it aids experimental improvement of proteins.\\nHowever, only limited data for protein activity are currently available, which\\nprevents the development of such models. Since protein activity and solubility\\nare correlated for some proteins, the publicly available solubility dataset may\\nbe adopted to develop models that can predict protein solubility from sequence.\\nThe models could serve as a tool to indirectly predict protein activity from\\nsequence. In literature, predicting protein solubility from sequence has been\\nintensively explored, but the predicted solubility represented in binary values\\nfrom all the developed models was not suitable for guiding experimental designs\\nto improve protein solubility. Here we propose new machine learning models for\\nimproving protein solubility in vivo. We first implemented a novel approach\\nthat predicted protein solubility in continuous numerical values instead of\\nbinary ones. After combing it with various machine learning algorithms, we\\nachieved a prediction accuracy of 76.28 percent when Support Vector Machine\\nalgorithm was used. Continuous values of solubility are more meaningful in\\nprotein engineering, as they enable researchers to choose proteins with higher\\npredicted solubility for experimental validation, while binary values fail to\\ndistinguish proteins with the same value. There are only two possible values so\\nmany proteins have the same one.'}]}\u001b[0m\n",
      "ℹ️ Downloading 'Develop machine learning based predictive models for engineering protein\n",
      "  solubility' from https://arxiv.org/pdf/1806.11369v2.pdf\n",
      "\u001b[32m2025-04-13 08:41:05.684\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - ✅ Downloaded 1 paper into the 'papers' folder\u001b[0m\n",
      "\u001b[32m2025-04-13 08:41:07.118\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 2: Run the PDF Reader tool to extract the full text from all PDFs in the local 'papers' folder.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:41:09.763\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking PDF reader tool with args: {}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:41:12.758\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - {\"Develop_machine_learning_.pdf\": \"--- Page 1 ---\\nDevelop machine learning based predictive \\nmodels for engineering protein solubility  \\nXi Han1, Xiaonan Wang1,*, Kang Zhou1,* \\n1Department of Chemical and Biomolecular Engineering, National University of Singapore, Singapore, \\n117585.  \\n*To whom correspondence should be addressed. \\nAbstract \\nMotivation: Protein activity is a significant characteristic for recombinant proteins which can be used \\nas biocatalysts. High activity of proteins reduces the cost of biocatalysts. A model that can predict \\nprotein activity from amino acid sequence is highly desired, as it aids experimental improvement of \\nproteins. However, only limited data for protein activity are currently available, which prevents the de-\\nvelopment of such models. Since protein activity and solubility are correlated for some proteins, the \\npublicly available solubility dataset may be adopted to develop models that can predict protein solubility \\nfrom sequence. The models could serve as a tool to indirectly predict protein activity from sequence. \\nIn literature, predicting protein solubility from sequence has been intensively explored, but the predicted \\nsolubility represented in binary values from all the developed models was not suitable for guiding ex-\\nperimental designs to improve protein solubility. Here we propose new machine learning models for \\nimproving protein solubility in vivo.       \\nResults: We first implemented a novel approach that predicted protein solubility in continuous numer-\\nical values instead of binary ones. After combing it with various machine learning algorithms, we \\nachieved a prediction accuracy of 76.28% when Support Vector Machine (SVM) algorithm was used. \\nContinuous values of solubility are more meaningful in protein engineering, as they enable researchers \\nto choose proteins with higher predicted solubility for experimental validation, while binary values fail \\nto distinguish proteins with the same value – there are only two possible values so many proteins have \\nthe same one. \\nAvailability: We present the machine learning workflow as a series of IPython notebooks hosted on \\nGitHub (https://github.com/xiaomizhou616/protein_solubility). The workflow can be used as a template \\nfor analysis of other expression and solubility datasets. \\nContact: kang.zhou@nus.edu.sg, chewxia@nus.edu.sg  \\nSupplementary information: Supplementary data are available on GitHub (https://github.com/xiaomi-\\nzhou616/protein_solubility). \\n \\n \\n1 \\nIntroduction  \\nEscherichia coli is a bacterium commonly used in genetic engineering to \\nexpress recombinant proteins (Chan, et al., 2010), which is a pivotal pro-\\ncess in biotechnology. Producing some proteins in E. coli was, however, \\ninefficient in industrial setting, resulting from low activity of the recom-\\nbinant proteins. As biocatalysts and therapeutic agents, proteins with high \\nactivity have lower unit production cost. Developing biocatalysts with \\nhigh activity has thus become an important goal for further development \\nof many biocatalytic processes.  \\nSome experimental strategies in vivo can improve the expression of re-\\ncombinant proteins, such as using suitable promoters, optimizing codon \\nusage, or changing culture media, temperature and/or other culture condi-\\ntions (Idicula‐Thomas and Balaji, 2005; Magnan, et al., 2009). Such em-\\npirical optimizations have, however, been time-consuming and expensive. \\nMoreover, experiments often fail due to opaque reasons. A generic solu-\\ntion is highly desired for enhancing the heterologous protein overexpres-\\nsion and may be ultimately provided by using a computational model that \\ncan predict activity of any enzyme accurately from its amino acid se-\\nquence and other input information. Developing such model would require \\na large dataset which contains at least two columns, protein sequence and \\nactivity. No such dataset is currently available, because conventional pro-\\ntein engineering was driven by chasing high activity, which has provided \\nvery little information on protein sequence. For instance, most proteins \\nwith low or intermediate activity values were discarded without their se-\\nquence being revealed. It is currently prohibitively expensive to generate \\na complete, large dataset for protein activity, due to technical limitations. \\nA compromised solution is to predict protein activity by using solubility \\nof the protein as a proxy, because (1) activity and solubility are correlated \\nfor some proteins, and (2) there is a relatively large dataset available for \\nsolubility as solubility data from different proteins can be pooled together. \\nUntil now, a number of machine learning predictors have been developed \\nto address the interconnection between protein solubility and amino acid \\nsequence. The prediction for protein solubility from amino acid sequence \\nwas first proposed by Wilkinson and Harrison (Wilkinson and Harrison,\\n\\n--- Page 2 ---\\nX. Han et al. \\n1991). A simple regression method was used by them to achieve high ac-\\ncuracy (0.88) by using several features of 81 protein sequences, such as \\nhydrophilicity, molecular weight and averaged surface charge. The accu-\\nracy was improved by logistic regression to 0.94 based on a small database \\nwith 212 proteins (Diaz, et al., 2010). SVM is a core machine learning \\n(ML) classification method that has achieved competitive performance in \\nthis field. With SVM, Idicula-Thomas analysed 192 proteins with an un-\\nbalanced correlation score filter, which achieved an accuracy of 0.74 by \\nusing physicochemical properties, mono-peptide frequencies, dipeptide \\nfrequencies and reduced alphabet set as features (Idicula-Thomas, et al., \\n2005). These datasets are, however, not large enough to generate correla-\\ntions that can be widely applied (Idicula-Thomas and Balaji, 2005). Mo-\\nlecular weight, isometric point and amino acid composition were used as \\ninputs to build a SVM model by Niwa et al. (Niwa, et al., 2009), who for \\nthe first time generated and studied a dataset that contained more than \\n1000 entries. Prediction of protein solubility was subsequently conducted \\nwith SVM based on databases with 2159 proteins (Agostini, et al., 2012) \\nand 5692 proteins (Xiaohui, et al., 2014). Besides, decision tree \\n(Christendat, et al., 2000; Goh, et al., 2004; Rumelhart, et al., 1985), Naïve \\nBayes (Smialowski, et al., 2006), random forest (Fang and Fang, 2013; \\nHirose, et al., 2011), gradient boosting machine (Rawi, et al., 2017) and \\nsome computational methods combining multiple machine learning mod-\\nels have been utilized to predict if a protein is soluble or not by using their \\nprimary sequence. In addition, several software and web servers have been \\ndeveloped for protein solubility prediction, including ESPRESSO (Hirose \\nand Noguchi, 2013), Pros (Hirose and Noguchi, 2013), SCM (Huang, et \\nal., 2012), PROSOII (Smialowski, et al., 2012), SOLpro (Magnan, et al., \\n2009) and PROSO (Smialowski, et al., 2006). . Some of the predictors \\navailable online are introduced below. ESPRESSO selects features by us-\\ning Student’s t-test filter and trains a model by SVM and another sequence \\npattern recognition based method. Pros implements random forest with \\nStudent’s t-test as filter and random forest as wrapper. PROSOII is a two-\\nlayer model using a Parzen window in the first layer and logistic regres-\\nsion in both layers. \\nVarious databases were utilized in the previous studies, whereas eSol \\ndatabase (Niwa, et al., 2009) is a unique one, which has a relatively large \\nsize and continuous values of solubility, ranging from 0 to 1. Comparison \\nof model performances using this database is summarized in Table 1. All \\nthe ML models developed based on this database, however, only predicted \\nwhether a protein was soluble or insoluble (outputting binary values), \\nwhich was not useful in guiding protein engineering, because even a sub-\\nstantial increase in solubility may not turn the status from ‘insoluble’ to \\n‘soluble’ and thus be falsefully viewed as useless by the model. Moreover, \\nit is challenging to select several proteins with highest solubility to con-\\nduct experiments because there are too many proteins with the same value \\n1 of solubility in a large dataset. \\nIn the present study, we built models that can predict protein solubility \\nin numerical values that span from 0 to 1, which would allow fair evalua-\\ntion of a proposed mutation to a protein even when the resulting improve-\\nment is small. We compared various model-building techniques which use \\nthis new output format, and found SVM performed the best, achieving an \\naccuracy of 76.28%. \\n2 \\nMethods \\n2.1 Protein database \\nAll the proteins used in the study were downloaded from eSol database \\n(Niwa, et al., 2009). A brief summary of how solubility values in eSol \\ndatabase were obtained is provided here. Proteins were produced in vitro \\nby using cell-free protein expression technology and plasmids carrying \\nOpen Reading Frame (ORF) from complete E. coli ORF library (ASKA \\nlibrary) (Kitagawa, et al., 2005). Synthesized proteins were fractionated \\ninto soluble and insoluble fractions by using centrifugation, and the pro-\\nteins in both fractions were quantified by using SDS-PAGE. Solubility \\nwas defined as the ratio of protein quantity of the supernatant to the total \\nprotein quantity. We used all the entries from the database, unless the en-\\ntries contain no sequence information, or poorly determined sequence \\n(containing N instead of A, T, C, or G, or multiple stop-codons). We ex-\\ncluded 26 entries in total, and the size of the dataset used was 3147. \\n2.2 Features \\nDifferent features were extracted from sequences of proteins by using \\nprotr package (Xiao, et al., 2014) within R software, which generates var-\\nious numerical representation schemes. In our study, the description of \\nfeatures is listed in Table 2 according to different descriptors in protr pack-\\nage.  \\n2.3 Training flowsheet \\nData pre-processing is a very important part in data mining, which trans-\\nfers raw data into a well-organized dataset that can be taken as inputs in \\nmodel training. Only sequence and protein solubility were retrieved from \\nthe original dataset. Protein sequence was then transformed into numeric \\nvalues (features) by using an algorithm from Table 2. Finally, data nor-\\nmalization was conducted by making numerical values of each column in \\nthe range of 0-1. \\nTraining machine learning models for protein solubility prediction in-\\ncluded several steps (as shown in Fig. 3). Seventy five percent of the orig-\\ninal dataset (2364 proteins) were used in GANs model training and 90 \\npercent of the original dataset (2833 proteins) were used in other machine \\nlearning model training. The remaining dataset were validation data which \\ncan be used to evaluate the performance of each model. After model train-\\ning, all the evaluation metrics were calculated with 10-fold cross valida-\\ntion to compare the performance of different machine learning models. \\n2.4 Machine learning models \\nSeven supervised machine learning algorithms were applied to our da-\\ntaset: logistic regression, decision tree, SVM, Naïve Bayes, conditional \\nrandom forest (cforest), XGboost and artificial neural networks (ANNs). \\nFor binary values of solubility, classification algorithms were used and for \\ncontinuous values of solubility, regression algorithms were applied. \\n2.5 Tuning SVM \\nAmong the machine learning models used to predict the protein solubility, \\nSVM achieved the best performance, so SVM was chosen for further op-\\ntimization to achieve higher prediction accuracy. The parameters of SVM \\nwere tuned (Package e1071 in R software was used to implement SVM \\nmodel). Different kernels in SVM represent different functions. Another \\nthree parameters tuned were cost, gamma and epsilon. Cost is the regular-\\nization parameter that controls the trade-off between achieving a low train-\\ning error and a low testing error. Epsilon is a margin of tolerance where \\nno penalty is given to errors. Gamma indicates the threshold to determine \\nif two points are considered to be similar.\\n\\n--- Page 3 ---\\nDevelop machine learning based predictive models for engineering protein solubility \\nTable 1. Performance of published methods based on eSol database \\n \\n \\nMethod \\nSize \\nAccuracy \\nReference \\nRandom forest \\nTotal: 1918 \\nSoluble: 886 \\nInsoluble: 1032 \\n0.84 \\n(Fang and Fang, 2013) \\n1. Support vector machine \\n2. Random forest \\n3. Conditional inference trees \\n4. Rule ensemble \\nTotal: 1625 \\nSoluble: 843 \\nInsoluble: 782 \\n0.81a  \\n(Samak, et al., 2012) \\nDecision tree \\nSize: 1625 \\nSoluble: 843 \\nInsoluble: 782 \\n0.75 \\n(Stiglic, et al., 2012) \\nSupport vector machine \\nSize: 2159 \\nSoluble: 1081 \\nInsoluble: 1078 \\nN/A \\n(Agostini, et al., 2012) \\nSupport vector machine \\nSize: 3173 \\nSoluble: \\>0.7 \\nInsoluble: \\<0.3b \\n0.80 \\n(Niwa, et al., 2009) \\na Value estimated from the figure in the reference \\nb Proteins with solubility higher than 0.7 are labelled as soluble and lower than 0.3 are labelled as insoluble \\nTable 2. Description of different descriptors in protr package of R software \\nDescriptors \\nDescription \\nextractAAC \\ncomposition of each amino acid \\nextractPAAC \\nhydrophobicity, hydrophilicity and side chain mass \\nextractAPAAC \\nhydrophobicity, hydrophilicity and sequence order \\nextractMoreauBroto \\nnormalized average hydrophobicity scales, average flexibility indices, polarizability parameter, free energy of solution in \\nwater \\nextractMoran \\nnormalized average hydrophobicity scales, average flexibility indices, polarizability parameter, free energy of solution in \\nwater c \\nextractGeary \\nnormalized average hydrophobicity scales, average flexibility indices, polarizability parameter, free energy of solution in \\nwater c \\nextractCTDC \\nhydrophobicity, normalized van der Waals volume, polarity, polarizability, charge, second structure, solvent accessibility \\nextractCTDT \\npercentage of position of attributes in extractCTDC \\nextractCTDD  \\ndistribution of attributes in extractCTDT \\nextractCTriad  \\nprotein-protein interactions including electrostatic and hydrophobic interactions \\nextractSOCN  \\nSchneider-Wrede physicochemical distance matrix and Grantham chemical distance matrix \\nextractQSO  \\nSchneider-Wrede physicochemical distance matrix and Grantham chemical distance matrix d \\nc Different calculation methods are used for extractMoreauBroto, extractMoran and extractGeary using the same attributes listed above \\nd Different calculation methods are used for extractSOCN and extractQSO using the same attributes listed above\\n2.6 Data augmentation algorithms \\nGenerative Adversarial Networks (GANs) are an emerging Artificial In-\\ntelligence (AI) algorithm used for data augmentation in this work. There \\nare two main models within GANs, Generator Neural Network and Dis-\\ncriminator Neural Network. The generative model G takes random data z \\nfrom probability distribution p(z) as input. The discriminative model D \\ntakes data generated from generative model G and real data from training \\ndataset as inputs and distinguishes whether the data are artificial or real. \\nThe training process is an adversarial game with backpropagation between \\ntwo networks (Fig. 2).  \\nIn Equation (1), D(x) calculates the probability that x comes from the orig-\\ninal data. D’s objective is to maximize the probability of labelling both the \\nreal and artificial data correctly (termed as max(V)), whereas G’s objective \\nis to minimize max (V), which can be expressed as min(max(V)).  Overall, \\nit is a two-player game between D and G.  \\n \\nFor training GANs, discriminator is first trained for n epochs based on real \\ndata. Second, data artificially generated from generator is fed into discrim-\\ninator networks which gave a judgement if the data was artificial. Third, \\ngenerator networks were further trained with the feedbacks from discrim-\\ninator to improve its data generation. The training process was iterated \\nuntil the data generated from generator cannot be distinguished by the dis-\\ncriminator networks from the original dataset.  \\n \\n𝑚𝑖𝑛\\n%\\n𝑚𝑎𝑥\\n(\\n𝑉𝐷, 𝐺= 𝐸/~12343 5  [log 𝐷𝑥] +\\n 𝐸=~1\\> \\>   [log(1 −𝐷(𝐺𝑧))]                                                      (1)\\n\\n--- Page 4 ---\\nX. Han et al. \\n \\nFig. 1. Workflow of model training used in our study \\nWith the development of AI algorithms for data augmentation, other algo-\\nrithms based on GANs have also been developed, which enhanced the per-\\nformance of GANs to some extent. CGAN is the conditional version of \\nGANs, which generates mimic data with class labels (Mirza and Osindero, \\n2014). WGAN uses Wasserstein distance metric rather than Jensen-Shan-\\nnon distance metric used in GANs when cross-entropy is calculated \\n(Arjovsky, et al., 2017). The cross-entropy loss is a measurement of the \\nability of discriminator to distinguish real data and generated data. Was-\\nserstein distance measures the minimum cost to make the distribution of \\ngenerated data more similar to that of real data, with the critic function in \\nthe discriminator model. WCGAN is the conditional version of the Was-\\nserstein GANs (Gulrajani, et al., 2017). \\n \\n \\nFig. 2. The workflow of training GANs \\n2.7 Evaluation metrics \\nSeveral evaluation metrics were used to quantitatively evaluate the perfor-\\nmance of different machine learning models. For comparison with prior \\nwork, we transformed continuous predicted solubility values into binary \\nones. Accuracy was calculated based on the binary values. The definition \\nof TP (true positive), TN (true negative), FP (false positive) and FN (false \\nnegative) is illustrated in Table 3. How to use TP, TN, FP and FN to cal-\\nculate accuracy is shown in Equation (2). \\n \\nTable 3. The definition of TP, TN, FP and FN for binary values of solu-\\nbility \\n \\n \\nPredicted solubility from models \\nActual solu-\\nbility  \\nfrom exper-\\niments \\n \\n \\nSolubility=1 \\nSolubility=0 \\nSolubil-\\nity = 1 \\na (TP: true posi-\\ntive) \\nb (FN: false nega-\\ntive) \\nSolubil-\\nity = 0 \\nc (FP: false posi-\\ntive) \\nd (TN: true nega-\\ntive) \\n \\nWe also evaluated the model performance by using coefficient of deter-\\nmination (R2) calculated from predicted and actual solubility values. Com-\\nputing R2 can only be done by using continuous values our models gener-\\nated. \\nK-fold cross-validation is a technique to partition the original dataset into \\nk equal-size parts. And each part has similar proportion of samples from \\ndifferent classes. In the k subsamples, one subsample is taken as validation \\ndata to test the model and the other data is used as training data to train the \\nmodel. The accuracy and R2 are the average of all the cases. In our study, \\nwe used k-fold cross-validation, in which k was set as 10.  \\n3 \\nResults \\n3.1 Develop models that output continuous numerical values \\n \\nWe used continuous values of protein solubility to train machine learning \\nmodels, which we set to output continuous values instead of binary ones. \\nWe have trained seven distinct models and compared their performance \\nby using two different evaluation criteria (accuracy and R2, Table 4). For \\nthe purpose of calculating the accuracy, we converted the continuous val-\\nues of predicted solubility into binary ones by using 0.44 (median value \\nof the solubility data of our database) as cut-off threshold because the data \\nabove and below median exhibit same sizes. SVM showed the best pre-\\ndiction performance among all the machine learning models (Table 4). \\nThe tuned SVM parameters were listed in Table S1 and Fig. S1 in Supple-\\nmentary Information. After plotting the correlation between the accuracy \\nvs. cost, epsilon, or gamma, optimized values were found (1.26, 0.029 and \\n0.057 for cost, gamma, or epsilon, respectively).  \\nFor comparison, we also trained the models by using the conventional ap-\\nproach, which converted continuous actual solubility values into binary \\nones BEFORE model training. Though using continuous values did not \\nalways improve model prediction performance, it improved SVM model \\nand made it to be the best one, in terms of both accuracy and R2 (Table 4).  \\n \\nAccuracy =\\n𝒂K𝒅\\n𝒂K𝒃K𝒄K𝒅                                                                   (2)\\n\\n--- Page 5 ---\\nDevelop machine learning based predictive models for engineering protein solubility \\nTable 4. Performance of different machine learning models  \\nModel \\nAccuracy \\nR2 \\nContinuous \\nvalues of solu-\\nbility \\nBinary \\nval-\\nues of solu-\\nbility \\nContinuous \\nvalues \\nof \\nsolubility \\nLogistic regression \\n0.6372 \\n0.6868 \\n0.2507 \\nDecision tree \\n0.6734 \\n0.6750 \\n0.2459 \\nSVM \\n0.7538 \\n0.7001 \\n0.4107 \\nNaïve Bayes \\n0.6674 \\n0.6979 \\n0.2376 \\ncforest \\n0.7055 \\n0.7123 \\n0.3764 \\nXGboost \\nANNs     \\n0.7052 \\n0.6632 \\n0.7103 \\n0.7086 \\n0.3997 \\n0.2029 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Effect of cut-off threshold on SVM prediction accuracy \\nCut-off points were used to transfer continuous values of solubility to bi-\\nnary values for model evaluation. How to choose the cut-off threshold \\nsubstantially affects accuracy that is used to gauge model performance. In \\nprevious studies, many arbitrarily set it at 0.5. Here we did a simple opti-\\nmization of the threshold with maximizing the accuracy as the objective. \\nWhen the cut-off threshold was too low (\\< 0.3) or too high (\\> 0.7), the \\naccuracy would be very high, but such accuracy was not useful because \\nalmost all the proteins would be classified either as soluble or insoluble \\nproteins. We varied the threshold from 0.3 to 0.7 with step size of 0.01 \\nand calculated the SVM model prediction accuracy. From Fig. 3, the high-\\nest accuracy (0.7628) was obtained when the cut-off point was 0.36 or \\n0.39. Therefore, 0.39 was randomly selected from those two values and \\nwas used in the following analysis. Although this optimization did not \\nsubstantially improve SVM model prediction accuracy, the results showed \\nthat the threshold can substantially change the accuracy (Fig. 3). Since it \\nis arbitrary to choose a cut-off threshold, accuracy based on the cut-off \\nthreshold is intrinsically subjective. With our new methods and the result-\\ning continuous model output values, we call for using the coefficient of \\ndetermination (R2) as the model evaluation metric, which does not require \\nany cut-off threshold. We used the R2 as the sole evaluation metric in the \\nrest of this study. \\n3.2 Attempt to improve model performance by optimizing \\ndata pre-processing \\n \\nTo further improve SVM model prediction performance, we evaluated all \\nthe available algorithms that convert amino acid sequence into numerical \\nvalues. This step is critical as it extracts a fraction of information (usually \\nonly a small fraction, termed as features) from the sequence data, and \\nfeeds it to the model-training algorithms. Since most information in raw \\ndata was discarded here, avoiding loss of critical information by using a \\nbetter algorithm may improve the model performance. It turned out that \\nthe best was the basic algorithm that simply converted sequence into \\namino acid composition, and that we used in prior sessions of this study \\n(Table 5).  \\n \\nTable 5. Performance of different sequence descriptors by SVM e \\n \\nDescriptors  \\nR2 \\nData dimension \\nextractAAC \\n0.4108 \\n20 \\nextractPAAC  \\n0.3564 \\n50 \\nextractAAC+extractPAAC \\n0.3719 \\n70 \\nextractAPAAC \\n0.3049 \\n80 \\nextractMoreauBroto \\n0.0806 \\n240 \\nextractMoran \\n0.0542 \\n240 \\nextractGeary \\n0.0562 \\n240 \\nextractCTDC \\n \\n0.2484f \\n21 \\nextractCTDT \\n21 \\nextractCTDD  \\n105 \\nextractCTriad  \\n0.0650 \\n343 \\nextractSOCN  \\n0.1647 \\n80 \\nextractQSO  \\n0.3224 \\n100 \\ne The description of each descriptor is shown in Table 2 \\nf The R2 is the result combining extractCTDC, extractCTDT and extractCTDD \\n4 \\nDiscussion  \\n4.1 Predict protein solubility with continuous values \\nIn this study, we developed a new model from amino acid sequence that \\ncan predict protein solubility as continuous values from 0 to 1, which \\nwould help improve solubility of recombinant proteins through protein en-\\ngineering. Having model output in such format also enables to use coeffi-\\ncient of determination (R2) as a metric to evaluate model performance, \\nwhich does not depend on a subjective threshold to determine if a protein \\nis soluble or insoluble. In literature, all the studies used such threshold(s) \\nto convert continuous values of protein solubility in eSol database into \\nbinary values. \\nWith continuous values of solubility, we will study how to use a developed \\nmodel to guide experiments in the future. First, we will use statistical tools \\nto determine the minimal performance of a model (in terms of R2) required \\nto conduct a meaningful in silico screening of protein mutants – the aver-\\nage quality of selected mutants should be substantially higher than that of \\nthe initial pool of mutants. The next step is to validate the model prediction \\nby selecting a number of proteins with high predicted solubility and meas-\\nuring their actual solubility through experiments. We plan to generate the \\npool of mutants for in silico screening by randomly introducing mutants \\nto a model protein, However, binding of substrate/co-factor and catalytic \\nactive site may be damaged by such random mutation, which would influ-\\nence the catalytic function. Therefore, mutation of residues responsible for \\nbinding substrates and the catalytic activity will be avoided. Then sensi-\\ntivity analysis will be conducted to determine how many mutations should\\n\\n--- Page 6 ---\\nX. Han et al. \\nbe made to improve protein solubility substantially. The model we devel-\\noped in this study laid the foundation for such future works, which cannot \\nbe done by using any existing models.  \\n4.2 Attempt to improve model performance by using data \\naugmentation \\nHaving adequate volume of data is a key to develop useful models by us-\\ning machine learning. In biotechnology field, generating and collecting \\nlarge amount of data is, however, time-consuming and expensive, result-\\ning in much smaller dataset than other fields such as facial recognition. \\nWe hypothesized that data augmentation algorithms might alleviate the \\nproblem that biotechnology applications do not have sufficient data for \\ndeveloping machine learning models. We attempted to use GANs to im-\\nprove prediction of our SVM model by generating artificial data.  \\nIn the first attempt, four versions of GANs were trained for 500 iterations \\nto generate the artificial data, which were used together with the original \\ndata to train ML models by using SVM. This attempt, however, failed to \\nimprove the model performance, as R2 was not increased (Supplementary \\nTable S2). Over iterations, the quality of data generated by GANs im-\\nproved (Supplementary Fig. S3), which showed GANs were implemented \\ncorrectly. By comparing generated data with original data (Supplementary \\nFig. S2) and the performance of four versions of GANs (Supplementary \\nTable S2), GANs have the best performance among the four versions of \\nGANs. Therefore, GANs were explored for more iterations.  \\nIn the second attempt, we increased the number of iterations from 500 to \\n5000. Three randomly selected datasets were tested to make the results \\nmore representative (Supplementary Table S3). However, there is no im-\\nprovement of prediction performance after applying data augmentation al-\\ngorithms. Optimization for GANs or other data augmentation algorithms \\nneeds to be explored further. \\nDespite that GANs have improved ML in many applications, it has failed \\nto build better models in this case. There may be two reasons for the un-\\ndesired results. First, the parameters of GANs or the architecture of gen-\\nerator and discriminator were suboptimal, so the quality of the generated \\ndata was poor. To improve GANs, we plan to conduct more optimizations \\nof GANs parameters, such as learning rate of generator and discriminator. \\nSecond, model prediction performance is strongly influenced by the qual-\\nity of dataset, so the quality of the dataset we used may limit us from de-\\nveloping better models. In this case, GANs can be tried for a part of the \\noriginal dataset only including specific groups of proteins with acceptable \\nprediction accuracy.  \\n4.3 Prediction of protein yield \\nThe yield of biocatalysts here means the concentration of biocatalysts pro-\\nduced by the host cells (‘titer’ is used to describe it in some fields). De-\\nveloping biocatalysts with high yield cuts down the unit cost of biocata-\\nlysts in pharmaceutical, chemical and food industry. Random mutagenesis \\nfor improving the yield of recombinant proteins is very time-consuming \\nand expensive. Utilizing the data collected in experiments to guide the \\nmutation should be very helpful. Predicting protein yield in silico by ma-\\nchine learning models has never been done. If such a model can be suc-\\ncessfully developed, the design of better biocatalytic processes should be \\nmuch easier considering useful guidance of such a model on improving \\nprotein yield. We used eSol database, which also provides protein yield in \\nunit of µg/mL (Niwa, et al., 2009). \\nIn yield dataset, there are some data points with very high yield values, \\nwhich are considered to be outliers that will influence the modelling sig-\\nnificantly and need to be removed. We compared two approaches to re-\\nmove outliers (Supplementary Table S4) and found that the IQR approach \\n(R2=0.0759) was better than the standard deviation approach (R2=0.0622), \\nbased on R2, the metric used to evaluate performance of the developed \\nmodels (SVM was used in this evaluation). The IQR approach was utilized \\nin the rest of the study. \\nWe compared SVM with other ML methods to develop models for pre-\\ndicting protein yield and found SVM was the best though its performance \\nwas not satisfactory – R2 was only 0.1163 (Supplementary Table S5). The \\ntuning process was conducted for the best model SVM, and optimized pa-\\nrameters were used for further analysis (Supplementary Table S6, Supple-\\nmentary Fig. S4). We further attempted to improve the SVM model by \\nusing different descriptors and found that using extractAPAAC could \\nslightly improve the model prediction accuracy (increased to 0.1264 in \\nSupplementary Table S7), after optimization of SVM parameters. De-\\nscriptor extractAPAAC extracted hydrophobicity, hydrophilicity and the \\nsequence of amino acids from the sequence data.   \\nThe R2 achieved is very low and that may be caused by three reasons. First, \\nthe descriptors we used to transfer amino acid sequence into numerical \\nvalues may not be suitable (the right information may not be extracted \\nfrom sequence). Future work includes exploring more powerful de-\\nscriptors or combing several descriptors to extract more features from se-\\nquence. The second reason is that the ML methods we used were not suit-\\nable, so we will explore others including convolutional neural network \\n(CNN) and recurrent neural network (RNN). Last, there may be no asso-\\nciation between amino acid sequence and protein yield at all. Until now, \\nthere is no report on predicting protein yield from its sequence. It is pos-\\nsible that this idea was explored but no correlation can be found between \\nthem. We need to use both biological and computational approaches to \\nvalidate or falsify this hypothesis. \\nAcknowledgements \\nWe thank Wee Chin Wong for technical discussions and Shen Yifan for assistance. \\nFunding \\nThis work was supported by MOE Research Scholarship, MOE Tier-1 grant (R-279-\\n000-452-133) and NRF CRP grant (R-279-000-512-281) in Singapore. \\n \\nConflict of Interest: none declared. \\nReferences \\n \\nAgostini, F., et al. (2012) Sequence-based prediction of protein solubility. J. Mol. \\nBiol., 421, 237–241. \\nChan, W.-C., et al. (2010) Learning to predict expression efficacy of vectors in \\nrecombinant protein production. BMC Bioinform., 11, S21. \\nChristendat, D., et al. (2000) Structural proteomics of an archaeon. Nat. Struct. Mol. \\nBiol., 7, 903–909. \\nDiaz, A.A., et al. (2010) Prediction of protein solubility in Escherichia coli using \\nlogistic regression. Biotechnol. Bioeng., 105, 374–383. \\nFang, Y. and Fang, J. (2013) Discrimination of soluble and aggregation-prone \\nproteins based on sequence information. Mol. Biosyst., 9, 806–811. \\nGoh, C.-S., et al. (2004) Mining the structural genomics pipeline: identification of \\nprotein properties that affect high-throughput experimental analysis. J. Mol. \\nBiol., 336, 115–130. \\nHabibi, N., et al. (2014) A review of machine learning methods to predict the \\nsolubility of overexpressed recombinant proteins in Escherichia coli. BMC \\nBioinform., 15, 134.\\n\\n--- Page 7 ---\\nDevelop machine learning based predictive models for engineering protein solubility \\nHirose, S., et al. (2011) Statistical analysis of features associated with protein \\nexpression/solubility in an in vivo Escherichia coli expression system and a \\nwheat germ cell-free expression system. J. Biochem., 150, 73–81. \\nHirose, S. and Noguchi, T. (2013) ESPRESSO: a system for estimating protein \\nexpression and solubility in protein expression systems. Proteomics, 13, 1444–\\n1456. \\nHuang, H.-L., et al. (2012) Prediction and analysis of protein solubility using a novel \\nscoring card method with dipeptide composition. In, Bmc Bioinformatics. \\nBioMed Central, p, S3. \\nIdicula-Thomas, S., et al. (2005) A support vector machine-based method for \\npredicting the propensity of a protein to be soluble or to form inclusion body on \\noverexpression in Escherichia coli. Bioinformatics, 22, 278–284. \\nIdicula-Thomas, S. and Balaji, P.V. (2005) Understanding the relationship between \\nthe primary structure of proteins and its propensity to be soluble on \\noverexpression in Escherichia coli. Protein Sci., 14, 582–592. \\nKitagawa, M., et al. (2005) Complete set of ORF clones of Escherichia coli ASKA \\nlibrary (a complete set of E. coli K-12 ORF archive): unique resources for \\nbiological research. DNA Res., 12, 291–299. \\nMagnan, C.N., et al. (2009) SOLpro: accurate sequence-based prediction of protein \\nsolubility. Bioinformatics, 25, 2200–2207. \\nNiwa, T., et al. (2009) Bimodal protein solubility distribution revealed by an \\naggregation analysis of the entire ensemble of Escherichia coli proteins. Proc. \\nNatl. Acad. Sci. U.S.A., 106, 4201–4206. \\nRawi, R., et al. (2017) PaRSnIP: sequence-based protein solubility prediction using \\ngradient boosting machine. Bioinformatics. \\nRumelhart, D.E., et al. (1985) Learning internal representations by error propagation. \\nTechnical report DTIC Document. \\nSamak, T., et al. (2012) Prediction of protein solubility in E. coli. Chicago, IL: E-\\nScience (e-Science), IEEE 8th International Conference on Date of Conference: \\n8–12 Oct. 2012; 2012:2011–2018. \\nSmialowski, P., et al. (2012) PROSO II–a new method for protein solubility \\nprediction. FEBS J., 279, 2192–2200. \\nSmialowski, P., et al. (2006) Protein solubility: sequence based prediction and \\nexperimental verification. Bioinformatics, 23, 2536–2542. \\nStiglic, G., et al. (2012) Comprehensive decision tree models in bioinformatics. PLoS \\nOne, 7, e33812. \\nWilkinson, D.L. and Harrison, R.G. (1991) Predicting the solubility of recombinant \\nproteins in Escherichia coli. Nat. Biotechnol., 9, 443. \\nXiao, N., et al. (2014) Protr: Protein sequence descriptor calculation and similarity \\ncomputation with R. R package version 0.2-1. \\nXiaohui, N., et al. (2014) Predicting the protein solubility by integrating chaos games \\nrepresentation and entropy in information theory. Expert Syst. Appl., 41, 1672–\\n1679.\"}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:41:14.291\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 3: Create and populate the Page Summary subpage by using the previously downloaded paper information and the extracted full text from the PDFs.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:44:34.375\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking PS Tool with args: {'papers': [{'title': 'Develop machine learning based predictive models for engineering protein\\n  solubility', 'summary': 'Protein activity is a significant characteristic for recombinant proteins\\nwhich can be used as biocatalysts. High activity of proteins reduces the cost\\nof biocatalysts. A model that can predict protein activity from amino acid\\nsequence is highly desired, as it aids experimental improvement of proteins.\\nHowever, only limited data for protein activity are currently available, which\\nprevents the development of such models. Since protein activity and solubility\\nare correlated for some proteins, the publicly available solubility dataset may\\nbe adopted to develop models that can predict protein solubility from sequence.\\nThe models could serve as a tool to indirectly predict protein activity from\\nsequence. In literature, predicting protein solubility from sequence has been\\nintensively explored, but the predicted solubility represented in binary values\\nfrom all the developed models was not suitable for guiding experimental designs\\nto improve protein solubility. Here we propose new machine learning models for\\nimproving protein solubility in vivo. We first implemented a novel approach\\nthat predicted protein solubility in continuous numerical values instead of\\nbinary ones. After combing it with various machine learning algorithms, we\\nachieved a prediction accuracy of 76.28 percent when Support Vector Machine\\nalgorithm was used. Continuous values of solubility are more meaningful in\\nprotein engineering, as they enable researchers to choose proteins with higher\\npredicted solubility for experimental validation, while binary values fail to\\ndistinguish proteins with the same value. There are only two possible values so\\nmany proteins have the same one.', 'link': 'https://arxiv.org/pdf/1806.11369v2.pdf'}], 'pdf_texts': {'Develop_machine_learning_.pdf': '--- Page 1 ---\\nDevelop machine learning based predictive \\nmodels for engineering protein solubility  \\nXi Han1, Xiaonan Wang1,*, Kang Zhou1,* \\n1Department of Chemical and Biomolecular Engineering, National University of Singapore, Singapore, \\n117585.  \\n*To whom correspondence should be addressed. \\nAbstract \\nMotivation: Protein activity is a significant characteristic for recombinant proteins which can be used \\nas biocatalysts. High activity of proteins reduces the cost of biocatalysts. A model that can predict \\nprotein activity from amino acid sequence is highly desired, as it aids experimental improvement of \\nproteins. However, only limited data for protein activity are currently available, which prevents the de-\\nvelopment of such models. Since protein activity and solubility are correlated for some proteins, the \\npublicly available solubility dataset may be adopted to develop models that can predict protein solubility \\nfrom sequence. The models could serve as a tool to indirectly predict protein activity from sequence. \\nIn literature, predicting protein solubility from sequence has been intensively explored, but the predicted \\nsolubility represented in binary values from all the developed models was not suitable for guiding ex-\\nperimental designs to improve protein solubility. Here we propose new machine learning models for \\nimproving protein solubility in vivo.       \\nResults: We first implemented a novel approach that predicted protein solubility in continuous numer-\\nical values instead of binary ones. After combing it with various machine learning algorithms, we \\nachieved a prediction accuracy of 76.28% when Support Vector Machine (SVM) algorithm was used. \\nContinuous values of solubility are more meaningful in protein engineering, as they enable researchers \\nto choose proteins with higher predicted solubility for experimental validation, while binary values fail \\nto distinguish proteins with the same value – there are only two possible values so many proteins have \\nthe same one. \\nAvailability: We present the machine learning workflow as a series of IPython notebooks hosted on \\nGitHub (https://github.com/xiaomizhou616/protein_solubility). The workflow can be used as a template \\nfor analysis of other expression and solubility datasets. \\nContact: kang.zhou@nus.edu.sg, chewxia@nus.edu.sg  \\nSupplementary information: Supplementary data are available on GitHub (https://github.com/xiaomi-\\nzhou616/protein_solubility). \\n \\n \\n1 \\nIntroduction  \\nEscherichia coli is a bacterium commonly used in genetic engineering to \\nexpress recombinant proteins (Chan, et al., 2010), which is a pivotal pro-\\ncess in biotechnology. Producing some proteins in E. coli was, however, \\ninefficient in industrial setting, resulting from low activity of the recom-\\nbinant proteins. As biocatalysts and therapeutic agents, proteins with high \\nactivity have lower unit production cost. Developing biocatalysts with \\nhigh activity has thus become an important goal for further development \\nof many biocatalytic processes.  \\nSome experimental strategies in vivo can improve the expression of re-\\ncombinant proteins, such as using suitable promoters, optimizing codon \\nusage, or changing culture media, temperature and/or other culture condi-\\ntions (Idicula‐Thomas and Balaji, 2005; Magnan, et al., 2009). Such em-\\npirical optimizations have, however, been time-consuming and expensive. \\nMoreover, experiments often fail due to opaque reasons. A generic solu-\\ntion is highly desired for enhancing the heterologous protein overexpres-\\nsion and may be ultimately provided by using a computational model that \\ncan predict activity of any enzyme accurately from its amino acid se-\\nquence and other input information. Developing such model would require \\na large dataset which contains at least two columns, protein sequence and \\nactivity. No such dataset is currently available, because conventional pro-\\ntein engineering was driven by chasing high activity, which has provided \\nvery little information on protein sequence. For instance, most proteins \\nwith low or intermediate activity values were discarded without their se-\\nquence being revealed. It is currently prohibitively expensive to generate \\na complete, large dataset for protein activity, due to technical limitations. \\nA compromised solution is to predict protein activity by using solubility \\nof the protein as a proxy, because (1) activity and solubility are correlated \\nfor some proteins, and (2) there is a relatively large dataset available for \\nsolubility as solubility data from different proteins can be pooled together. \\nUntil now, a number of machine learning predictors have been developed \\nto address the interconnection between protein solubility and amino acid \\nsequence. The prediction for protein solubility from amino acid sequence \\nwas first proposed by Wilkinson and Harrison (Wilkinson and Harrison,\\n\\n--- Page 2 ---\\nX. Han et al. \\n1991). A simple regression method was used by them to achieve high ac-\\ncuracy (0.88) by using several features of 81 protein sequences, such as \\nhydrophilicity, molecular weight and averaged surface charge. The accu-\\nracy was improved by logistic regression to 0.94 based on a small database \\nwith 212 proteins (Diaz, et al., 2010). SVM is a core machine learning \\n(ML) classification method that has achieved competitive performance in \\nthis field. With SVM, Idicula-Thomas analysed 192 proteins with an un-\\nbalanced correlation score filter, which achieved an accuracy of 0.74 by \\nusing physicochemical properties, mono-peptide frequencies, dipeptide \\nfrequencies and reduced alphabet set as features (Idicula-Thomas, et al., \\n2005). These datasets are, however, not large enough to generate correla-\\ntions that can be widely applied (Idicula-Thomas and Balaji, 2005). Mo-\\nlecular weight, isometric point and amino acid composition were used as \\ninputs to build a SVM model by Niwa et al. (Niwa, et al., 2009), who for \\nthe first time generated and studied a dataset that contained more than \\n1000 entries. Prediction of protein solubility was subsequently conducted \\nwith SVM based on databases with 2159 proteins (Agostini, et al., 2012) \\nand 5692 proteins (Xiaohui, et al., 2014). Besides, decision tree \\n(Christendat, et al., 2000; Goh, et al., 2004; Rumelhart, et al., 1985), Naïve \\nBayes (Smialowski, et al., 2006), random forest (Fang and Fang, 2013; \\nHirose, et al., 2011), gradient boosting machine (Rawi, et al., 2017) and \\nsome computational methods combining multiple machine learning mod-\\nels have been utilized to predict if a protein is soluble or not by using their \\nprimary sequence. In addition, several software and web servers have been \\ndeveloped for protein solubility prediction, including ESPRESSO (Hirose \\nand Noguchi, 2013), Pros (Hirose and Noguchi, 2013), SCM (Huang, et \\nal., 2012), PROSOII (Smialowski, et al., 2012), SOLpro (Magnan, et al., \\n2009) and PROSO (Smialowski, et al., 2006). . Some of the predictors \\navailable online are introduced below. ESPRESSO selects features by us-\\ning Student’s t-test filter and trains a model by SVM and another sequence \\npattern recognition based method. Pros implements random forest with \\nStudent’s t-test as filter and random forest as wrapper. PROSOII is a two-\\nlayer model using a Parzen window in the first layer and logistic regres-\\nsion in both layers. \\nVarious databases were utilized in the previous studies, whereas eSol \\ndatabase (Niwa, et al., 2009) is a unique one, which has a relatively large \\nsize and continuous values of solubility, ranging from 0 to 1. Comparison \\nof model performances using this database is summarized in Table 1. All \\nthe ML models developed based on this database, however, only predicted \\nwhether a protein was soluble or insoluble (outputting binary values), \\nwhich was not useful in guiding protein engineering, because even a sub-\\nstantial increase in solubility may not turn the status from ‘insoluble’ to \\n‘soluble’ and thus be falsefully viewed as useless by the model. Moreover, \\nit is challenging to select several proteins with highest solubility to con-\\nduct experiments because there are too many proteins with the same value \\n1 of solubility in a large dataset. \\nIn the present study, we built models that can predict protein solubility \\nin numerical values that span from 0 to 1, which would allow fair evalua-\\ntion of a proposed mutation to a protein even when the resulting improve-\\nment is small. We compared various model-building techniques which use \\nthis new output format, and found SVM performed the best, achieving an \\naccuracy of 76.28%. \\n2 \\nMethods \\n2.1 Protein database \\nAll the proteins used in the study were downloaded from eSol database \\n(Niwa, et al., 2009). A brief summary of how solubility values in eSol \\ndatabase were obtained is provided here. Proteins were produced in vitro \\nby using cell-free protein expression technology and plasmids carrying \\nOpen Reading Frame (ORF) from complete E. coli ORF library (ASKA \\nlibrary) (Kitagawa, et al., 2005). Synthesized proteins were fractionated \\ninto soluble and insoluble fractions by using centrifugation, and the pro-\\nteins in both fractions were quantified by using SDS-PAGE. Solubility \\nwas defined as the ratio of protein quantity of the supernatant to the total \\nprotein quantity. We used all the entries from the database, unless the en-\\ntries contain no sequence information, or poorly determined sequence \\n(containing N instead of A, T, C, or G, or multiple stop-codons). We ex-\\ncluded 26 entries in total, and the size of the dataset used was 3147. \\n2.2 Features \\nDifferent features were extracted from sequences of proteins by using \\nprotr package (Xiao, et al., 2014) within R software, which generates var-\\nious numerical representation schemes. In our study, the description of \\nfeatures is listed in Table 2 according to different descriptors in protr pack-\\nage.  \\n2.3 Training flowsheet \\nData pre-processing is a very important part in data mining, which trans-\\nfers raw data into a well-organized dataset that can be taken as inputs in \\nmodel training. Only sequence and protein solubility were retrieved from \\nthe original dataset. Protein sequence was then transformed into numeric \\nvalues (features) by using an algorithm from Table 2. Finally, data nor-\\nmalization was conducted by making numerical values of each column in \\nthe range of 0-1. \\nTraining machine learning models for protein solubility prediction in-\\ncluded several steps (as shown in Fig. 3). Seventy five percent of the orig-\\ninal dataset (2364 proteins) were used in GANs model training and 90 \\npercent of the original dataset (2833 proteins) were used in other machine \\nlearning model training. The remaining dataset were validation data which \\ncan be used to evaluate the performance of each model. After model train-\\ning, all the evaluation metrics were calculated with 10-fold cross valida-\\ntion to compare the performance of different machine learning models. \\n2.4 Machine learning models \\nSeven supervised machine learning algorithms were applied to our da-\\ntaset: logistic regression, decision tree, SVM, Naïve Bayes, conditional \\nrandom forest (cforest), XGboost and artificial neural networks (ANNs). \\nFor binary values of solubility, classification algorithms were used and for \\ncontinuous values of solubility, regression algorithms were applied. \\n2.5 Tuning SVM \\nAmong the machine learning models used to predict the protein solubility, \\nSVM achieved the best performance, so SVM was chosen for further op-\\ntimization to achieve higher prediction accuracy. The parameters of SVM \\nwere tuned (Package e1071 in R software was used to implement SVM \\nmodel). Different kernels in SVM represent different functions. Another \\nthree parameters tuned were cost, gamma and epsilon. Cost is the regular-\\nization parameter that controls the trade-off between achieving a low train-\\ning error and a low testing error. Epsilon is a margin of tolerance where \\nno penalty is given to errors. Gamma indicates the threshold to determine \\nif two points are considered to be similar.\\n\\n--- Page 3 ---\\nDevelop machine learning based predictive models for engineering protein solubility \\nTable 1. Performance of published methods based on eSol database \\n \\n \\nMethod \\nSize \\nAccuracy \\nReference \\nRandom forest \\nTotal: 1918 \\nSoluble: 886 \\nInsoluble: 1032 \\n0.84 \\n(Fang and Fang, 2013) \\n1. Support vector machine \\n2. Random forest \\n3. Conditional inference trees \\n4. Rule ensemble \\nTotal: 1625 \\nSoluble: 843 \\nInsoluble: 782 \\n0.81a  \\n(Samak, et al., 2012) \\nDecision tree \\nSize: 1625 \\nSoluble: 843 \\nInsoluble: 782 \\n0.75 \\n(Stiglic, et al., 2012) \\nSupport vector machine \\nSize: 2159 \\nSoluble: 1081 \\nInsoluble: 1078 \\nN/A \\n(Agostini, et al., 2012) \\nSupport vector machine \\nSize: 3173 \\nSoluble: \\>0.7 \\nInsoluble: \\<0.3b \\n0.80 \\n(Niwa, et al., 2009) \\na Value estimated from the figure in the reference \\nb Proteins with solubility higher than 0.7 are labelled as soluble and lower than 0.3 are labelled as insoluble \\nTable 2. Description of different descriptors in protr package of R software \\nDescriptors \\nDescription \\nextractAAC \\ncomposition of each amino acid \\nextractPAAC \\nhydrophobicity, hydrophilicity and side chain mass \\nextractAPAAC \\nhydrophobicity, hydrophilicity and sequence order \\nextractMoreauBroto \\nnormalized average hydrophobicity scales, average flexibility indices, polarizability parameter, free energy of solution in \\nwater \\nextractMoran \\nnormalized average hydrophobicity scales, average flexibility indices, polarizability parameter, free energy of solution in \\nwater c \\nextractGeary \\nnormalized average hydrophobicity scales, average flexibility indices, polarizability parameter, free energy of solution in \\nwater c \\nextractCTDC \\nhydrophobicity, normalized van der Waals volume, polarity, polarizability, charge, second structure, solvent accessibility \\nextractCTDT \\npercentage of position of attributes in extractCTDC \\nextractCTDD  \\ndistribution of attributes in extractCTDT \\nextractCTriad  \\nprotein-protein interactions including electrostatic and hydrophobic interactions \\nextractSOCN  \\nSchneider-Wrede physicochemical distance matrix and Grantham chemical distance matrix \\nextractQSO  \\nSchneider-Wrede physicochemical distance matrix and Grantham chemical distance matrix d \\nc Different calculation methods are used for extractMoreauBroto, extractMoran and extractGeary using the same attributes listed above \\nd Different calculation methods are used for extractSOCN and extractQSO using the same attributes listed above\\n2.6 Data augmentation algorithms \\nGenerative Adversarial Networks (GANs) are an emerging Artificial In-\\ntelligence (AI) algorithm used for data augmentation in this work. There \\nare two main models within GANs, Generator Neural Network and Dis-\\ncriminator Neural Network. The generative model G takes random data z \\nfrom probability distribution p(z) as input. The discriminative model D \\ntakes data generated from generative model G and real data from training \\ndataset as inputs and distinguishes whether the data are artificial or real. \\nThe training process is an adversarial game with backpropagation between \\ntwo networks (Fig. 2).  \\nIn Equation (1), D(x) calculates the probability that x comes from the orig-\\ninal data. D’s objective is to maximize the probability of labelling both the \\nreal and artificial data correctly (termed as max(V)), whereas G’s objective \\nis to minimize max (V), which can be expressed as min(max(V)).  Overall, \\nit is a two-player game between D and G.  \\n \\nFor training GANs, discriminator is first trained for n epochs based on real \\ndata. Second, data artificially generated from generator is fed into discrim-\\ninator networks which gave a judgement if the data was artificial. Third, \\ngenerator networks were further trained with the feedbacks from discrim-\\ninator to improve its data generation. The training process was iterated \\nuntil the data generated from generator cannot be distinguished by the dis-\\ncriminator networks from the original dataset.  \\n \\n𝑚𝑖𝑛\\n%\\n𝑚𝑎𝑥\\n(\\n𝑉𝐷, 𝐺= 𝐸/~12343 5  [log 𝐷𝑥] +\\n 𝐸=~1\\> \\>   [log(1 −𝐷(𝐺𝑧))]                                                      (1)\\n\\n--- Page 4 ---\\nX. Han et al. \\n \\nFig. 1. Workflow of model training used in our study \\nWith the development of AI algorithms for data augmentation, other algo-\\nrithms based on GANs have also been developed, which enhanced the per-\\nformance of GANs to some extent. CGAN is the conditional version of \\nGANs, which generates mimic data with class labels (Mirza and Osindero, \\n2014). WGAN uses Wasserstein distance metric rather than Jensen-Shan-\\nnon distance metric used in GANs when cross-entropy is calculated \\n(Arjovsky, et al., 2017). The cross-entropy loss is a measurement of the \\nability of discriminator to distinguish real data and generated data. Was-\\nserstein distance measures the minimum cost to make the distribution of \\ngenerated data more similar to that of real data, with the critic function in \\nthe discriminator model. WCGAN is the conditional version of the Was-\\nserstein GANs (Gulrajani, et al., 2017). \\n \\n \\nFig. 2. The workflow of training GANs \\n2.7 Evaluation metrics \\nSeveral evaluation metrics were used to quantitatively evaluate the perfor-\\nmance of different machine learning models. For comparison with prior \\nwork, we transformed continuous predicted solubility values into binary \\nones. Accuracy was calculated based on the binary values. The definition \\nof TP (true positive), TN (true negative), FP (false positive) and FN (false \\nnegative) is illustrated in Table 3. How to use TP, TN, FP and FN to cal-\\nculate accuracy is shown in Equation (2). \\n \\nTable 3. The definition of TP, TN, FP and FN for binary values of solu-\\nbility \\n \\n \\nPredicted solubility from models \\nActual solu-\\nbility  \\nfrom exper-\\niments \\n \\n \\nSolubility=1 \\nSolubility=0 \\nSolubil-\\nity = 1 \\na (TP: true posi-\\ntive) \\nb (FN: false nega-\\ntive) \\nSolubil-\\nity = 0 \\nc (FP: false posi-\\ntive) \\nd (TN: true nega-\\ntive) \\n \\nWe also evaluated the model performance by using coefficient of deter-\\nmination (R2) calculated from predicted and actual solubility values. Com-\\nputing R2 can only be done by using continuous values our models gener-\\nated. \\nK-fold cross-validation is a technique to partition the original dataset into \\nk equal-size parts. And each part has similar proportion of samples from \\ndifferent classes. In the k subsamples, one subsample is taken as validation \\ndata to test the model and the other data is used as training data to train the \\nmodel. The accuracy and R2 are the average of all the cases. In our study, \\nwe used k-fold cross-validation, in which k was set as 10.  \\n3 \\nResults \\n3.1 Develop models that output continuous numerical values \\n \\nWe used continuous values of protein solubility to train machine learning \\nmodels, which we set to output continuous values instead of binary ones. \\nWe have trained seven distinct models and compared their performance \\nby using two different evaluation criteria (accuracy and R2, Table 4). For \\nthe purpose of calculating the accuracy, we converted the continuous val-\\nues of predicted solubility into binary ones by using 0.44 (median value \\nof the solubility data of our database) as cut-off threshold because the data \\nabove and below median exhibit same sizes. SVM showed the best pre-\\ndiction performance among all the machine learning models (Table 4). \\nThe tuned SVM parameters were listed in Table S1 and Fig. S1 in Supple-\\nmentary Information. After plotting the correlation between the accuracy \\nvs. cost, epsilon, or gamma, optimized values were found (1.26, 0.029 and \\n0.057 for cost, gamma, or epsilon, respectively).  \\nFor comparison, we also trained the models by using the conventional ap-\\nproach, which converted continuous actual solubility values into binary \\nones BEFORE model training. Though using continuous values did not \\nalways improve model prediction performance, it improved SVM model \\nand made it to be the best one, in terms of both accuracy and R2 (Table 4).  \\n \\nAccuracy =\\n𝒂K𝒅\\n𝒂K𝒃K𝒄K𝒅                                                                   (2)\\n\\n--- Page 5 ---\\nDevelop machine learning based predictive models for engineering protein solubility \\nTable 4. Performance of different machine learning models  \\nModel \\nAccuracy \\nR2 \\nContinuous \\nvalues of solu-\\nbility \\nBinary \\nval-\\nues of solu-\\nbility \\nContinuous \\nvalues \\nof \\nsolubility \\nLogistic regression \\n0.6372 \\n0.6868 \\n0.2507 \\nDecision tree \\n0.6734 \\n0.6750 \\n0.2459 \\nSVM \\n0.7538 \\n0.7001 \\n0.4107 \\nNaïve Bayes \\n0.6674 \\n0.6979 \\n0.2376 \\ncforest \\n0.7055 \\n0.7123 \\n0.3764 \\nXGboost \\nANNs     \\n0.7052 \\n0.6632 \\n0.7103 \\n0.7086 \\n0.3997 \\n0.2029 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Effect of cut-off threshold on SVM prediction accuracy \\nCut-off points were used to transfer continuous values of solubility to bi-\\nnary values for model evaluation. How to choose the cut-off threshold \\nsubstantially affects accuracy that is used to gauge model performance. In \\nprevious studies, many arbitrarily set it at 0.5. Here we did a simple opti-\\nmization of the threshold with maximizing the accuracy as the objective. \\nWhen the cut-off threshold was too low (\\< 0.3) or too high (\\> 0.7), the \\naccuracy would be very high, but such accuracy was not useful because \\nalmost all the proteins would be classified either as soluble or insoluble \\nproteins. We varied the threshold from 0.3 to 0.7 with step size of 0.01 \\nand calculated the SVM model prediction accuracy. From Fig. 3, the high-\\nest accuracy (0.7628) was obtained when the cut-off point was 0.36 or \\n0.39. Therefore, 0.39 was randomly selected from those two values and \\nwas used in the following analysis. Although this optimization did not \\nsubstantially improve SVM model prediction accuracy, the results showed \\nthat the threshold can substantially change the accuracy (Fig. 3). Since it \\nis arbitrary to choose a cut-off threshold, accuracy based on the cut-off \\nthreshold is intrinsically subjective. With our new methods and the result-\\ning continuous model output values, we call for using the coefficient of \\ndetermination (R2) as the model evaluation metric, which does not require \\nany cut-off threshold. We used the R2 as the sole evaluation metric in the \\nrest of this study. \\n3.2 Attempt to improve model performance by optimizing \\ndata pre-processing \\n \\nTo further improve SVM model prediction performance, we evaluated all \\nthe available algorithms that convert amino acid sequence into numerical \\nvalues. This step is critical as it extracts a fraction of information (usually \\nonly a small fraction, termed as features) from the sequence data, and \\nfeeds it to the model-training algorithms. Since most information in raw \\ndata was discarded here, avoiding loss of critical information by using a \\nbetter algorithm may improve the model performance. It turned out that \\nthe best was the basic algorithm that simply converted sequence into \\namino acid composition, and that we used in prior sessions of this study \\n(Table 5).  \\n \\nTable 5. Performance of different sequence descriptors by SVM e \\n \\nDescriptors  \\nR2 \\nData dimension \\nextractAAC \\n0.4108 \\n20 \\nextractPAAC  \\n0.3564 \\n50 \\nextractAAC+extractPAAC \\n0.3719 \\n70 \\nextractAPAAC \\n0.3049 \\n80 \\nextractMoreauBroto \\n0.0806 \\n240 \\nextractMoran \\n0.0542 \\n240 \\nextractGeary \\n0.0562 \\n240 \\nextractCTDC \\n \\n0.2484f \\n21 \\nextractCTDT \\n21 \\nextractCTDD  \\n105 \\nextractCTriad  \\n0.0650 \\n343 \\nextractSOCN  \\n0.1647 \\n80 \\nextractQSO  \\n0.3224 \\n100 \\ne The description of each descriptor is shown in Table 2 \\nf The R2 is the result combining extractCTDC, extractCTDT and extractCTDD \\n4 \\nDiscussion  \\n4.1 Predict protein solubility with continuous values \\nIn this study, we developed a new model from amino acid sequence that \\ncan predict protein solubility as continuous values from 0 to 1, which \\nwould help improve solubility of recombinant proteins through protein en-\\ngineering. Having model output in such format also enables to use coeffi-\\ncient of determination (R2) as a metric to evaluate model performance, \\nwhich does not depend on a subjective threshold to determine if a protein \\nis soluble or insoluble. In literature, all the studies used such threshold(s) \\nto convert continuous values of protein solubility in eSol database into \\nbinary values. \\nWith continuous values of solubility, we will study how to use a developed \\nmodel to guide experiments in the future. First, we will use statistical tools \\nto determine the minimal performance of a model (in terms of R2) required \\nto conduct a meaningful in silico screening of protein mutants – the aver-\\nage quality of selected mutants should be substantially higher than that of \\nthe initial pool of mutants. The next step is to validate the model prediction \\nby selecting a number of proteins with high predicted solubility and meas-\\nuring their actual solubility through experiments. We plan to generate the \\npool of mutants for in silico screening by randomly introducing mutants \\nto a model protein, However, binding of substrate/co-factor and catalytic \\nactive site may be damaged by such random mutation, which would influ-\\nence the catalytic function. Therefore, mutation of residues responsible for \\nbinding substrates and the catalytic activity will be avoided. Then sensi-\\ntivity analysis will be conducted to determine how many mutations should\\n\\n--- Page 6 ---\\nX. Han et al. \\nbe made to improve protein solubility substantially. The model we devel-\\noped in this study laid the foundation for such future works, which cannot \\nbe done by using any existing models.  \\n4.2 Attempt to improve model performance by using data \\naugmentation \\nHaving adequate volume of data is a key to develop useful models by us-\\ning machine learning. In biotechnology field, generating and collecting \\nlarge amount of data is, however, time-consuming and expensive, result-\\ning in much smaller dataset than other fields such as facial recognition. \\nWe hypothesized that data augmentation algorithms might alleviate the \\nproblem that biotechnology applications do not have sufficient data for \\ndeveloping machine learning models. We attempted to use GANs to im-\\nprove prediction of our SVM model by generating artificial data.  \\nIn the first attempt, four versions of GANs were trained for 500 iterations \\nto generate the artificial data, which were used together with the original \\ndata to train ML models by using SVM. This attempt, however, failed to \\nimprove the model performance, as R2 was not increased (Supplementary \\nTable S2). Over iterations, the quality of data generated by GANs im-\\nproved (Supplementary Fig. S3), which showed GANs were implemented \\ncorrectly. By comparing generated data with original data (Supplementary \\nFig. S2) and the performance of four versions of GANs (Supplementary \\nTable S2), GANs have the best performance among the four versions of \\nGANs. Therefore, GANs were explored for more iterations.  \\nIn the second attempt, we increased the number of iterations from 500 to \\n5000. Three randomly selected datasets were tested to make the results \\nmore representative (Supplementary Table S3). However, there is no im-\\nprovement of prediction performance after applying data augmentation al-\\ngorithms. Optimization for GANs or other data augmentation algorithms \\nneeds to be explored further. \\nDespite that GANs have improved ML in many applications, it has failed \\nto build better models in this case. There may be two reasons for the un-\\ndesired results. First, the parameters of GANs or the architecture of gen-\\nerator and discriminator were suboptimal, so the quality of the generated \\ndata was poor. To improve GANs, we plan to conduct more optimizations \\nof GANs parameters, such as learning rate of generator and discriminator. \\nSecond, model prediction performance is strongly influenced by the qual-\\nity of dataset, so the quality of the dataset we used may limit us from de-\\nveloping better models. In this case, GANs can be tried for a part of the \\noriginal dataset only including specific groups of proteins with acceptable \\nprediction accuracy.  \\n4.3 Prediction of protein yield \\nThe yield of biocatalysts here means the concentration of biocatalysts pro-\\nduced by the host cells (‘titer’ is used to describe it in some fields). De-\\nveloping biocatalysts with high yield cuts down the unit cost of biocata-\\nlysts in pharmaceutical, chemical and food industry. Random mutagenesis \\nfor improving the yield of recombinant proteins is very time-consuming \\nand expensive. Utilizing the data collected in experiments to guide the \\nmutation should be very helpful. Predicting protein yield in silico by ma-\\nchine learning models has never been done. If such a model can be suc-\\ncessfully developed, the design of better biocatalytic processes should be \\nmuch easier considering useful guidance of such a model on improving \\nprotein yield. We used eSol database, which also provides protein yield in \\nunit of µg/mL (Niwa, et al., 2009). \\nIn yield dataset, there are some data points with very high yield values, \\nwhich are considered to be outliers that will influence the modelling sig-\\nnificantly and need to be removed. We compared two approaches to re-\\nmove outliers (Supplementary Table S4) and found that the IQR approach \\n(R2=0.0759) was better than the standard deviation approach (R2=0.0622), \\nbased on R2, the metric used to evaluate performance of the developed \\nmodels (SVM was used in this evaluation). The IQR approach was utilized \\nin the rest of the study. \\nWe compared SVM with other ML methods to develop models for pre-\\ndicting protein yield and found SVM was the best though its performance \\nwas not satisfactory – R2 was only 0.1163 (Supplementary Table S5). The \\ntuning process was conducted for the best model SVM, and optimized pa-\\nrameters were used for further analysis (Supplementary Table S6, Supple-\\nmentary Fig. S4). We further attempted to improve the SVM model by \\nusing different descriptors and found that using extractAPAAC could \\nslightly improve the model prediction accuracy (increased to 0.1264 in \\nSupplementary Table S7), after optimization of SVM parameters. De-\\nscriptor extractAPAAC extracted hydrophobicity, hydrophilicity and the \\nsequence of amino acids from the sequence data.   \\nThe R2 achieved is very low and that may be caused by three reasons. First, \\nthe descriptors we used to transfer amino acid sequence into numerical \\nvalues may not be suitable (the right information may not be extracted \\nfrom sequence). Future work includes exploring more powerful de-\\nscriptors or combing several descriptors to extract more features from se-\\nquence. The second reason is that the ML methods we used were not suit-\\nable, so we will explore others including convolutional neural network \\n(CNN) and recurrent neural network (RNN). Last, there may be no asso-\\nciation between amino acid sequence and protein yield at all. Until now, \\nthere is no report on predicting protein yield from its sequence. It is pos-\\nsible that this idea was explored but no correlation can be found between \\nthem. We need to use both biological and computational approaches to \\nvalidate or falsify this hypothesis. \\nAcknowledgements \\nWe thank Wee Chin Wong for technical discussions and Shen Yifan for assistance. \\nFunding \\nThis work was supported by MOE Research Scholarship, MOE Tier-1 grant (R-279-\\n000-452-133) and NRF CRP grant (R-279-000-512-281) in Singapore. \\n \\nConflict of Interest: none declared. \\nReferences \\n \\nAgostini, F., et al. (2012) Sequence-based prediction of protein solubility. J. Mol. \\nBiol., 421, 237–241. \\nChan, W.-C., et al. (2010) Learning to predict expression efficacy of vectors in \\nrecombinant protein production. BMC Bioinform., 11, S21. \\nChristendat, D., et al. (2000) Structural proteomics of an archaeon. Nat. Struct. Mol. \\nBiol., 7, 903–909. \\nDiaz, A.A., et al. (2010) Prediction of protein solubility in Escherichia coli using \\nlogistic regression. Biotechnol. Bioeng., 105, 374–383. \\nFang, Y. and Fang, J. (2013) Discrimination of soluble and aggregation-prone \\nproteins based on sequence information. Mol. Biosyst., 9, 806–811. \\nGoh, C.-S., et al. (2004) Mining the structural genomics pipeline: identification of \\nprotein properties that affect high-throughput experimental analysis. J. Mol. \\nBiol., 336, 115–130. \\nHabibi, N., et al. (2014) A review of machine learning methods to predict the \\nsolubility of overexpressed recombinant proteins in Escherichia coli. BMC \\nBioinform., 15, 134.\\n\\n--- Page 7 ---\\nDevelop machine learning based predictive models for engineering protein solubility \\nHirose, S., et al. (2011) Statistical analysis of features associated with protein \\nexpression/solubility in an in vivo Escherichia coli expression system and a \\nwheat germ cell-free expression system. J. Biochem., 150, 73–81. \\nHirose, S. and Noguchi, T. (2013) ESPRESSO: a system for estimating protein \\nexpression and solubility in protein expression systems. Proteomics, 13, 1444–\\n1456. \\nHuang, H.-L., et al. (2012) Prediction and analysis of protein solubility using a novel \\nscoring card method with dipeptide composition. In, Bmc Bioinformatics. \\nBioMed Central, p, S3. \\nIdicula-Thomas, S., et al. (2005) A support vector machine-based method for \\npredicting the propensity of a protein to be soluble or to form inclusion body on \\noverexpression in Escherichia coli. Bioinformatics, 22, 278–284. \\nIdicula-Thomas, S. and Balaji, P.V. (2005) Understanding the relationship between \\nthe primary structure of proteins and its propensity to be soluble on \\noverexpression in Escherichia coli. Protein Sci., 14, 582–592. \\nKitagawa, M., et al. (2005) Complete set of ORF clones of Escherichia coli ASKA \\nlibrary (a complete set of E. coli K-12 ORF archive): unique resources for \\nbiological research. DNA Res., 12, 291–299. \\nMagnan, C.N., et al. (2009) SOLpro: accurate sequence-based prediction of protein \\nsolubility. Bioinformatics, 25, 2200–2207. \\nNiwa, T., et al. (2009) Bimodal protein solubility distribution revealed by an \\naggregation analysis of the entire ensemble of Escherichia coli proteins. Proc. \\nNatl. Acad. Sci. U.S.A., 106, 4201–4206. \\nRawi, R., et al. (2017) PaRSnIP: sequence-based protein solubility prediction using \\ngradient boosting machine. Bioinformatics. \\nRumelhart, D.E., et al. (1985) Learning internal representations by error propagation. \\nTechnical report DTIC Document. \\nSamak, T., et al. (2012) Prediction of protein solubility in E. coli. Chicago, IL: E-\\nScience (e-Science), IEEE 8th International Conference on Date of Conference: \\n8–12 Oct. 2012; 2012:2011–2018. \\nSmialowski, P., et al. (2012) PROSO II–a new method for protein solubility \\nprediction. FEBS J., 279, 2192–2200. \\nSmialowski, P., et al. (2006) Protein solubility: sequence based prediction and \\nexperimental verification. Bioinformatics, 23, 2536–2542. \\nStiglic, G., et al. (2012) Comprehensive decision tree models in bioinformatics. PLoS \\nOne, 7, e33812. \\nWilkinson, D.L. and Harrison, R.G. (1991) Predicting the solubility of recombinant \\nproteins in Escherichia coli. Nat. Biotechnol., 9, 443. \\nXiao, N., et al. (2014) Protr: Protein sequence descriptor calculation and similarity \\ncomputation with R. R package version 0.2-1. \\nXiaohui, N., et al. (2014) Predicting the protein solubility by integrating chaos games \\nrepresentation and entropy in information theory. Expert Syst. Appl., 41, 1672–\\n1679.'}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:44:47.183\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - \u001b[0m\n",
      "\u001b[32m2025-04-13 08:44:48.743\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 4: Extract the core mathematical and scientific concepts from the full text output to determine the generalizable topics for a learning pathway. The extraction should focus solely on overarching mathematical and scientific topics with no additional explanation or study-specific details.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:45:02.540\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking LLM Tool with args: {'task': 'Extract the core mathematical and scientific concepts from the full text output to determine the generalizable topics for a learning pathway. The extraction should focus solely on overarching mathematical and scientific topics with no additional explanation or study-specific details.'}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:45:08.483\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - 1. Machine Learning Algorithms\n",
      "2. Predictive Modeling\n",
      "3. Protein Solubility Prediction\n",
      "4. Support Vector Machine (SVM)\n",
      "5. Data Augmentation\n",
      "6. Generative Adversarial Networks (GANs)\n",
      "7. Data Pre-processing and Feature Extraction\n",
      "8. Regression and Classification Techniques\n",
      "9. Evaluation Metrics (Accuracy, R²)\n",
      "10. Data Normalization\n",
      "11. Sequence Analysis and Descriptor Calculation\n",
      "12. Optimization of Model Parameters\u001b[0m\n",
      "\u001b[32m2025-04-13 08:45:10.013\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 5: Use the Topic Selector tool by providing the extracted topics as the raw topics list to allow the selection of topics.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:45:15.033\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Topic Selector Tool with args: {'raw_topics': ['Machine Learning Algorithms', 'Predictive Modeling', 'Protein Solubility Prediction', 'Support Vector Machine (SVM)', 'Data Augmentation', 'Generative Adversarial Networks (GANs)', 'Data Pre-processing and Feature Extraction', 'Regression and Classification Techniques', 'Evaluation Metrics (Accuracy, R²)', 'Data Normalization', 'Sequence Analysis and Descriptor Calculation', 'Optimization of Model Parameters'], 'selected_indices': None}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:45:18.012\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [{\"id\": \"clar-9c7763fd-f6ba-4789-aa96-da7c2fd58e3a\", \"plan_run_id\": \"prun-a587cefc-258f-40f9-8b45-8d66356c24e2\", \"category\": \"Input\", \"response\": null, \"step\": null, \"user_guidance\": \"Please enter the numbers of the topics you'd like to learn about, separated by commas (e.g. 1, 3, 5):\\n\\n1. Machine Learning Algorithms\\n2. Predictive Modeling\\n3. Protein Solubility Prediction\\n4. Support Vector Machine (SVM)\\n5. Data Augmentation\\n6. Generative Adversarial Networks (GANs)\\n7. Data Pre-processing and Feature Extraction\\n8. Regression and Classification Techniques\\n9. Evaluation Metrics (Accuracy, R²)\\n10. Data Normalization\\n11. Sequence Analysis and Descriptor Calculation\\n12. Optimization of Model Parameters\", \"resolved\": false, \"argument_name\": \"selected_indices\"}]\u001b[0m\n",
      "\u001b[32m2025-04-13 08:45:18.014\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.portia\u001b[0m:\u001b[38;5;87m_raise_clarifications\u001b[0m:\u001b[38;5;87m791\u001b[0m - \u001b[1mClarification requested - category: ClarificationCategory.INPUT, user_guidance: Please enter the numbers of the topics you'd like to learn about, separated by commas (e.g. 1, 3, 5):\n",
      "\n",
      "1. Machine Learning Algorithms\n",
      "2. Predictive Modeling\n",
      "3. Protein Solubility Prediction\n",
      "4. Support Vector Machine (SVM)\n",
      "5. Data Augmentation\n",
      "6. Generative Adversarial Networks (GANs)\n",
      "7. Data Pre-processing and Feature Extraction\n",
      "8. Regression and Classification Techniques\n",
      "9. Evaluation Metrics (Accuracy, R²)\n",
      "10. Data Normalization\n",
      "11. Sequence Analysis and Descriptor Calculation\n",
      "12. Optimization of Model Parameters.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[38;5;87mPlease enter the numbers of the topics you'd like to learn about, separated by commas (e.g. 1, 3, 5):\n",
      "\n",
      "1. Machine Learning Algorithms\n",
      "2. Predictive Modeling\n",
      "3. Protein Solubility Prediction\n",
      "4. Support Vector Machine (SVM)\n",
      "5. Data Augmentation\n",
      "6. Generative Adversarial Networks (GANs)\n",
      "7. Data Pre-processing and Feature Extraction\n",
      "8. Regression and Classification Techniques\n",
      "9. Evaluation Metrics (Accuracy, R²)\n",
      "10. Data Normalization\n",
      "11. Sequence Analysis and Descriptor Calculation\n",
      "12. Optimization of Model Parameters\n",
      "Please enter a value\u001b[0m:\u001b[32m2025-04-13 08:46:20.979\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.portia\u001b[0m:\u001b[38;5;87mresolve_clarification\u001b[0m:\u001b[38;5;87m366\u001b[0m - \u001b[1mClarification resolved with response: 2, 3, 8\u001b[0m\n",
      "\u001b[32m2025-04-13 08:46:25.384\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[37mportia.portia\u001b[0m:\u001b[37mwait_for_ready\u001b[0m:\u001b[37m477\u001b[0m - \u001b[1mRun prun-a587cefc-258f-40f9-8b45-8d66356c24e2 is ready to resume\u001b[0m\n",
      "\u001b[32m2025-04-13 08:46:26.843\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m528\u001b[0m - \u001b[1mPlan Run State is updated to PlanRunState.IN_PROGRESS. View in your Portia AI dashboard: https://app.portialabs.ai/dashboard/plan-runs?plan_run_id=prun-a587cefc-258f-40f9-8b45-8d66356c24e2\u001b[0m\n",
      "\u001b[32m2025-04-13 08:46:26.845\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 5: Use the Topic Selector tool by providing the extracted topics as the raw topics list to allow the selection of topics.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:46:33.206\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Topic Selector Tool with args: {'raw_topics': ['Machine Learning Algorithms', 'Predictive Modeling', 'Protein Solubility Prediction', 'Support Vector Machine (SVM)', 'Data Augmentation', 'Generative Adversarial Networks (GANs)', 'Data Pre-processing and Feature Extraction', 'Regression and Classification Techniques', 'Evaluation Metrics (Accuracy, R²)', 'Data Normalization', 'Sequence Analysis and Descriptor Calculation', 'Optimization of Model Parameters'], 'selected_indices': '2, 3, 8'}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:46:36.240\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [\"Predictive Modeling\", \"Protein Solubility Prediction\", \"Regression and Classification Techniques\"]\u001b[0m\n",
      "\u001b[32m2025-04-13 08:46:38.001\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 6: Create Notion pages for each of the selected topics by passing the topics list to the creation process.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:46:41.459\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Notion Tool with args: {'topics': ['Predictive Modeling', 'Protein Solubility Prediction', 'Regression and Classification Techniques']}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:47:21.009\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [{\"topic\": \"Predictive Modeling\", \"page_id\": \"1d46ccbb-ecba-8124-92ca-ec4db74ccd33\", \"content\": \"[[Introduction]]\\nPredictive modeling involves using statistical techniques to create models that can predict future outcomes based on historical data. These models are essential in various fields such as finance, healthcare, and marketing, where forecasting future trends and behaviors is crucial for strategic planning and decision-making.\\n\\n[[Key Definitions]]\\n- **Predictive Model**: A mathematical model or algorithm that uses historical data to predict future outcomes.\\n- **Training Data**: A dataset used to train a predictive model, allowing it to learn patterns and relationships.\\n- **Overfitting**: A modeling error that occurs when a model is too complex and captures noise in the data rather than the underlying pattern.\\n\\n[[Relevant Formulas]]\\n**Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\ldots + \\\\beta_n x_n + \\\\epsilon \\\\)\\n\\n**Logistic Regression Probability**\\n\\\\( P(Y=1) = \\\\frac{1}{1 + e^{-(\\\\beta_0 + \\\\beta_1 x_1 + \\\\ldots + \\\\beta_n x_n)} \\\\)\\n\\n**Mean Squared Error (MSE)**\\n\\\\( \\\\text{MSE} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 \\\\)\\n\\n[[Examples]]\\n1. **Credit Scoring**: Banks use predictive modeling to assess the creditworthiness of loan applicants. By analyzing historical data on borrowers, banks can predict the likelihood of a new applicant defaulting on a loan.\\n2. **Healthcare Diagnostics**: Predictive models are used to forecast the likelihood of patients developing certain diseases based on their medical history and lifestyle factors. This helps in early intervention and personalized treatment plans.\\n\\n[[Reflective Questions]]\\n1. How does predictive modeling differ from descriptive analytics?\\n2. What are the potential risks of overfitting in predictive modeling, and how can they be mitigated?\\n3. Why is it important to validate a predictive model with a separate test dataset?\\n4. How can predictive modeling be applied to improve customer retention strategies in businesses?\\n5. What ethical considerations should be taken into account when using predictive models in decision-making processes?\"}, {\"topic\": \"Protein Solubility Prediction\", \"page_id\": \"1d46ccbb-ecba-8132-b4ce-c0d62761c0ef\", \"content\": \"[[Introduction]]\\nProtein solubility prediction is a crucial aspect of biotechnology and pharmaceutical research, as it helps in understanding how proteins behave in different environments. Soluble proteins are essential for various biological functions and industrial applications. Predicting solubility can aid in protein engineering and drug design.\\n\\n[[Key Definitions]]\\n- **Protein Solubility**: The ability of a protein to dissolve in a solvent, forming a homogeneous solution at a molecular level.\\n- **Hydrophobicity**: A measure of how a molecule repels water; in proteins, hydrophobic regions tend to be buried inside the structure.\\n- **Amino Acid Composition**: The specific sequence and proportion of amino acids in a protein, which significantly influences its solubility.\\n\\n[[Relevant Formulas]]\\n- **Solubility Prediction Formula**\\n  \\\\( S = \\\\sum_{i=1}^{20} f_i \\\\times H_i \\\\)\\n  Where \\\\( S \\\\) is the solubility score, \\\\( f_i \\\\) is the fraction of the \\\\( i \\\\)-th amino acid, and \\\\( H_i \\\\) is the hydrophobicity index of the \\\\( i \\\\)-th amino acid.\\n\\n- **Hydrophobicity Index Calculation**\\n  \\\\( H = \\\\frac{\\\\sum_{i=1}^{n} H_i}{n} \\\\)\\n  Where \\\\( H \\\\) is the average hydrophobicity, \\\\( H_i \\\\) is the hydrophobicity of the \\\\( i \\\\)-th amino acid, and \\\\( n \\\\) is the total number of amino acids.\\n\\n[[Examples]]\\n1. **Biopharmaceuticals**: In drug development, predicting protein solubility is vital for ensuring that therapeutic proteins can be effectively delivered in the body. For instance, insulin must remain soluble to be administered as an injectable drug.\\n\\n2. **Food Industry**: Enzymes used in food processing, such as those in cheese production, need to be soluble to function properly. Predicting solubility helps in selecting the right enzymes for specific food products.\\n\\n[[Reflective Questions]]\\n1. Why is protein solubility important in the pharmaceutical industry?\\n2. How does the hydrophobicity of a protein affect its solubility?\\n3. What role does amino acid composition play in determining protein solubility?\\n4. Can you think of a scenario where low protein solubility might be beneficial?\\n5. How might changes in environmental conditions (e.g., pH, temperature) impact protein solubility?\"}, {\"topic\": \"Regression and Classification Techniques\", \"page_id\": \"1d46ccbb-ecba-8140-a3d2-c2dcb0f52836\", \"content\": \"[[Introduction]]\\nRegression and classification are two fundamental types of predictive modeling techniques used in machine learning and statistics. Regression is used to predict continuous outcomes, while classification is used to predict discrete categories. Understanding these techniques is crucial for analyzing data and making informed decisions based on patterns identified in datasets.\\n\\n[[Key Definitions]]\\n- **Regression**: A statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to predict continuous outcomes.\\n- **Classification**: A process in machine learning where the model assigns a category label to input data based on its features. It is used for predicting discrete outcomes.\\n- **Linear Regression**: A type of regression analysis where the relationship between the dependent variable and independent variables is modeled as a linear equation.\\n\\n[[Relevant Formulas]]\\n- **Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\cdots + \\\\beta_n x_n \\\\)\\nwhere \\\\( y \\\\) is the dependent variable, \\\\( \\\\beta_0 \\\\) is the intercept, \\\\( \\\\beta_1, \\\\beta_2, \\\\ldots, \\\\beta_n \\\\) are the coefficients, and \\\\( x_1, x_2, \\\\ldots, x_n \\\\) are the independent variables.\\n\\n- **Logistic Regression Sigmoid Function**\\n\\\\( \\\\sigma(z) = \\\\frac{1}{1 + e^{-z} \\\\)\\nwhere \\\\( z \\\\) is the linear combination of input features.\\n\\n- **Accuracy for Classification**\\n\\\\( \\\\text{Accuracy} = \\\\frac{\\\\text{Number of Correct Predictions}{\\\\text{Total Number of Predictions} \\\\)\\n\\n[[Examples]]\\n1. **Predicting House Prices**: A real estate company wants to predict house prices based on features such as size, location, and number of bedrooms. They use linear regression to model the relationship between these features and the house prices, allowing them to estimate the price of a new property.\\n\\n2. **Email Spam Detection**: An email service provider uses classification techniques to identify spam emails. By analyzing features such as the presence of certain keywords, the frequency of links, and the sender's address, the model classifies incoming emails as either 'spam' or 'not spam'.\\n\\n[[Reflective Questions]]\\n1. What are the main differences between regression and classification techniques?\\n2. How does the choice of features affect the performance of a regression model?\\n3. Why might logistic regression be preferred over linear regression for binary classification tasks?\\n4. How can overfitting be addressed in regression and classification models?\\n5. What are some real-world applications where classification techniques are particularly useful?\"}]\u001b[0m\n",
      "\u001b[32m2025-04-13 08:47:22.520\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 7: Find relevant YouTube videos for the selected topics by supplying the topics list to the video search function.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:47:54.685\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking YouTube Tool with args: {'topics': [{'topic': 'Predictive Modeling', 'page_id': '1d46ccbb-ecba-8124-92ca-ec4db74ccd33', 'content': '[[Introduction]]\\nPredictive modeling involves using statistical techniques to create models that can predict future outcomes based on historical data. These models are essential in various fields such as finance, healthcare, and marketing, where forecasting future trends and behaviors is crucial for strategic planning and decision-making.\\n\\n[[Key Definitions]]\\n- **Predictive Model**: A mathematical model or algorithm that uses historical data to predict future outcomes.\\n- **Training Data**: A dataset used to train a predictive model, allowing it to learn patterns and relationships.\\n- **Overfitting**: A modeling error that occurs when a model is too complex and captures noise in the data rather than the underlying pattern.\\n\\n[[Relevant Formulas]]\\n**Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\ldots + \\\\beta_n x_n + \\\\epsilon \\\\)\\n\\n**Logistic Regression Probability**\\n\\\\( P(Y=1) = \\\\frac{1}{1 + e^{-(\\\\beta_0 + \\\\beta_1 x_1 + \\\\ldots + \\\\beta_n x_n)} \\\\)\\n\\n**Mean Squared Error (MSE)**\\n\\\\( \\\\text{MSE} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 \\\\)\\n\\n[[Examples]]\\n1. **Credit Scoring**: Banks use predictive modeling to assess the creditworthiness of loan applicants. By analyzing historical data on borrowers, banks can predict the likelihood of a new applicant defaulting on a loan.\\n2. **Healthcare Diagnostics**: Predictive models are used to forecast the likelihood of patients developing certain diseases based on their medical history and lifestyle factors. This helps in early intervention and personalized treatment plans.\\n\\n[[Reflective Questions]]\\n1. How does predictive modeling differ from descriptive analytics?\\n2. What are the potential risks of overfitting in predictive modeling, and how can they be mitigated?\\n3. Why is it important to validate a predictive model with a separate test dataset?\\n4. How can predictive modeling be applied to improve customer retention strategies in businesses?\\n5. What ethical considerations should be taken into account when using predictive models in decision-making processes?'}, {'topic': 'Protein Solubility Prediction', 'page_id': '1d46ccbb-ecba-8132-b4ce-c0d62761c0ef', 'content': '[[Introduction]]\\nProtein solubility prediction is a crucial aspect of biotechnology and pharmaceutical research, as it helps in understanding how proteins behave in different environments. Soluble proteins are essential for various biological functions and industrial applications. Predicting solubility can aid in protein engineering and drug design.\\n\\n[[Key Definitions]]\\n- **Protein Solubility**: The ability of a protein to dissolve in a solvent, forming a homogeneous solution at a molecular level.\\n- **Hydrophobicity**: A measure of how a molecule repels water; in proteins, hydrophobic regions tend to be buried inside the structure.\\n- **Amino Acid Composition**: The specific sequence and proportion of amino acids in a protein, which significantly influences its solubility.\\n\\n[[Relevant Formulas]]\\n- **Solubility Prediction Formula**\\n  \\\\( S = \\\\sum_{i=1}^{20} f_i \\\\times H_i \\\\)\\n  Where \\\\( S \\\\) is the solubility score, \\\\( f_i \\\\) is the fraction of the \\\\( i \\\\)-th amino acid, and \\\\( H_i \\\\) is the hydrophobicity index of the \\\\( i \\\\)-th amino acid.\\n\\n- **Hydrophobicity Index Calculation**\\n  \\\\( H = \\\\frac{\\\\sum_{i=1}^{n} H_i}{n} \\\\)\\n  Where \\\\( H \\\\) is the average hydrophobicity, \\\\( H_i \\\\) is the hydrophobicity of the \\\\( i \\\\)-th amino acid, and \\\\( n \\\\) is the total number of amino acids.\\n\\n[[Examples]]\\n1. **Biopharmaceuticals**: In drug development, predicting protein solubility is vital for ensuring that therapeutic proteins can be effectively delivered in the body. For instance, insulin must remain soluble to be administered as an injectable drug.\\n\\n2. **Food Industry**: Enzymes used in food processing, such as those in cheese production, need to be soluble to function properly. Predicting solubility helps in selecting the right enzymes for specific food products.\\n\\n[[Reflective Questions]]\\n1. Why is protein solubility important in the pharmaceutical industry?\\n2. How does the hydrophobicity of a protein affect its solubility?\\n3. What role does amino acid composition play in determining protein solubility?\\n4. Can you think of a scenario where low protein solubility might be beneficial?\\n5. How might changes in environmental conditions (e.g., pH, temperature) impact protein solubility?'}, {'topic': 'Regression and Classification Techniques', 'page_id': '1d46ccbb-ecba-8140-a3d2-c2dcb0f52836', 'content': \"[[Introduction]]\\nRegression and classification are two fundamental types of predictive modeling techniques used in machine learning and statistics. Regression is used to predict continuous outcomes, while classification is used to predict discrete categories. Understanding these techniques is crucial for analyzing data and making informed decisions based on patterns identified in datasets.\\n\\n[[Key Definitions]]\\n- **Regression**: A statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to predict continuous outcomes.\\n- **Classification**: A process in machine learning where the model assigns a category label to input data based on its features. It is used for predicting discrete outcomes.\\n- **Linear Regression**: A type of regression analysis where the relationship between the dependent variable and independent variables is modeled as a linear equation.\\n\\n[[Relevant Formulas]]\\n- **Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\cdots + \\\\beta_n x_n \\\\)\\nwhere \\\\( y \\\\) is the dependent variable, \\\\( \\\\beta_0 \\\\) is the intercept, \\\\( \\\\beta_1, \\\\beta_2, \\\\ldots, \\\\beta_n \\\\) are the coefficients, and \\\\( x_1, x_2, \\\\ldots, x_n \\\\) are the independent variables.\\n\\n- **Logistic Regression Sigmoid Function**\\n\\\\( \\\\sigma(z) = \\\\frac{1}{1 + e^{-z} \\\\)\\nwhere \\\\( z \\\\) is the linear combination of input features.\\n\\n- **Accuracy for Classification**\\n\\\\( \\\\text{Accuracy} = \\\\frac{\\\\text{Number of Correct Predictions}{\\\\text{Total Number of Predictions} \\\\)\\n\\n[[Examples]]\\n1. **Predicting House Prices**: A real estate company wants to predict house prices based on features such as size, location, and number of bedrooms. They use linear regression to model the relationship between these features and the house prices, allowing them to estimate the price of a new property.\\n\\n2. **Email Spam Detection**: An email service provider uses classification techniques to identify spam emails. By analyzing features such as the presence of certain keywords, the frequency of links, and the sender's address, the model classifies incoming emails as either 'spam' or 'not spam'.\\n\\n[[Reflective Questions]]\\n1. What are the main differences between regression and classification techniques?\\n2. How does the choice of features affect the performance of a regression model?\\n3. Why might logistic regression be preferred over linear regression for binary classification tasks?\\n4. How can overfitting be addressed in regression and classification models?\\n5. What are some real-world applications where classification techniques are particularly useful?\"}]}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:48:00.717\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - \u001b[0m\n",
      "\u001b[32m2025-04-13 08:48:02.154\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 8: Find recommended reading resources for each selected topic by using the reading resources tool with the topics list as input.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:48:46.596\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking RecRead Tool with args: {'topics': [{'topic': 'Predictive Modeling', 'page_id': '1d46ccbb-ecba-8124-92ca-ec4db74ccd33', 'content': '[[Introduction]]\\nPredictive modeling involves using statistical techniques to create models that can predict future outcomes based on historical data. These models are essential in various fields such as finance, healthcare, and marketing, where forecasting future trends and behaviors is crucial for strategic planning and decision-making.\\n\\n[[Key Definitions]]\\n- **Predictive Model**: A mathematical model or algorithm that uses historical data to predict future outcomes.\\n- **Training Data**: A dataset used to train a predictive model, allowing it to learn patterns and relationships.\\n- **Overfitting**: A modeling error that occurs when a model is too complex and captures noise in the data rather than the underlying pattern.\\n\\n[[Relevant Formulas]]\\n**Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\ldots + \\\\beta_n x_n + \\\\epsilon \\\\)\\n\\n**Logistic Regression Probability**\\n\\\\( P(Y=1) = \\\\frac{1}{1 + e^{-(\\\\beta_0 + \\\\beta_1 x_1 + \\\\ldots + \\\\beta_n x_n)} \\\\)\\n\\n**Mean Squared Error (MSE)**\\n\\\\( \\\\text{MSE} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 \\\\)\\n\\n[[Examples]]\\n1. **Credit Scoring**: Banks use predictive modeling to assess the creditworthiness of loan applicants. By analyzing historical data on borrowers, banks can predict the likelihood of a new applicant defaulting on a loan.\\n2. **Healthcare Diagnostics**: Predictive models are used to forecast the likelihood of patients developing certain diseases based on their medical history and lifestyle factors. This helps in early intervention and personalized treatment plans.\\n\\n[[Reflective Questions]]\\n1. How does predictive modeling differ from descriptive analytics?\\n2. What are the potential risks of overfitting in predictive modeling, and how can they be mitigated?\\n3. Why is it important to validate a predictive model with a separate test dataset?\\n4. How can predictive modeling be applied to improve customer retention strategies in businesses?\\n5. What ethical considerations should be taken into account when using predictive models in decision-making processes?'}, {'topic': 'Protein Solubility Prediction', 'page_id': '1d46ccbb-ecba-8132-b4ce-c0d62761c0ef', 'content': '[[Introduction]]\\nProtein solubility prediction is a crucial aspect of biotechnology and pharmaceutical research, as it helps in understanding how proteins behave in different environments. Soluble proteins are essential for various biological functions and industrial applications. Predicting solubility can aid in protein engineering and drug design.\\n\\n[[Key Definitions]]\\n- **Protein Solubility**: The ability of a protein to dissolve in a solvent, forming a homogeneous solution at a molecular level.\\n- **Hydrophobicity**: A measure of how a molecule repels water; in proteins, hydrophobic regions tend to be buried inside the structure.\\n- **Amino Acid Composition**: The specific sequence and proportion of amino acids in a protein, which significantly influences its solubility.\\n\\n[[Relevant Formulas]]\\n- **Solubility Prediction Formula**\\n  \\\\( S = \\\\sum_{i=1}^{20} f_i \\\\times H_i \\\\)\\n  Where \\\\( S \\\\) is the solubility score, \\\\( f_i \\\\) is the fraction of the \\\\( i \\\\)-th amino acid, and \\\\( H_i \\\\) is the hydrophobicity index of the \\\\( i \\\\)-th amino acid.\\n\\n- **Hydrophobicity Index Calculation**\\n  \\\\( H = \\\\frac{\\\\sum_{i=1}^{n} H_i}{n} \\\\)\\n  Where \\\\( H \\\\) is the average hydrophobicity, \\\\( H_i \\\\) is the hydrophobicity of the \\\\( i \\\\)-th amino acid, and \\\\( n \\\\) is the total number of amino acids.\\n\\n[[Examples]]\\n1. **Biopharmaceuticals**: In drug development, predicting protein solubility is vital for ensuring that therapeutic proteins can be effectively delivered in the body. For instance, insulin must remain soluble to be administered as an injectable drug.\\n\\n2. **Food Industry**: Enzymes used in food processing, such as those in cheese production, need to be soluble to function properly. Predicting solubility helps in selecting the right enzymes for specific food products.\\n\\n[[Reflective Questions]]\\n1. Why is protein solubility important in the pharmaceutical industry?\\n2. How does the hydrophobicity of a protein affect its solubility?\\n3. What role does amino acid composition play in determining protein solubility?\\n4. Can you think of a scenario where low protein solubility might be beneficial?\\n5. How might changes in environmental conditions (e.g., pH, temperature) impact protein solubility?'}, {'topic': 'Regression and Classification Techniques', 'page_id': '1d46ccbb-ecba-8140-a3d2-c2dcb0f52836', 'content': \"[[Introduction]]\\nRegression and classification are two fundamental types of predictive modeling techniques used in machine learning and statistics. Regression is used to predict continuous outcomes, while classification is used to predict discrete categories. Understanding these techniques is crucial for analyzing data and making informed decisions based on patterns identified in datasets.\\n\\n[[Key Definitions]]\\n- **Regression**: A statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to predict continuous outcomes.\\n- **Classification**: A process in machine learning where the model assigns a category label to input data based on its features. It is used for predicting discrete outcomes.\\n- **Linear Regression**: A type of regression analysis where the relationship between the dependent variable and independent variables is modeled as a linear equation.\\n\\n[[Relevant Formulas]]\\n- **Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\cdots + \\\\beta_n x_n \\\\)\\nwhere \\\\( y \\\\) is the dependent variable, \\\\( \\\\beta_0 \\\\) is the intercept, \\\\( \\\\beta_1, \\\\beta_2, \\\\ldots, \\\\beta_n \\\\) are the coefficients, and \\\\( x_1, x_2, \\\\ldots, x_n \\\\) are the independent variables.\\n\\n- **Logistic Regression Sigmoid Function**\\n\\\\( \\\\sigma(z) = \\\\frac{1}{1 + e^{-z} \\\\)\\nwhere \\\\( z \\\\) is the linear combination of input features.\\n\\n- **Accuracy for Classification**\\n\\\\( \\\\text{Accuracy} = \\\\frac{\\\\text{Number of Correct Predictions}{\\\\text{Total Number of Predictions} \\\\)\\n\\n[[Examples]]\\n1. **Predicting House Prices**: A real estate company wants to predict house prices based on features such as size, location, and number of bedrooms. They use linear regression to model the relationship between these features and the house prices, allowing them to estimate the price of a new property.\\n\\n2. **Email Spam Detection**: An email service provider uses classification techniques to identify spam emails. By analyzing features such as the presence of certain keywords, the frequency of links, and the sender's address, the model classifies incoming emails as either 'spam' or 'not spam'.\\n\\n[[Reflective Questions]]\\n1. What are the main differences between regression and classification techniques?\\n2. How does the choice of features affect the performance of a regression model?\\n3. Why might logistic regression be preferred over linear regression for binary classification tasks?\\n4. How can overfitting be addressed in regression and classification models?\\n5. What are some real-world applications where classification techniques are particularly useful?\"}]}\u001b[0m\n",
      "Processing topic: Predictive Modeling\n",
      "Processing topic: Protein Solubility Prediction\n",
      "Processing topic: Regression and Classification Techniques\n",
      "\u001b[32m2025-04-13 08:48:57.044\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - ✅ Recommended Reading added to Notion pages successfully.\u001b[0m\n",
      "\u001b[32m2025-04-13 08:48:58.513\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 9: Create quizzes on each of the selected topics by passing the topics list to the quiz creation function.\u001b[0m | {'plan': 'plan-6c6a618c-a179-4b0b-8464-cce8af0b38ee', 'plan_run': 'prun-a587cefc-258f-40f9-8b45-8d66356c24e2'}\n",
      "\u001b[32m2025-04-13 08:49:28.993\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Quiz Tool with args: {'topics': [{'topic': 'Predictive Modeling', 'page_id': '1d46ccbb-ecba-8124-92ca-ec4db74ccd33', 'content': '[[Introduction]]\\nPredictive modeling involves using statistical techniques to create models that can predict future outcomes based on historical data. These models are essential in various fields such as finance, healthcare, and marketing, where forecasting future trends and behaviors is crucial for strategic planning and decision-making.\\n\\n[[Key Definitions]]\\n- **Predictive Model**: A mathematical model or algorithm that uses historical data to predict future outcomes.\\n- **Training Data**: A dataset used to train a predictive model, allowing it to learn patterns and relationships.\\n- **Overfitting**: A modeling error that occurs when a model is too complex and captures noise in the data rather than the underlying pattern.\\n\\n[[Relevant Formulas]]\\n**Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\ldots + \\\\beta_n x_n + \\\\epsilon \\\\)\\n\\n**Logistic Regression Probability**\\n\\\\( P(Y=1) = \\\\frac{1}{1 + e^{-(\\\\beta_0 + \\\\beta_1 x_1 + \\\\ldots + \\\\beta_n x_n)} \\\\)\\n\\n**Mean Squared Error (MSE)**\\n\\\\( \\\\text{MSE} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2 \\\\)\\n\\n[[Examples]]\\n1. **Credit Scoring**: Banks use predictive modeling to assess the creditworthiness of loan applicants. By analyzing historical data on borrowers, banks can predict the likelihood of a new applicant defaulting on a loan.\\n2. **Healthcare Diagnostics**: Predictive models are used to forecast the likelihood of patients developing certain diseases based on their medical history and lifestyle factors. This helps in early intervention and personalized treatment plans.\\n\\n[[Reflective Questions]]\\n1. How does predictive modeling differ from descriptive analytics?\\n2. What are the potential risks of overfitting in predictive modeling, and how can they be mitigated?\\n3. Why is it important to validate a predictive model with a separate test dataset?\\n4. How can predictive modeling be applied to improve customer retention strategies in businesses?\\n5. What ethical considerations should be taken into account when using predictive models in decision-making processes?'}, {'topic': 'Protein Solubility Prediction', 'page_id': '1d46ccbb-ecba-8132-b4ce-c0d62761c0ef', 'content': '[[Introduction]]\\nProtein solubility prediction is a crucial aspect of biotechnology and pharmaceutical research, as it helps in understanding how proteins behave in different environments. Soluble proteins are essential for various biological functions and industrial applications. Predicting solubility can aid in protein engineering and drug design.\\n\\n[[Key Definitions]]\\n- **Protein Solubility**: The ability of a protein to dissolve in a solvent, forming a homogeneous solution at a molecular level.\\n- **Hydrophobicity**: A measure of how a molecule repels water; in proteins, hydrophobic regions tend to be buried inside the structure.\\n- **Amino Acid Composition**: The specific sequence and proportion of amino acids in a protein, which significantly influences its solubility.\\n\\n[[Relevant Formulas]]\\n- **Solubility Prediction Formula**\\n  \\\\( S = \\\\sum_{i=1}^{20} f_i \\\\times H_i \\\\)\\n  Where \\\\( S \\\\) is the solubility score, \\\\( f_i \\\\) is the fraction of the \\\\( i \\\\)-th amino acid, and \\\\( H_i \\\\) is the hydrophobicity index of the \\\\( i \\\\)-th amino acid.\\n\\n- **Hydrophobicity Index Calculation**\\n  \\\\( H = \\\\frac{\\\\sum_{i=1}^{n} H_i}{n} \\\\)\\n  Where \\\\( H \\\\) is the average hydrophobicity, \\\\( H_i \\\\) is the hydrophobicity of the \\\\( i \\\\)-th amino acid, and \\\\( n \\\\) is the total number of amino acids.\\n\\n[[Examples]]\\n1. **Biopharmaceuticals**: In drug development, predicting protein solubility is vital for ensuring that therapeutic proteins can be effectively delivered in the body. For instance, insulin must remain soluble to be administered as an injectable drug.\\n\\n2. **Food Industry**: Enzymes used in food processing, such as those in cheese production, need to be soluble to function properly. Predicting solubility helps in selecting the right enzymes for specific food products.\\n\\n[[Reflective Questions]]\\n1. Why is protein solubility important in the pharmaceutical industry?\\n2. How does the hydrophobicity of a protein affect its solubility?\\n3. What role does amino acid composition play in determining protein solubility?\\n4. Can you think of a scenario where low protein solubility might be beneficial?\\n5. How might changes in environmental conditions (e.g., pH, temperature) impact protein solubility?'}, {'topic': 'Regression and Classification Techniques', 'page_id': '1d46ccbb-ecba-8140-a3d2-c2dcb0f52836', 'content': \"[[Introduction]]\\nRegression and classification are two fundamental types of predictive modeling techniques used in machine learning and statistics. Regression is used to predict continuous outcomes, while classification is used to predict discrete categories. Understanding these techniques is crucial for analyzing data and making informed decisions based on patterns identified in datasets.\\n\\n[[Key Definitions]]\\n- **Regression**: A statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to predict continuous outcomes.\\n- **Classification**: A process in machine learning where the model assigns a category label to input data based on its features. It is used for predicting discrete outcomes.\\n- **Linear Regression**: A type of regression analysis where the relationship between the dependent variable and independent variables is modeled as a linear equation.\\n\\n[[Relevant Formulas]]\\n- **Linear Regression Equation**\\n\\\\( y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\cdots + \\\\beta_n x_n \\\\)\\nwhere \\\\( y \\\\) is the dependent variable, \\\\( \\\\beta_0 \\\\) is the intercept, \\\\( \\\\beta_1, \\\\beta_2, \\\\ldots, \\\\beta_n \\\\) are the coefficients, and \\\\( x_1, x_2, \\\\ldots, x_n \\\\) are the independent variables.\\n\\n- **Logistic Regression Sigmoid Function**\\n\\\\( \\\\sigma(z) = \\\\frac{1}{1 + e^{-z} \\\\)\\nwhere \\\\( z \\\\) is the linear combination of input features.\\n\\n- **Accuracy for Classification**\\n\\\\( \\\\text{Accuracy} = \\\\frac{\\\\text{Number of Correct Predictions}{\\\\text{Total Number of Predictions} \\\\)\\n\\n[[Examples]]\\n1. **Predicting House Prices**: A real estate company wants to predict house prices based on features such as size, location, and number of bedrooms. They use linear regression to model the relationship between these features and the house prices, allowing them to estimate the price of a new property.\\n\\n2. **Email Spam Detection**: An email service provider uses classification techniques to identify spam emails. By analyzing features such as the presence of certain keywords, the frequency of links, and the sender's address, the model classifies incoming emails as either 'spam' or 'not spam'.\\n\\n[[Reflective Questions]]\\n1. What are the main differences between regression and classification techniques?\\n2. How does the choice of features affect the performance of a regression model?\\n3. Why might logistic regression be preferred over linear regression for binary classification tasks?\\n4. How can overfitting be addressed in regression and classification models?\\n5. What are some real-world applications where classification techniques are particularly useful?\"}]}\u001b[0m\n",
      "\u001b[32m2025-04-13 08:50:00.221\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - Quizzes created successfully!\u001b[0m\n",
      "\u001b[32m2025-04-13 08:50:05.450\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[37mportia.portia\u001b[0m:\u001b[37m_log_final_output\u001b[0m:\u001b[37m618\u001b[0m - \u001b[1mFinal output: The research assistant executed a series of tasks to support a query on protein modeling. Initially, a paper titled \"Develop machine learning based predictive models for engineering protein solubility\" was downloaded and its full text extracted. Core topics identified included machine learning algorithms, predictive modeling, and protein solubility prediction. From these, three key topics were selected: Predictive Modeling, Protein Solubility Prediction, and Regression and Classification Techniques. Notion pages were created for these topics, and quizzes were developed. However, no YouTube videos were found, but recommended readings were added to the Notion pages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "constraints = []\n",
    "# topic = input(\"What topic are you interested in covering today?\")\n",
    "# number_of_papers = int(input(\"How many papers do you want to download?\"))\n",
    "topic = \"Protein Modelling\"\n",
    "video_preference = True\n",
    "rec_reading_preference = True\n",
    "quiz_preference = True\n",
    "\n",
    "# Define the path to your papers folder\n",
    "papers_folder = \"papers\"\n",
    "\n",
    "# Ensure the papers folder exists\n",
    "if not os.path.exists(papers_folder):\n",
    "    os.makedirs(papers_folder)\n",
    "else:\n",
    "    # If it exists, clear its contents\n",
    "    for filename in os.listdir(papers_folder):\n",
    "        file_path = os.path.join(papers_folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # remove file or symlink\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # remove directory\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "\n",
    "task = (\n",
    "            lambda : f\"\"\"You are a research assistant running these tasks: \n",
    "                      - Find and download 1 paper on the topic of {topic} using the ArXivTool. \n",
    "                      - Run the PDFReaderTool to extract the full text from the pdfs in the local folder.\n",
    "                      - Use PSTool to create and populate the Page Summary subpage.\n",
    "                      - From the text, extract the core mathematical and scientific concepts required \n",
    "                        to understand the paper. Focus only on generalizable topics that could be included \n",
    "                        in a learning pathway or curriculum—avoid content specific to the study's location, \n",
    "                        data, or outcomes. List only the overarching topics, with no explanations or extra text.\n",
    "                      - Then use the TopicSelectorTool on these topics. \n",
    "                      - Then use the Notion Tool to create Notion pages for these topics.\n",
    "\n",
    "                      - {video_preference * \"Use the YouTubeTool to find videos on each topic.\"}\n",
    "                      - {rec_reading_preference * \"Use the RecReadTool to find resources on each topic.\"}\n",
    "                      - {quiz_preference * \"Use the QuizTool to create quizzes on each topic.\"}\n",
    "                      \n",
    "                      \n",
    "                        Take into account these constraints: {constraints}\n",
    "                      \"\"\"\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# Iterate on the plan with the user until they are happy with it\n",
    "with execution_context(end_user_id=\"learning_enthusiast\",):\n",
    "    plan = portia.plan(task())\n",
    "    print(\"\\nHere are the steps in the generated plan:\")\n",
    "    [print(step.model_dump_json(indent=2)) for step in plan.steps]\n",
    "    ready_to_proceed = False\n",
    "    while not ready_to_proceed:\n",
    "        user_input = input(\"Are you happy with the plan? (y/n):\\n\")\n",
    "        if user_input == \"y\":\n",
    "            ready_to_proceed = True\n",
    "        else:\n",
    "            user_input = input(\"Any additional guidance for the planner?:\\n\")\n",
    "            constraints.append(user_input)\n",
    "            plan = portia.plan(task())\n",
    "            print(\"\\nHere are the updated steps in the plan:\")\n",
    "            [print(step.model_dump_json(indent=2)) for step in plan.steps]\n",
    "\n",
    "    # Execute the plan\n",
    "    print(\"\\nThe plan will now be executed. Please wait...\")\n",
    "    run = portia.run_plan(plan)\n",
    "    \n",
    "    if run.state != PlanRunState.COMPLETE:\n",
    "        raise Exception(\n",
    "            f\"Plan run failed with state {run.state}. Check logs for details.\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
