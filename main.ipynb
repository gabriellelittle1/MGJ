{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from portia.cli import CLIExecutionHooks\n",
    "from portia import *\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Generic, TypeVar, List, Literal\n",
    "import os\n",
    "from notion_client import Client\n",
    "from my_custom_tools.registry import custom_tool_registry\n",
    "import shutil\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Fetch the Notion API key and set up client\n",
    "notion_api_key = os.getenv(\"NOTION_API_KEY\")\n",
    "notion_parent_id = os.getenv(\"NOTION_PARENT_ID\")\n",
    "\n",
    "# Initialize the Notion client\n",
    "notion = Client(auth=notion_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = Config.from_default()\n",
    "complete_tool_registry = PortiaToolRegistry(my_config) + custom_tool_registry\n",
    "\n",
    "portia = Portia(config = my_config,\n",
    "                tools = complete_tool_registry,\n",
    "                execution_hooks=CLIExecutionHooks(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 06:10:50.486\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;39mportia.portia\u001b[0m:\u001b[38;5;39mplan\u001b[0m:\u001b[38;5;39m197\u001b[0m - \u001b[1mRunning planning_agent for query - You are a research assistant running these tasks: \n",
      "                      - Find and download 1 paper on the topic of Holodiffusion using the ArXivTool. \n",
      "                      - Run the PDFReaderTool to extract the full text from the pdfs in the local folder.\n",
      "                      - Use PSTool to create and populate the Page Summary subpage.\n",
      "                      - From the text, extract the core mathematical and scientific concepts required \n",
      "                        to understand the paper. Focus only on generalizable topics that could be included \n",
      "                        in a learning pathway or curriculum—avoid content specific to the study's location, \n",
      "                        data, or outcomes. List only the overarching topics, with no explanations or extra text.\n",
      "                      - Then use the TopicSelectorTool on these topics. \n",
      "                      - Then use the Notion Tool to create Notion pages for these topics.\n",
      "\n",
      "                      - Use the YouTubeTool to find videos on each topic.\n",
      "                      - Use the RecReadTool to find resources on each topic.\n",
      "                      - Use the QuizTool to create quizzes on each topic.\n",
      "\n",
      "\n",
      "                        Take into account these constraints: []\n",
      "                      \u001b[0m\n",
      "\u001b[32m2025-04-13 06:11:08.359\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;39mportia.portia\u001b[0m:\u001b[38;5;39mplan\u001b[0m:\u001b[38;5;39m222\u001b[0m - \u001b[1mPlan created with 10 steps\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1'}\n",
      "\n",
      "Here are the steps in the generated plan:\n",
      "{\n",
      "  \"task\": \"Find 1 paper on the topic of Holodiffusion using the provided service.\",\n",
      "  \"inputs\": [],\n",
      "  \"tool_id\": \"arxiv_tool\",\n",
      "  \"output\": \"$arxiv_paper\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Download the paper retrieved in the previous step.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$arxiv_paper\",\n",
      "      \"description\": \"The paper details obtained from the search on Holodiffusion.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"download_tool\",\n",
      "  \"output\": \"$downloaded_paper\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Extract the full text from the PDFs available in the local folder.\",\n",
      "  \"inputs\": [],\n",
      "  \"tool_id\": \"pdf_reader_tool\",\n",
      "  \"output\": \"$pdf_texts\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Create and populate the Page Summary subpage using the paper information and the extracted full text.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$arxiv_paper\",\n",
      "      \"description\": \"The paper details retrieved for Holodiffusion.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"$pdf_texts\",\n",
      "      \"description\": \"The full text extracted from the downloaded PDFs.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"pstool\",\n",
      "  \"output\": \"$page_summary\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Extract the core mathematical and scientific concepts from the full text focusing on generalizable topics for a learning pathway.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$pdf_texts\",\n",
      "      \"description\": \"The full text of the paper for extracting core concepts.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"llm_tool\",\n",
      "  \"output\": \"$core_topics\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Select the topics from the extracted core concepts using the provided topic selection service.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$core_topics\",\n",
      "      \"description\": \"The core generalizable topics extracted from the paper text.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"topic_selector_tool\",\n",
      "  \"output\": \"$selected_topics\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Create pages for the selected topics in the knowledge management tool.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$selected_topics\",\n",
      "      \"description\": \"The list of topics chosen from the extraction step.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"notion_tool\",\n",
      "  \"output\": \"$notion_pages\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Find relevant videos for each topic based on the created pages.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$notion_pages\",\n",
      "      \"description\": \"The Notion pages created for each selected topic.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"youtube_tool\",\n",
      "  \"output\": \"$youtube_videos\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Identify recommended resources for each topic linked to the created pages.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$notion_pages\",\n",
      "      \"description\": \"The Notion pages for each selected topic.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"RecRead_tool\",\n",
      "  \"output\": \"$rec_resources\",\n",
      "  \"condition\": null\n",
      "}\n",
      "{\n",
      "  \"task\": \"Generate quizzes for each topic based on the created pages.\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"$notion_pages\",\n",
      "      \"description\": \"The Notion pages containing topic details for quiz creation.\"\n",
      "    }\n",
      "  ],\n",
      "  \"tool_id\": \"Quiz_tool\",\n",
      "  \"output\": \"$quiz_pages\",\n",
      "  \"condition\": null\n",
      "}\n",
      "\n",
      "The plan will now be executed. Please wait...\n",
      "\u001b[32m2025-04-13 06:11:15.122\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m528\u001b[0m - \u001b[1mPlan Run State is updated to PlanRunState.IN_PROGRESS. View in your Portia AI dashboard: https://app.portialabs.ai/dashboard/plan-runs?plan_run_id=prun-903a1d6a-ffbe-408a-9e81-853bdb094723\u001b[0m\n",
      "\u001b[32m2025-04-13 06:11:15.124\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 0: Find 1 paper on the topic of Holodiffusion using the provided service.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:11:17.682\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking arXiv Tool with args: {'topic': 'Holodiffusion'}\u001b[0m\n",
      "\u001b[32m2025-04-13 06:11:20.852\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [{\"title\": \"HoloFusion: Towards Photo-realistic 3D Generative Modeling\", \"summary\": \"Diffusion-based image generators can now produce high-quality and diverse\\nsamples, but their success has yet to fully translate to 3D generation:\\nexisting diffusion methods can either generate low-resolution but 3D consistent\\noutputs, or detailed 2D views of 3D objects but with potential structural\\ndefects and lacking view consistency or realism. We present HoloFusion, a\\nmethod that combines the best of these approaches to produce high-fidelity,\\nplausible, and diverse 3D samples while learning from a collection of\\nmulti-view 2D images only. The method first generates coarse 3D samples using a\\nvariant of the recently proposed HoloDiffusion generator. Then, it\\nindependently renders and upsamples a large number of views of the coarse 3D\\nmodel, super-resolves them to add detail, and distills those into a single,\\nhigh-fidelity implicit 3D representation, which also ensures view consistency\\nof the final renders. The super-resolution network is trained as an integral\\npart of HoloFusion, end-to-end, and the final distillation uses a new sampling\\nscheme to capture the space of super-resolved signals. We compare our method\\nagainst existing baselines, including DreamFusion, Get3D, EG3D, and\\nHoloDiffusion, and achieve, to the best of our knowledge, the most realistic\\nresults on the challenging CO3Dv2 dataset.\", \"link\": \"https://arxiv.org/pdf/2308.14244v1.pdf\"}]\u001b[0m\n",
      "\u001b[32m2025-04-13 06:11:22.265\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 1: Download the paper retrieved in the previous step.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:11:32.632\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Download Tool with args: {'papers': [{'title': 'HoloFusion: Towards Photo-realistic 3D Generative Modeling', 'link': 'https://arxiv.org/pdf/2308.14244v1.pdf', 'summary': 'Diffusion-based image generators can now produce high-quality and diverse samples, but their success has yet to fully translate to 3D generation: existing diffusion methods can either generate low-resolution but 3D consistent outputs, or detailed 2D views of 3D objects but with potential structural defects and lacking view consistency or realism. We present HoloFusion, a method that combines the best of these approaches to produce high-fidelity, plausible, and diverse 3D samples while learning from a collection of multi-view 2D images only. The method first generates coarse 3D samples using a variant of the recently proposed HoloDiffusion generator. Then, it independently renders and upsamples a large number of views of the coarse 3D model, super-resolves them to add detail, and distills those into a single, high-fidelity implicit 3D representation, which also ensures view consistency of the final renders. The super-resolution network is trained as an integral part of HoloFusion, end-to-end, and the final distillation uses a new sampling scheme to capture the space of super-resolved signals. We compare our method against existing baselines, including DreamFusion, Get3D, EG3D, and HoloDiffusion, and achieve, to the best of our knowledge, the most realistic results on the challenging CO3Dv2 dataset.'}]}\u001b[0m\n",
      "ℹ️ Downloading 'HoloFusion Towards Photo-realistic 3D Generative Modeling' from https://arxiv.org/pdf/2308.14244v1.pdf\n",
      "\u001b[32m2025-04-13 06:11:38.573\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - ✅ Downloaded 1 paper into the 'papers' folder\u001b[0m\n",
      "\u001b[32m2025-04-13 06:11:40.003\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 2: Extract the full text from the PDFs available in the local folder.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:11:42.566\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking PDF reader tool with args: {}\u001b[0m\n",
      "Stopping at page 9 (found References section).\n",
      "\u001b[32m2025-04-13 06:11:45.547\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - {\"HoloFusion_Towards_Photo-.pdf\": \"--- Page 1 ---\\nHOLOFUSION: Towards Photo-realistic 3D Generative Modeling\\nAnimesh Karnewar\\nUCL\\na.karnewar@ucl.ac.uk\\nNiloy J. Mitra\\nUCL\\nn.mitra@ucl.ac.uk\\nAndrea Vedaldi\\nMeta AI\\nvedaldi@meta.com\\nDavid Novotny\\nMeta AI\\ndnovotny@meta.com\\nHigh-quality 3D shape\\nDistillation loss\\nHoloDiffusion samples\\nHoloFusion high quality samples\\nDistillation\\nEA Rendering\\nFigure 1: We propose HOLOFUSION to generate photo-realistic 3D radiance fields by extending the HoloDiffusion method\\nwith a jointly trained 2D ‘super resolution’ network. The independently super-resolved images are fused back into the 3D\\nrepresentation to improve the fidelity of the 3D model via distillation, while preserving the consistency across view changes.\\nAbstract\\nDiffusion-based image generators can now produce\\nhigh-quality and diverse samples, but their success has yet\\nto fully translate to 3D generation: existing diffusion meth-\\nods can either generate low-resolution but 3D consistent\\noutputs, or detailed 2D views of 3D objects but with poten-\\ntial structural defects and lacking view consistency or real-\\nism. We present HOLOFUSION, a method that combines the\\nbest of these approaches to produce high-fidelity, plausible,\\nand diverse 3D samples while learning from a collection\\nof multi-view 2D images only. The method first generates\\ncoarse 3D samples using a variant of the recently proposed\\nHoloDiffusion generator.\\nThen, it independently renders\\nand upsamples a large number of views of the coarse 3D\\nmodel, super-resolves them to add detail, and distills those\\ninto a single, high-fidelity implicit 3D representation, which\\nalso ensures view-consistency of the final renders.\\nThe\\nsuper-resolution network is trained as an integral part of\\nHOLOFUSION, end-to-end, and the final distillation uses a\\nnew sampling scheme to capture the space of super-resolved\\nsignals. We compare our method against existing baselines,\\nincluding DreamFusion, Get3D, EG3D, and HoloDiffusion,\\nand achieve, to the best of our knowledge, the most realistic\\nresults on the challenging CO3Dv2 dataset.\\n1. Introduction\\nDiffusion models [32, 7, 31] are at the basis of state-of-\\nthe-art 2D image generators which can now produce very\\nhigh-quality and diverse outputs. However, their success\\nhas yet to be translated to 3D and there is no generator that\\ncan produce 3D assets of a comparable quality.\\nRecent attempts at extending diffusion to 3D generation\\nhave reported mixed success. Some authors have attempted\\nto apply diffusion directly in 3D [18], or still in 2D but us-\\ning a 3D-aware neural network [42, 1]. This requires solv-\\ning two problems: first, finding a suitable 3D representa-\\ntion (e.g., triplane features [4], mesh [19], voxels [18]) that\\nscales well with resolution and is amenable to diffusion;\\n\\n--- Page 2 ---\\nand, second, obtaining a large amount of 3D training data,\\nfor example using synthetic models [41, 27], or training the\\nmodel using only 2D images [18], often via differentiable\\n(volume) rendering [13, 26]. However, the quality of results\\nso far is limited, especially when training on real images.\\nOther authors have proposed to distill 3D objects from\\npre-trained 2D image generators. For instance Score Distil-\\nlation Sampling (SDS) [29] can sample 3D objects from a\\nhigh-quality off-the-shelf 2D diffusion model while requir-\\ning no (re)training. However, without any 3D guidance, dis-\\ntillation methods often produce implausible results; for ex-\\nample, they suffer from the ‘Janus effect’, where details of\\nthe front of the object are replicated on its back. They also\\ncreate overly-smooth outputs that average out inconsisten-\\ncies arising from the fact that the signal obtained from the\\n2D model is analogous to sampling independent views of\\nthe object (see Sec. 4.2 for examples). Furthermore, distil-\\nlation methods do not support unconditional sampling, even\\nif the underlying image generator does, as strong language\\nguidance is required to stabilise the 3D reconstruction.\\nIn this work, we propose HOLOFUSION, a method that\\ncombines the best of both approaches.\\nWe start from\\nHoloDiffusion [18], a diffusion-based 3D generator. This\\nmodel can be trained using only a multiview image dataset\\nlike [30] and produces outputs that are 3D consistent. How-\\never, the output resolution is limited by computation and\\nmemory. We augment the base model with a lightweight\\nsuper-resolution network that upscales the initial renders.\\nCrucially, the 2D super-resolution model is integrated and\\ntrained jointly with the 3D generator, end-to-end.\\nThe super-resolution network outputs detailed views of\\nthe 3D object, and the underlying 3D generator ensures\\nthat the coarse structure of these views is indeed consis-\\ntent (e.g., avoiding the Janus effect and other structural arti-\\nfacts). However, the 2D upscaling still progresses indepen-\\ndently for different views, which means that fine grained\\ndetails may still be inconsistent between views. We address\\nthis issue by distilling a single, coherent, high quality 3D\\nmodel of the object from the output of the upsampler. For\\nthis, we propose a new distillation technique that efficiently\\ncombines several putative super-resolved views of the ob-\\nject into a single, coherent 3D reconstruction.\\nWith this, we are able to train a high-quality 3D genera-\\ntor model purely from real 2D data. This model is capable\\nof generating consistent and detailed 3D objects, which in\\nturn result in view-consistent renderings (see Fig. 1) at a\\nquality not achievable by prior methods.\\nWe evaluate HOLOFUSION on real images (CO3Dv2\\ndataset [30]) and compare with a variety of compet-\\ning alternatives (e.g., HoloDiffusion [18], Get3D [9],\\nEG3D [4], DreamFusion [40]) demonstrating that view-\\nconsistent high-quality 3D generation is possible using our\\nsimple, effective, easy-to-implement hybrid approach.\\n2. Related Work\\n3D generators that use adversarial learning.\\nGenera-\\ntive Adversarial Learning (GAN) [10] learns a generator\\nnetwork so that its “fake” samples cannot be distinguished\\nfrom real images by a second discriminator network. Ap-\\nproaches such as PlatonicGAN [13], HoloGAN [28], and\\nPrGAN [8] introduced 3D structure into the generator net-\\nwork, achieving 3D shape generation with only image-level\\nsupervision. Our method is related to those as it renders\\nimages from a generated voxel grid, as well as to Holo-\\nGAN [28], which renders features and then converts them\\ninto an image by a lightweight 2D convolutional network.\\nOther voxel-based 3D generators include VoxGRAF [34]\\nand NeuralVolumes [21].\\nMore recently, 3D generators have built on neural ra-\\ndiance fields [26]. GRAF [33] was the first to adopt the\\nNeRF framework; analogous to PlatonicGAN, they gener-\\nate the parameters of an MLP which renders realistic im-\\nages of the object from a random viewpoint. This idea has\\nbeen improved in StyleNeRF [11] and EG3D [4] by adding\\na 2D convolutional post-processing step after emission-\\nabsorption rendering, which is analogous to our super-\\nresolution network.\\nEG3D also introduced a novel ‘tri-\\nplane’ representation of the radiance field which, in a mem-\\nory efficient manner, factorises the latter into a triplet of 2D\\nfeature planes. EG3D inspired several improvements such\\nas GAUDI [2] and EpiGRAF [37].\\nMesh-based 3D generators have been explored in [43].\\nRecently, GET3D [9] replaced the radiance field with a\\nsigned distance function to regularise the representation of\\ngeometry. The latter is converted into a mesh and rendered\\nin a differentiable manner by using the marching tetrahedral\\nrepresentation [35].\\nModeling 3D with diffusion.\\nDiffusion methods [38]\\nhave recently became the go-to framework for generative\\nmodeling of any kind, including 3D generative modeling.\\nThe first applications of diffusion to 3D considered point-\\ncloud generators trained on synthetic data [23, 45, 44].\\n3D distillation of 2D diffusion models.\\nMore recently,\\nDreamFusion [29] ported the idea of distillation to diffu-\\nsion models: they extract a neural radiance field so that\\nits renders match the belief of a pre-trained 2D diffusion\\ngenerator [32, 7, 31]. They introduce the Score Distillation\\nSampling (SDS) loss which makes distillation relatively ef-\\nficient (but still in the order of several minutes for a single\\n3D sample). Their generation can be conditioned by an im-\\nage or by a textual description, making the process rather\\nflexible. Magic3D [19] further increases the quality of the\\noutput by distilling a mesh-based 3D representation instead\\nof a radiance field.\\nImage-conditioned 3D diffusion.\\nThe idea of distillation\\nhas been applied to few-view conditioned reconstruction in\\n\\n--- Page 3 ---\\n[42, 12, 25, 46, 6]. SparseFusion [46] employs a 3D-based\\nnew-view synthesis model [39] followed by a 2D diffusion\\nupsampler. They complete the process by 3D distillation,\\nensuring that the generated views of the object are consis-\\ntent. NeRFDiff [12] and 3DiM [42] bypass an explicit 3D\\nmodel and directly generate new views of an object using a\\n2D image generator and, in the case of NeRFDiff, refine the\\nresults using distillation.\\nWhile SparseFusion and NeRFDiff need to be trained on\\na dataset of object-centric multi-view images with pose in-\\nformation, RealFusion [25] and NeRDi [6] can be used for\\nzero-shot monocular 3D reconstruction, starting from a pre-\\ntrained 2D diffusion model. Given a single image as in-\\nput, they automatically generate a prompt for the diffusion\\nmodel, using a form of prompt inversion, and then use dis-\\ntillation to extract a radiance field.\\nUnconditional generation.\\nMost relevantly to us, uncon-\\nditional generation, i.e., generation which does not require\\neither text or image conditioning, was explored in [41, 36,\\n27, 18]. While [27, 36, 41] train generators given synthetic\\n3D ground truth, similar to us, HoloDiffusion [18] is su-\\npervised only with real object-centric images and camera\\nposes. While HoloDiffusion was the first to demonstrate\\nsuccessful training on real image data, its renders contain\\nconsiderably lower amount of detail than samples from a\\nconventional 2D image generator that uses diffusion. We\\nthus leverage a 2D diffusion upsampler, conditioned on the\\nlower-fidelity HoloDiffusion renders, to distill higher reso-\\nlution images and, eventually, 3D models.\\n3. HOLOFUSION\\nWe present HOLOFUSION, a method that can learn a\\nhigh-quality diffusion-based 3D generator from a collection\\nof multiview 2D images. HOLOFUSION first obtains an un-\\nconditional low-resolution 3D sample using diffusion and\\nthen distills a high-resolution 3D radiance field represent-\\ning a higher-quality version of the generated object. We\\nfirst summarize the Denoising Diffusion Probabilistic Mod-\\nels (DDPMs) [16] that we utilize in Sec. 3.1. Then, we dis-\\ncuss the low-resolution 3D generator in Sec. 3.2 followed\\nby super-resolution distillation in Sec. 3.3.\\n3.1. Preliminaries: DDPMs\\nLet x = x0 be a random vector whose probability distri-\\nbution p(x|y) we seek to model. The DDPM [16] defines\\na hierarchy of latent variables xt, t = 0, . . . , T and an en-\\ncoder q comprising a sequence of Gaussian distributions\\nq(xt|xt−1) = N(xt; √αtxt−1, (1 −αt)I),\\n(1)\\nwhere αt, . . . , αT is a predefined ‘noising schedule’. Given\\nknowledge of x0, a sample xt can be drawn in a closed-form\\ndirectly from q(xt|x0) = N(xt; √¯αtx0, (1 −¯αt)I), where\\n¯αt = Qt\\ni=1 αi. Hence, we can express xt as:\\nxt = ˆϵt(x0) = √¯αtx0 +\\n√\\n1 −¯αtϵt\\n(2)\\nThe noising schedule is chosen such that ¯αT ≈0. In\\nthis manner, q(xT |x0) ≈N(xT ; 0, I) is approximately nor-\\nmal, and so is q(xT ). To generate a sample x = x0, we\\nstart by sampling xT from this normal distribution and then\\nsample the intermediate latent variables backward. This is\\ndone by using a variational approximation of the probabili-\\nties q(xt−1|xt) given by the Gaussian factors:\\np(xt−1|xt) = N(xt−1; √¯αt−1Dθ(xt, t), (1−¯αt−1)I) (3)\\nwhere Dθ is a neural network with parameters θ.\\nThe network Dθ is trained by maximizing the ELBO\\n(Evidence Lower Bound), which reduces to the denoising\\nobjective [22]:\\nL(θ) = Et,ϵ,x\\n\\u0014\\n¯αt−1\\n2(1 −¯αt−1)2 ∥Dθ(ˆϵt(x), t) −x∥2\\n\\u0015\\n, (4)\\nwhere t is sampled uniformly from U[1, T].\\nHence,\\nDθ(xt, t) approximates the clean sample x0 given the noisy\\nsample xt (obtained using (2)).\\n3.2. HoloDiffusion revisited\\nGiven a large dataset of 3D models, the framework of\\nSec. 3.1 could be used to train a corresponding probability\\ndistribution. However, such a dataset is not available, and\\nwe must instead learn from 2D images of physical 3D ob-\\njects. Given a dataset containing several views of a large\\nnumber of objects, we could use image-based reconstruc-\\ntion (e.g., using neural rendering) to obtain corresponding\\n3D models first, and then use those to train a diffusion\\nmodel. Instead, we adopt, and slightly upgrade, the HoloD-\\niffusion method [18], which learns a 3D diffusion model\\ndirectly from the 2D images.\\nTraining data.\\nHoloDiffusion learns from a collection D\\nof N image sequences si = (Ii\\nj, Ci\\nj)Nframe\\nj=1 , i = 1, . . . , N,\\nwhere frame Ii\\nj ∈R3×H×W is an RGB image and Ci\\nj ∈\\nR4×4 is the corresponding camera projection matrix, col-\\nlectively defining the motion of the camera.\\n3D representation and rendering.\\nThe shape and ap-\\npearance of the object are represented by a voxel grid V ∈\\nRd×S×S×S with resolution S containing a d-dimensional\\nfeature vector per voxel.\\nGiven a 3D point p ∈R3,\\nits opacity σ(p) ∈R+ and color c(p) ∈R3\\n[0,1] are ob-\\ntained from the voxel grid by an MLP Mη(V (p)) that takes\\nas input the d-dimensional feature vector V (p) extracted\\nform the grid via trilinear interpolation [20].\\nThe usual\\nemission-absorption model [26, 24] is then used to imple-\\nment a differentiable rendering function Rη, mapping the\\nvoxel grid V and the camera viewpoint C into an image\\nˆI = Rη(V, C), where η are the parameters of the MLP.\\n\\n--- Page 4 ---\\nNeural renderer\\nChannelwise concatenation\\nHoloDiffusion\\nDenoiser\\nUnprojected\\nfeatures\\n3D noise\\nFeature grid\\n2D noise\\nUpsampled render\\n2D Upsampling\\nDenoiser\\nGround truth\\nGround truth\\nLow-res render\\nCamera\\nFigure 2: Overview. HOLOFUSION, which trains the 3D denoiser network Dθ, is augmented with the 2D ‘super-resolution’\\ndiffusion model Dβ. Both models are trained end-to-end by supervising their outputs with 2D photometric error.\\nTraining scheme.\\nHoloDiffusion leverages the DDPM\\nframework (revised in the previous paragraphs) to recover\\nthe density p(V ) over voxel grids x = V encoding plau-\\nsible real-life objects. In order to train a DDPM on such\\n3D data, we would need access to ground-truth 3D models\\nV , which are not available. HoloDiffusion addresses this\\nproblem by making three changes to DDPM.\\nFirst, it replaces the data denoising loss with a photo-\\nmetric reconstruction loss. Given a pair (I, C) ∈s from\\none of the training sequences s, it replaces Eq. (4) with\\nEt,ϵ,C\\n\\u0002\\n∥I −Rη(Dθ(ˆϵt(V ), t), C)∥2\\u0003\\nwhere the goal is not\\nto reconstruct the ‘clean’ volume V (which is unknown),\\nbut rather its image I (which is known).\\nSecond, also because the ‘clean’ volume V is not avail-\\nable, we cannot use Eq. (3) to generate the noisy volumes Vt\\nto denoise; the only exception is the last sample VT , which\\nis pure noise. This suggests to adopt a ‘double denoising’\\nstep. First, pure noise VT is fed into the denoiser to ob-\\ntain an (approximate) version of V0 = Dθ(VT , T) of the\\nclean volume V0 = V . Then, noise is applied to obtain\\nVt = ˆϵt(V0) = √¯αT V0 + √1 −¯αtϵt according to Eq. (3),\\nand the latter is fed back into the denoiser as above.\\nFinally, there is the issue that unconditional generation\\nof the clean volume V0 from pure noise VT is difficult, es-\\npecially in the first iterations of training. On the other hand,\\nthe problem of view-conditioned generation is considerably\\neasier. Hence, the third idea is to learn a conditional gener-\\nator, using a variable number of input views. Specifically,\\ngiven a training sequence s, the method extracts a random\\nsubset of frames ¯s ⊂s (which could be empty, which cor-\\nresponds to unconditional generation). Then, a feature vol-\\nume ¯V = Φ(¯s) ∈Rd×S×S×S is obtained from the selected\\nframes. This extracts 2D image features using a pre-trained\\nand frozen 2D image encoder and then pools them in 3D via\\n‘unprojection’ [17, 14] into ¯V , where ¯V = 0 if ¯s is empty.\\nFinally, these pooled features are used to condition the de-\\nnoising network V0 = Dθ(VT , ¯V , T), which, on average,\\nleads to a simpler reconstruction problem.\\nPutting it all together, the training loss becomes:\\nL(θ|I, C, ¯s) = Et,ˆϵ,VT\\nh\\n∥ˆI −I∥2i\\n,\\n(5)\\nwhere ˆI = Rη(Dθ(Vt, ¯V , t), C),\\n(6)\\nVt = ˆϵt(V0),\\nV0 = Dθ(VT , ¯V , T),\\n¯V = Φ(¯s).\\nThis loss is averaged over training sequences s, subse-\\nquences ¯s ⊂s, and views (I, C) ∈s therein. Note that this\\nis slightly different than the original HoloDiffusion, where\\nfeature volume ¯V and reconstructed volumes Vt overlap as\\narguments of the denoiser; we found that keeping them sep-\\narated in the formulation leads to more stable training and\\nadditionally allows for view-conditioned generation.\\n3.3. HOLOFUSION\\nThe method of Sec. 3.2 learns to generate 3D objects\\nfrom 2D image supervision only, but the fidelity of the out-\\nput is limited by the resolution at which the operations are\\ncarried out. Increasing resolution is difficult due to the GPU\\n\\n--- Page 5 ---\\nmemory impact of the voxel-based representation, so we\\nseek a more efficient way to do so. The idea is to incor-\\nporate a 2D super-resolution network (Sec. 3.3.1), trained\\nend-to-end, that improves the output from the base model.\\nThe super-resolved images are eventually fused back in an\\nimproved 3D model, which also has the benefit of further\\nincreasing view consistency (Sec. 3.3.2).\\n3.3.1\\nIntegrating super-resolution\\nAs shown in Fig. 2, we augment the method of Sec. 3.2\\nwith a lightweight refinement post-processor network that\\ntakes the 2D image ˆI generated by the base model and out-\\nputs a higher quality version ˆIsuper of the same. This can\\nbe thought of as a form of super-resolution; however, due to\\nthe particular statistics of the input (‘low-res’) images ˆI that\\nHoloDiffusion generates, it is necessary to train this super-\\nresolution network in an end-to-end fashion with HoloDif-\\nfusion, integrating the two models.\\nTo make this integration seamless, we formulate super-\\nresolution as another diffusion process that runs ‘in parallel’\\nwith 3D reconstruction. Hence, the super-resolved image\\nˆIsuper = Dβ(It, ˆI, t) is the output of a denoiser network (a\\nlightweight U-Net), which takes as input the noised target\\nimage It = ˆϵt(I) and is also conditioned on the ‘low-res’\\noutput ˆI = Rη(V, C) of HoloDiffusion from Eq. (6). This\\ndenoiser is trained with the DDPM loss:\\nL(β|I) = Et,ˆϵ\\nh\\n∥Dβ(ˆϵt(I), ˆI, t) −I∥2i\\n.\\n(7)\\nTraining details\\nThe overall model (Dβ and Dθ) is\\ntrained end-to-end by optimising the loss L(θ|I, C, ¯s) +\\nL(β|I) obtained by summing Eqs. (5) and (7).\\nAs training data, we use a large dataset of images cap-\\nturing object-centric scenes ([30]). In each training batch,\\nwe pick a random training scene s and sample 15 differ-\\nent source images ¯ssrc ⊂s which are unprojected to gen-\\nerate the feature volume conditioning ¯V . Then, ¯V is ren-\\ndered into 4 random target views ¯stgt ⊂(s \\\\ ¯ssrc) which\\nallows to optimize the training image reconstruction loss\\nL(θ|I, C, ¯s) + L(β|I). The latter uses the Adam optimizer\\nwith an initial learning rate of 5 · 10−5 decaying tenfold\\nwhenever the loss plateaus until convergence.\\n3.3.2\\nFusing super-resolved views in 3D\\nThe method of Sec. 3.3.1 leaves us with high-resolution\\nviews ˆIsuper of the generated 3D object. However, we would\\nlike to obtain a single, high-quality 3D model, not just in-\\ndividual views of it. In this section, we discuss how to take\\nthe super-resolved images and fuse them into such a model,\\nwhile addressing the issues that these images are not per-\\nfectly view-consistent.\\nBest Match\\nPatch-based\\nphotometric error\\nNeural renderer\\nSuper resolution (2D)\\nPatch remix\\nFigure 3: Distillation. HOLOFUSION distills a single high-\\nresolution voxel grid V H\\n0\\nby minimizing a top-k patch-\\nremix loss Ldistil between the grid renders Rη′(V H\\n0 , C) and\\na bank IC of K = 5 high-res images output by the 2D dif-\\nfusion upsampler Dβ for each scene camera C.\\nThe basic idea is simple. We can generate a certain num-\\nber (e.g., 100) high-resolution images of the object from\\ndifferent viewpoints C and then use a technique, akin to\\nneural rendering, to fuse them back into a single 3D model.\\nHowever, there is a problem with this idea: The model\\nof Sec. 3.3.1 generates high-quality views Isuper, but these\\nare view-dependent samples from the distribution p(ˆIsuper|ˆI)\\nwhere ˆI = Rη(V, C) is the ‘low-res’ output form HoloDif-\\nfusion. Because super-resolving details is intrinsically am-\\nbiguous, there is no reason why samples Isuper taken from\\ndifferent viewpoints C would be consistent (Fig. 7). Fusing\\nthem into a single 3D model would then result in a blurry\\nappearance yet again.\\nAs described in Fig. 3, we address this issue in a princi-\\npled manner by considering several possible super-resolved\\nimages IC = {Isuper ∼p(ˆIsuper|ˆI)} sampled from each\\ngiven viewpoint C. Then, we optimize a high-resolution\\nvoxel grid V H\\n0\\nby minimizing the photometric loss:\\nLdistil(η′, V H\\n0 |IC) = EC\\n\\u0014\\nmin\\nIsuper∈IC ∥Isuper −Rη′(V H\\n0 , C)∥2\\n\\u0015\\n(8)\\nwhere Rη′(V H\\n0 , C) is the render of a high-resolution voxel\\ngrid V H\\n0\\n∈Rd×S′×S′×S′, S′ \\> S using the learnable ren-\\nderer Rη′ with scene specific parameters η′. Minimizing\\nwith respect to Isuper means that the 3D model must be con-\\nsistent with at least one of the possible super-resolved im-\\nages, drawn from the distribution of super-resolved sam-\\nples, for each view C.\\nPatch remix.\\nIn practice, this approach requires a very\\nlarge number of super resolved images IC to be effective.\\nWe found that we can significantly improve the statistical\\nefficiency by performing the minimization at the level of\\nindividual patches. Namely, we produce a stack of only\\n\\n--- Page 6 ---\\nπ-GAN\\nDreamFusion\\nHoloDiffusion\\nHoloDiffusion*\\nHoloFusion\\n0°\\n36°\\n72°\\n108°\\n144°\\n180°\\nFigure 4: Generated 3D samples visualized from a mov-\\ning camera. π-GAN and HoloDiffusion∗fail to produce\\n3D view consistent samples, while DreamFusion suffers\\nfrom the “Janus” problem (multiple heads).\\nK = |IC| = 5 super resolved images and perform the min-\\nimization in Eq. (8) at the level of small 16 × 16 patches\\nindependently (effectively allowing super-resolved images\\nto ‘remix’ as needed to fit the generated view Rη′(V H\\n0 , C)).\\nDistillation details.\\nLdistil is optimized independently for\\neach generated scene with Adam (lr=2·10−4) for 25K steps\\nuntil convergence.\\nWhile η′ is initialized using the pre-\\ntrained multi-sequence weights η, V H\\n0\\nis initialized by tri-\\nlinearly upsampling the low-resolution volume V0 output\\nby HoloDiffusion. Cameras C are sampled at uniform az-\\nimuths with elevation fixed at object’s equator.\\nMSE\\nSDS\\nOurs\\nFigure 5: Fusing views. Our patch-remix (Sec. 3.3.2) com-\\npared to the SDS and MSE distillation. MSE has “floaters”\\nand viewpoint inconsistencies, SDS over-smooths the tex-\\nture. Ours is robust and produces superior quality.\\n4. Experiments\\nWe begin with a description of the experiments con-\\nducted in Sec. 4.1, followed by an analysis and discussion\\nof the results in Sec. 4.2.\\n4.1. Details\\nDataset.\\nWe experiment on the challenging large-scale\\nCo3Dv2 [30] dataset which is a popular choice for a real-\\nworld 3D reconstruction benchmark. More specifically, 4\\ncategories are selected, Apple, Hydrant, TeddyBear,\\nand Donut, with 500 3D-scenes per category for training.\\nEach 3D scene contains ∼200 images of the object of in-\\nterest along with poses of their corresponding cameras.\\nBaselines.\\nWe use two sets of baselines for comparison\\n(Tab. 1):\\n(i) general 3D generative modeling baselines\\nand (ii) diffusion distillation based baselines. π-GAN [5],\\nEG3D [4], GET3D [9], and HoloDiffusion [18] are consid-\\nered as the 3D generative baselines. Along with HoloDiffu-\\nsion, we also test the super-resolution integrated model (de-\\nscribed in Sec. 3.3.1) HoloDiffusion∗. For the distillation-\\nbased baselines, we consider the open-source implementa-\\ntion of DreamFusion [29] titled Stable-DreamFusion [40].\\nFor the latter, scenes are generated by conditioning on\\nprompts comprising names of Co3Dv2 categories extended\\nwith color and style modifier phrases leading to ∼200\\nprompts / 3D shapes per class. More details regarding the\\nprompt creation are in the supplementary.\\nMetrics.\\nWe use FID [15] and KID [3] to compare the\\nquality of our 2D renders, as these are commonly used to\\nassess 2D and 3D generators.\\n4.2. Quantitative and qualitative analysis\\nTab. 1 evaluates quantitatively while Fig. 6 qualitatively.\\nFurthermore, Fig. 4 compares rendering view-consistency.\\nHoloFusion (Ours) yields better FID/KID scores than\\nthe general 3D generative baselines except for π-GAN on\\nApple and Donut classes. However, since π-GAN does\\nnot guarantee view consistency by design, it essentially\\nacts as a 2D image GAN, and thus does better on the\\n2D FID/KID metrics, but it generates significantly view-\\ninconsistent renders (see Fig. 4 and the supplementary).\\nWe observed that the other 3D-GAN baselines, EG3D\\nand GET3D, are prone to collapsing to a single adversarial\\nsample leading to poor FID/KID scores. The latter is prob-\\nably due to the 3D misalignment of the CO3Dv2 sequences\\nacross instances, which makes training harder.\\nHoloFusion also outperforms the text-to-3D Stable-\\nDreamFusion\\non\\nboth\\nFID/KID.\\nStable-DreamFusion\\nyields good shapes, but produces synthetic-looking and\\noverly-smooth textures and thus performs poorly when\\ncompared to the real-world images of Co3Dv2. As evi-\\n\\n--- Page 7 ---\\nApple\\nHydrant\\nTeddyBear\\nDonut\\n4285f4\\nπ-GAN\\nEG3D\\nGET-3D\\nDreamFusion\\nHoloDiffusion\\nHoloDiffusion*\\nHoloFusion\\nFigure 6: 3D samples generated by our HoloFusion compared to π-GAN, EG3D, GET3D, HoloDiffusion, HoloDiffusion∗,\\nand the text-to-3D Stable-DreamFusion.\\n\\n--- Page 8 ---\\nTable 1: FID (↓) and KID (↓) on 4 classes of Co3Dv2 [30]. We compare with 3D generative modeling baselines (rows 1–5);\\nwith an SDS distillation-based Stable-DreamFusion (row 6); and with ablations of our HoloFusion (rows 7–8). The column\\n“VP” denotes whether renders of a method are 3D view-consistent or not.\\nmethod\\nVP\\nApple\\nHydrant\\nTeddyBear\\nDonut\\nMean\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nπ-GAN [5]\\n✗\\n49.3\\n0.042\\n92.1\\n0.080\\n125.8\\n0.118\\n99.4\\n0.069\\n91.7\\n0.077\\nEG3D [4]\\n✓\\n170.5\\n0.203\\n229.5\\n0.253\\n236.1\\n0.239\\n222.3\\n0.237\\n214.6\\n0.233\\nGET3D [9]\\n✓\\n179.1\\n0.190\\n303.3\\n0.380\\n244.5\\n0.280\\n209.9\\n0.230\\n234.2\\n0.270\\nHoloDiffusion [18]\\n✓\\n94.5\\n0.095\\n100.5\\n0.079\\n109.2\\n0.106\\n115.4\\n0.085\\n122.5\\n0.102\\nHoloDiffusion∗\\n✗\\n55.9\\n0.045\\n62.6\\n0.045\\n116.6\\n0.101\\n99.6\\n0.079\\n83.7\\n0.068\\nStable-DreamFusion [40]\\n✓\\n139.0\\n0.104\\n185.2\\n0.132\\n183.4\\n0.125\\n169.3\\n0.114\\n169.2\\n0.119\\nHoloFusion (MSE)\\n✗\\n72.7\\n0.067\\n62.2\\n0.045\\n87.2\\n0.076\\n109.0\\n0.099\\n82.8\\n0.072\\nHoloFusion (SDS)\\n✓\\n123.0\\n0.105\\n77.1\\n0.058\\n117.8\\n0.090\\n142.8\\n0.087\\n115.2\\n0.085\\nHoloFusion (Ours)\\n✓\\n69.2\\n0.063\\n66.8\\n0.047\\n87.6\\n0.075\\n109.7\\n0.098\\n83.3\\n0.071\\nTeddyBear\\nHydrant\\nApple\\nDonut\\nNormalized std\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFigure 7: Heatmaps illustrating the per-pixel color variance\\nof K = 10 hypothesis produced by the upsampler Dβ.\\nSome samples contain artifacts around the object bound-\\naries which correspond to the high-variance regions in the\\nfigure. Our top-K patch-remix increases robustness by al-\\nlowing the loss to discard such artifacts during distillation.\\ndent from the TeddyBear samples, the method also suf-\\nfers from the “Janus” issue.\\nCompared to HoloDiffusion, we improve the FID/KID\\nscores by a significant margin, mainly due to the more\\nphoto-realistic renders that include high-frequency details.\\nAlthough the 2D Diffusion upsampler of HoloDiffusion∗\\nproduces renders with the highest amount of details yield-\\ning scores similar to ours, they are not 3D view-consistent\\n(as apparent from Fig. 4 and as explained in Sec. 3.3).\\nAblations.\\nIn Tab. 1 and in Fig. 5 we ablate components\\nof our HoloFusion to verify their contribution.\\nThe first variant, HoloFusion (SDS), replaces the Top-\\nk patch-remixed distillation loss with the score distillation\\nsampling (SDS) gradient as proposed in [29]. As apparent\\nfrom Fig. 5 and from the lower scores, SDS washes out all\\nthe high-frequency details in the textures.\\nSecondly, HoloFusion (MSE) reduces the number of up-\\nsampling hypotheses I to the minimum of |I| = 1. Even\\nthough this slightly improves the 2D metrics, as can be seen\\nfrom Fig. 5, the samples lack view-consistency and intro-\\nduce “floaters”. In Fig. 7 we further illustrate the variability\\nof the upsampling hypotheses.\\n5. Conclusion\\nWe have presented a hybrid diffusion-based method that\\ncan generate high-quality 3D neural radiance fields of real-\\nlife object categories.\\nOur method starts by producing\\ncoarse 3D models whose renders are independently super-\\nresolved, and finally consolidated using a robust distillation\\nprocess. We evaluated our method on the Co3D v2 dataset\\nand presented 3D-consistent, diverse, and high-quality re-\\nsults superior to all competing baselines.\\nOur method suffers from limitations that can be ad-\\ndressed in future work. First, our method is slow to sample\\nfrom as the sampling process takes about 30 mins for each\\ngeneration, because it is still a distillation-based method.\\nAn interesting extension would be to train another network\\nto directly distill a set of super-resolved images, without\\nrequiring explicit optimization during inference. Second,\\nwe do not produce an explicit surface representation (e.g., a\\nmesh), which could be done by integrating a differentiable\\nmesh render in the loop as done in some prior work.\\n6. Acknowledgements\\nAnimesh and Niloy were partially funded by the Euro-\\npean Union’s Horizon 2020 research and innovation pro-\"}\u001b[0m\n",
      "\u001b[32m2025-04-13 06:11:47.076\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 3: Create and populate the Page Summary subpage using the paper information and the extracted full text.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:14:52.438\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking PS Tool with args: {'papers': [{'title': 'HoloFusion: Towards Photo-realistic 3D Generative Modeling', 'summary': 'Diffusion-based image generators can now produce high-quality and diverse samples, but their success has yet to fully translate to 3D generation: existing diffusion methods can either generate low-resolution but 3D consistent outputs, or detailed 2D views of 3D objects but with potential structural defects and lacking view consistency or realism. We present HoloFusion, a method that combines the best of these approaches to produce high-fidelity, plausible, and diverse 3D samples while learning from a collection of multi-view 2D images only. The method first generates coarse 3D samples using a variant of the recently proposed HoloDiffusion generator. Then, it independently renders and upsamples a large number of views of the coarse 3D model, super-resolves them to add detail, and distills those into a single, high-fidelity implicit 3D representation, which also ensures view consistency of the final renders. The super-resolution network is trained as an integral part of HoloFusion, end-to-end, and the final distillation uses a new sampling scheme to capture the space of super-resolved signals. We compare our method against existing baselines, including DreamFusion, Get3D, EG3D, and HoloDiffusion, and achieve, to the best of our knowledge, the most realistic results on the challenging CO3Dv2 dataset.', 'link': 'https://arxiv.org/pdf/2308.14244v1.pdf'}], 'pdf_texts': {'HoloFusion_Towards_Photo-.pdf': '--- Page 1 ---\\nHOLOFUSION: Towards Photo-realistic 3D Generative Modeling\\nAnimesh Karnewar\\nUCL\\na.karnewar@ucl.ac.uk\\nNiloy J. Mitra\\nUCL\\nn.mitra@ucl.ac.uk\\nAndrea Vedaldi\\nMeta AI\\nvedaldi@meta.com\\nDavid Novotny\\nMeta AI\\ndnovotny@meta.com\\nHigh-quality 3D shape\\nDistillation loss\\nHoloDiffusion samples\\nHoloFusion high quality samples\\nDistillation\\nEA Rendering\\nFigure 1: We propose HOLOFUSION to generate photo-realistic 3D radiance fields by extending the HoloDiffusion method\\nwith a jointly trained 2D ‘super resolution’ network. The independently super-resolved images are fused back into the 3D\\nrepresentation to improve the fidelity of the 3D model via distillation, while preserving the consistency across view changes.\\nAbstract\\nDiffusion-based image generators can now produce\\nhigh-quality and diverse samples, but their success has yet\\nto fully translate to 3D generation: existing diffusion meth-\\nods can either generate low-resolution but 3D consistent\\noutputs, or detailed 2D views of 3D objects but with poten-\\ntial structural defects and lacking view consistency or real-\\nism. We present HOLOFUSION, a method that combines the\\nbest of these approaches to produce high-fidelity, plausible,\\nand diverse 3D samples while learning from a collection\\nof multi-view 2D images only. The method first generates\\ncoarse 3D samples using a variant of the recently proposed\\nHoloDiffusion generator.\\nThen, it independently renders\\nand upsamples a large number of views of the coarse 3D\\nmodel, super-resolves them to add detail, and distills those\\ninto a single, high-fidelity implicit 3D representation, which\\nalso ensures view-consistency of the final renders.\\nThe\\nsuper-resolution network is trained as an integral part of\\nHOLOFUSION, end-to-end, and the final distillation uses a\\nnew sampling scheme to capture the space of super-resolved\\nsignals. We compare our method against existing baselines,\\nincluding DreamFusion, Get3D, EG3D, and HoloDiffusion,\\nand achieve, to the best of our knowledge, the most realistic\\nresults on the challenging CO3Dv2 dataset.\\n1. Introduction\\nDiffusion models [32, 7, 31] are at the basis of state-of-\\nthe-art 2D image generators which can now produce very\\nhigh-quality and diverse outputs. However, their success\\nhas yet to be translated to 3D and there is no generator that\\ncan produce 3D assets of a comparable quality.\\nRecent attempts at extending diffusion to 3D generation\\nhave reported mixed success. Some authors have attempted\\nto apply diffusion directly in 3D [18], or still in 2D but us-\\ning a 3D-aware neural network [42, 1]. This requires solv-\\ning two problems: first, finding a suitable 3D representa-\\ntion (e.g., triplane features [4], mesh [19], voxels [18]) that\\nscales well with resolution and is amenable to diffusion;\\n\\n--- Page 2 ---\\nand, second, obtaining a large amount of 3D training data,\\nfor example using synthetic models [41, 27], or training the\\nmodel using only 2D images [18], often via differentiable\\n(volume) rendering [13, 26]. However, the quality of results\\nso far is limited, especially when training on real images.\\nOther authors have proposed to distill 3D objects from\\npre-trained 2D image generators. For instance Score Distil-\\nlation Sampling (SDS) [29] can sample 3D objects from a\\nhigh-quality off-the-shelf 2D diffusion model while requir-\\ning no (re)training. However, without any 3D guidance, dis-\\ntillation methods often produce implausible results; for ex-\\nample, they suffer from the ‘Janus effect’, where details of\\nthe front of the object are replicated on its back. They also\\ncreate overly-smooth outputs that average out inconsisten-\\ncies arising from the fact that the signal obtained from the\\n2D model is analogous to sampling independent views of\\nthe object (see Sec. 4.2 for examples). Furthermore, distil-\\nlation methods do not support unconditional sampling, even\\nif the underlying image generator does, as strong language\\nguidance is required to stabilise the 3D reconstruction.\\nIn this work, we propose HOLOFUSION, a method that\\ncombines the best of both approaches.\\nWe start from\\nHoloDiffusion [18], a diffusion-based 3D generator. This\\nmodel can be trained using only a multiview image dataset\\nlike [30] and produces outputs that are 3D consistent. How-\\never, the output resolution is limited by computation and\\nmemory. We augment the base model with a lightweight\\nsuper-resolution network that upscales the initial renders.\\nCrucially, the 2D super-resolution model is integrated and\\ntrained jointly with the 3D generator, end-to-end.\\nThe super-resolution network outputs detailed views of\\nthe 3D object, and the underlying 3D generator ensures\\nthat the coarse structure of these views is indeed consis-\\ntent (e.g., avoiding the Janus effect and other structural arti-\\nfacts). However, the 2D upscaling still progresses indepen-\\ndently for different views, which means that fine grained\\ndetails may still be inconsistent between views. We address\\nthis issue by distilling a single, coherent, high quality 3D\\nmodel of the object from the output of the upsampler. For\\nthis, we propose a new distillation technique that efficiently\\ncombines several putative super-resolved views of the ob-\\nject into a single, coherent 3D reconstruction.\\nWith this, we are able to train a high-quality 3D genera-\\ntor model purely from real 2D data. This model is capable\\nof generating consistent and detailed 3D objects, which in\\nturn result in view-consistent renderings (see Fig. 1) at a\\nquality not achievable by prior methods.\\nWe evaluate HOLOFUSION on real images (CO3Dv2\\ndataset [30]) and compare with a variety of compet-\\ning alternatives (e.g., HoloDiffusion [18], Get3D [9],\\nEG3D [4], DreamFusion [40]) demonstrating that view-\\nconsistent high-quality 3D generation is possible using our\\nsimple, effective, easy-to-implement hybrid approach.\\n2. Related Work\\n3D generators that use adversarial learning.\\nGenera-\\ntive Adversarial Learning (GAN) [10] learns a generator\\nnetwork so that its “fake” samples cannot be distinguished\\nfrom real images by a second discriminator network. Ap-\\nproaches such as PlatonicGAN [13], HoloGAN [28], and\\nPrGAN [8] introduced 3D structure into the generator net-\\nwork, achieving 3D shape generation with only image-level\\nsupervision. Our method is related to those as it renders\\nimages from a generated voxel grid, as well as to Holo-\\nGAN [28], which renders features and then converts them\\ninto an image by a lightweight 2D convolutional network.\\nOther voxel-based 3D generators include VoxGRAF [34]\\nand NeuralVolumes [21].\\nMore recently, 3D generators have built on neural ra-\\ndiance fields [26]. GRAF [33] was the first to adopt the\\nNeRF framework; analogous to PlatonicGAN, they gener-\\nate the parameters of an MLP which renders realistic im-\\nages of the object from a random viewpoint. This idea has\\nbeen improved in StyleNeRF [11] and EG3D [4] by adding\\na 2D convolutional post-processing step after emission-\\nabsorption rendering, which is analogous to our super-\\nresolution network.\\nEG3D also introduced a novel ‘tri-\\nplane’ representation of the radiance field which, in a mem-\\nory efficient manner, factorises the latter into a triplet of 2D\\nfeature planes. EG3D inspired several improvements such\\nas GAUDI [2] and EpiGRAF [37].\\nMesh-based 3D generators have been explored in [43].\\nRecently, GET3D [9] replaced the radiance field with a\\nsigned distance function to regularise the representation of\\ngeometry. The latter is converted into a mesh and rendered\\nin a differentiable manner by using the marching tetrahedral\\nrepresentation [35].\\nModeling 3D with diffusion.\\nDiffusion methods [38]\\nhave recently became the go-to framework for generative\\nmodeling of any kind, including 3D generative modeling.\\nThe first applications of diffusion to 3D considered point-\\ncloud generators trained on synthetic data [23, 45, 44].\\n3D distillation of 2D diffusion models.\\nMore recently,\\nDreamFusion [29] ported the idea of distillation to diffu-\\nsion models: they extract a neural radiance field so that\\nits renders match the belief of a pre-trained 2D diffusion\\ngenerator [32, 7, 31]. They introduce the Score Distillation\\nSampling (SDS) loss which makes distillation relatively ef-\\nficient (but still in the order of several minutes for a single\\n3D sample). Their generation can be conditioned by an im-\\nage or by a textual description, making the process rather\\nflexible. Magic3D [19] further increases the quality of the\\noutput by distilling a mesh-based 3D representation instead\\nof a radiance field.\\nImage-conditioned 3D diffusion.\\nThe idea of distillation\\nhas been applied to few-view conditioned reconstruction in\\n\\n--- Page 3 ---\\n[42, 12, 25, 46, 6]. SparseFusion [46] employs a 3D-based\\nnew-view synthesis model [39] followed by a 2D diffusion\\nupsampler. They complete the process by 3D distillation,\\nensuring that the generated views of the object are consis-\\ntent. NeRFDiff [12] and 3DiM [42] bypass an explicit 3D\\nmodel and directly generate new views of an object using a\\n2D image generator and, in the case of NeRFDiff, refine the\\nresults using distillation.\\nWhile SparseFusion and NeRFDiff need to be trained on\\na dataset of object-centric multi-view images with pose in-\\nformation, RealFusion [25] and NeRDi [6] can be used for\\nzero-shot monocular 3D reconstruction, starting from a pre-\\ntrained 2D diffusion model. Given a single image as in-\\nput, they automatically generate a prompt for the diffusion\\nmodel, using a form of prompt inversion, and then use dis-\\ntillation to extract a radiance field.\\nUnconditional generation.\\nMost relevantly to us, uncon-\\nditional generation, i.e., generation which does not require\\neither text or image conditioning, was explored in [41, 36,\\n27, 18]. While [27, 36, 41] train generators given synthetic\\n3D ground truth, similar to us, HoloDiffusion [18] is su-\\npervised only with real object-centric images and camera\\nposes. While HoloDiffusion was the first to demonstrate\\nsuccessful training on real image data, its renders contain\\nconsiderably lower amount of detail than samples from a\\nconventional 2D image generator that uses diffusion. We\\nthus leverage a 2D diffusion upsampler, conditioned on the\\nlower-fidelity HoloDiffusion renders, to distill higher reso-\\nlution images and, eventually, 3D models.\\n3. HOLOFUSION\\nWe present HOLOFUSION, a method that can learn a\\nhigh-quality diffusion-based 3D generator from a collection\\nof multiview 2D images. HOLOFUSION first obtains an un-\\nconditional low-resolution 3D sample using diffusion and\\nthen distills a high-resolution 3D radiance field represent-\\ning a higher-quality version of the generated object. We\\nfirst summarize the Denoising Diffusion Probabilistic Mod-\\nels (DDPMs) [16] that we utilize in Sec. 3.1. Then, we dis-\\ncuss the low-resolution 3D generator in Sec. 3.2 followed\\nby super-resolution distillation in Sec. 3.3.\\n3.1. Preliminaries: DDPMs\\nLet x = x0 be a random vector whose probability distri-\\nbution p(x|y) we seek to model. The DDPM [16] defines\\na hierarchy of latent variables xt, t = 0, . . . , T and an en-\\ncoder q comprising a sequence of Gaussian distributions\\nq(xt|xt−1) = N(xt; √αtxt−1, (1 −αt)I),\\n(1)\\nwhere αt, . . . , αT is a predefined ‘noising schedule’. Given\\nknowledge of x0, a sample xt can be drawn in a closed-form\\ndirectly from q(xt|x0) = N(xt; √¯αtx0, (1 −¯αt)I), where\\n¯αt = Qt\\ni=1 αi. Hence, we can express xt as:\\nxt = ˆϵt(x0) = √¯αtx0 +\\n√\\n1 −¯αtϵt\\n(2)\\nThe noising schedule is chosen such that ¯αT ≈0. In\\nthis manner, q(xT |x0) ≈N(xT ; 0, I) is approximately nor-\\nmal, and so is q(xT ). To generate a sample x = x0, we\\nstart by sampling xT from this normal distribution and then\\nsample the intermediate latent variables backward. This is\\ndone by using a variational approximation of the probabili-\\nties q(xt−1|xt) given by the Gaussian factors:\\np(xt−1|xt) = N(xt−1; √¯αt−1Dθ(xt, t), (1−¯αt−1)I) (3)\\nwhere Dθ is a neural network with parameters θ.\\nThe network Dθ is trained by maximizing the ELBO\\n(Evidence Lower Bound), which reduces to the denoising\\nobjective [22]:\\nL(θ) = Et,ϵ,x\\n\\\\x14\\n¯αt−1\\n2(1 −¯αt−1)2 ∥Dθ(ˆϵt(x), t) −x∥2\\n\\\\x15\\n, (4)\\nwhere t is sampled uniformly from U[1, T].\\nHence,\\nDθ(xt, t) approximates the clean sample x0 given the noisy\\nsample xt (obtained using (2)).\\n3.2. HoloDiffusion revisited\\nGiven a large dataset of 3D models, the framework of\\nSec. 3.1 could be used to train a corresponding probability\\ndistribution. However, such a dataset is not available, and\\nwe must instead learn from 2D images of physical 3D ob-\\njects. Given a dataset containing several views of a large\\nnumber of objects, we could use image-based reconstruc-\\ntion (e.g., using neural rendering) to obtain corresponding\\n3D models first, and then use those to train a diffusion\\nmodel. Instead, we adopt, and slightly upgrade, the HoloD-\\niffusion method [18], which learns a 3D diffusion model\\ndirectly from the 2D images.\\nTraining data.\\nHoloDiffusion learns from a collection D\\nof N image sequences si = (Ii\\nj, Ci\\nj)Nframe\\nj=1 , i = 1, . . . , N,\\nwhere frame Ii\\nj ∈R3×H×W is an RGB image and Ci\\nj ∈\\nR4×4 is the corresponding camera projection matrix, col-\\nlectively defining the motion of the camera.\\n3D representation and rendering.\\nThe shape and ap-\\npearance of the object are represented by a voxel grid V ∈\\nRd×S×S×S with resolution S containing a d-dimensional\\nfeature vector per voxel.\\nGiven a 3D point p ∈R3,\\nits opacity σ(p) ∈R+ and color c(p) ∈R3\\n[0,1] are ob-\\ntained from the voxel grid by an MLP Mη(V (p)) that takes\\nas input the d-dimensional feature vector V (p) extracted\\nform the grid via trilinear interpolation [20].\\nThe usual\\nemission-absorption model [26, 24] is then used to imple-\\nment a differentiable rendering function Rη, mapping the\\nvoxel grid V and the camera viewpoint C into an image\\nˆI = Rη(V, C), where η are the parameters of the MLP.\\n\\n--- Page 4 ---\\nNeural renderer\\nChannelwise concatenation\\nHoloDiffusion\\nDenoiser\\nUnprojected\\nfeatures\\n3D noise\\nFeature grid\\n2D noise\\nUpsampled render\\n2D Upsampling\\nDenoiser\\nGround truth\\nGround truth\\nLow-res render\\nCamera\\nFigure 2: Overview. HOLOFUSION, which trains the 3D denoiser network Dθ, is augmented with the 2D ‘super-resolution’\\ndiffusion model Dβ. Both models are trained end-to-end by supervising their outputs with 2D photometric error.\\nTraining scheme.\\nHoloDiffusion leverages the DDPM\\nframework (revised in the previous paragraphs) to recover\\nthe density p(V ) over voxel grids x = V encoding plau-\\nsible real-life objects. In order to train a DDPM on such\\n3D data, we would need access to ground-truth 3D models\\nV , which are not available. HoloDiffusion addresses this\\nproblem by making three changes to DDPM.\\nFirst, it replaces the data denoising loss with a photo-\\nmetric reconstruction loss. Given a pair (I, C) ∈s from\\none of the training sequences s, it replaces Eq. (4) with\\nEt,ϵ,C\\n\\\\x02\\n∥I −Rη(Dθ(ˆϵt(V ), t), C)∥2\\\\x03\\nwhere the goal is not\\nto reconstruct the ‘clean’ volume V (which is unknown),\\nbut rather its image I (which is known).\\nSecond, also because the ‘clean’ volume V is not avail-\\nable, we cannot use Eq. (3) to generate the noisy volumes Vt\\nto denoise; the only exception is the last sample VT , which\\nis pure noise. This suggests to adopt a ‘double denoising’\\nstep. First, pure noise VT is fed into the denoiser to ob-\\ntain an (approximate) version of V0 = Dθ(VT , T) of the\\nclean volume V0 = V . Then, noise is applied to obtain\\nVt = ˆϵt(V0) = √¯αT V0 + √1 −¯αtϵt according to Eq. (3),\\nand the latter is fed back into the denoiser as above.\\nFinally, there is the issue that unconditional generation\\nof the clean volume V0 from pure noise VT is difficult, es-\\npecially in the first iterations of training. On the other hand,\\nthe problem of view-conditioned generation is considerably\\neasier. Hence, the third idea is to learn a conditional gener-\\nator, using a variable number of input views. Specifically,\\ngiven a training sequence s, the method extracts a random\\nsubset of frames ¯s ⊂s (which could be empty, which cor-\\nresponds to unconditional generation). Then, a feature vol-\\nume ¯V = Φ(¯s) ∈Rd×S×S×S is obtained from the selected\\nframes. This extracts 2D image features using a pre-trained\\nand frozen 2D image encoder and then pools them in 3D via\\n‘unprojection’ [17, 14] into ¯V , where ¯V = 0 if ¯s is empty.\\nFinally, these pooled features are used to condition the de-\\nnoising network V0 = Dθ(VT , ¯V , T), which, on average,\\nleads to a simpler reconstruction problem.\\nPutting it all together, the training loss becomes:\\nL(θ|I, C, ¯s) = Et,ˆϵ,VT\\nh\\n∥ˆI −I∥2i\\n,\\n(5)\\nwhere ˆI = Rη(Dθ(Vt, ¯V , t), C),\\n(6)\\nVt = ˆϵt(V0),\\nV0 = Dθ(VT , ¯V , T),\\n¯V = Φ(¯s).\\nThis loss is averaged over training sequences s, subse-\\nquences ¯s ⊂s, and views (I, C) ∈s therein. Note that this\\nis slightly different than the original HoloDiffusion, where\\nfeature volume ¯V and reconstructed volumes Vt overlap as\\narguments of the denoiser; we found that keeping them sep-\\narated in the formulation leads to more stable training and\\nadditionally allows for view-conditioned generation.\\n3.3. HOLOFUSION\\nThe method of Sec. 3.2 learns to generate 3D objects\\nfrom 2D image supervision only, but the fidelity of the out-\\nput is limited by the resolution at which the operations are\\ncarried out. Increasing resolution is difficult due to the GPU\\n\\n--- Page 5 ---\\nmemory impact of the voxel-based representation, so we\\nseek a more efficient way to do so. The idea is to incor-\\nporate a 2D super-resolution network (Sec. 3.3.1), trained\\nend-to-end, that improves the output from the base model.\\nThe super-resolved images are eventually fused back in an\\nimproved 3D model, which also has the benefit of further\\nincreasing view consistency (Sec. 3.3.2).\\n3.3.1\\nIntegrating super-resolution\\nAs shown in Fig. 2, we augment the method of Sec. 3.2\\nwith a lightweight refinement post-processor network that\\ntakes the 2D image ˆI generated by the base model and out-\\nputs a higher quality version ˆIsuper of the same. This can\\nbe thought of as a form of super-resolution; however, due to\\nthe particular statistics of the input (‘low-res’) images ˆI that\\nHoloDiffusion generates, it is necessary to train this super-\\nresolution network in an end-to-end fashion with HoloDif-\\nfusion, integrating the two models.\\nTo make this integration seamless, we formulate super-\\nresolution as another diffusion process that runs ‘in parallel’\\nwith 3D reconstruction. Hence, the super-resolved image\\nˆIsuper = Dβ(It, ˆI, t) is the output of a denoiser network (a\\nlightweight U-Net), which takes as input the noised target\\nimage It = ˆϵt(I) and is also conditioned on the ‘low-res’\\noutput ˆI = Rη(V, C) of HoloDiffusion from Eq. (6). This\\ndenoiser is trained with the DDPM loss:\\nL(β|I) = Et,ˆϵ\\nh\\n∥Dβ(ˆϵt(I), ˆI, t) −I∥2i\\n.\\n(7)\\nTraining details\\nThe overall model (Dβ and Dθ) is\\ntrained end-to-end by optimising the loss L(θ|I, C, ¯s) +\\nL(β|I) obtained by summing Eqs. (5) and (7).\\nAs training data, we use a large dataset of images cap-\\nturing object-centric scenes ([30]). In each training batch,\\nwe pick a random training scene s and sample 15 differ-\\nent source images ¯ssrc ⊂s which are unprojected to gen-\\nerate the feature volume conditioning ¯V . Then, ¯V is ren-\\ndered into 4 random target views ¯stgt ⊂(s \\\\ ¯ssrc) which\\nallows to optimize the training image reconstruction loss\\nL(θ|I, C, ¯s) + L(β|I). The latter uses the Adam optimizer\\nwith an initial learning rate of 5 · 10−5 decaying tenfold\\nwhenever the loss plateaus until convergence.\\n3.3.2\\nFusing super-resolved views in 3D\\nThe method of Sec. 3.3.1 leaves us with high-resolution\\nviews ˆIsuper of the generated 3D object. However, we would\\nlike to obtain a single, high-quality 3D model, not just in-\\ndividual views of it. In this section, we discuss how to take\\nthe super-resolved images and fuse them into such a model,\\nwhile addressing the issues that these images are not per-\\nfectly view-consistent.\\nBest Match\\nPatch-based\\nphotometric error\\nNeural renderer\\nSuper resolution (2D)\\nPatch remix\\nFigure 3: Distillation. HOLOFUSION distills a single high-\\nresolution voxel grid V H\\n0\\nby minimizing a top-k patch-\\nremix loss Ldistil between the grid renders Rη′(V H\\n0 , C) and\\na bank IC of K = 5 high-res images output by the 2D dif-\\nfusion upsampler Dβ for each scene camera C.\\nThe basic idea is simple. We can generate a certain num-\\nber (e.g., 100) high-resolution images of the object from\\ndifferent viewpoints C and then use a technique, akin to\\nneural rendering, to fuse them back into a single 3D model.\\nHowever, there is a problem with this idea: The model\\nof Sec. 3.3.1 generates high-quality views Isuper, but these\\nare view-dependent samples from the distribution p(ˆIsuper|ˆI)\\nwhere ˆI = Rη(V, C) is the ‘low-res’ output form HoloDif-\\nfusion. Because super-resolving details is intrinsically am-\\nbiguous, there is no reason why samples Isuper taken from\\ndifferent viewpoints C would be consistent (Fig. 7). Fusing\\nthem into a single 3D model would then result in a blurry\\nappearance yet again.\\nAs described in Fig. 3, we address this issue in a princi-\\npled manner by considering several possible super-resolved\\nimages IC = {Isuper ∼p(ˆIsuper|ˆI)} sampled from each\\ngiven viewpoint C. Then, we optimize a high-resolution\\nvoxel grid V H\\n0\\nby minimizing the photometric loss:\\nLdistil(η′, V H\\n0 |IC) = EC\\n\\\\x14\\nmin\\nIsuper∈IC ∥Isuper −Rη′(V H\\n0 , C)∥2\\n\\\\x15\\n(8)\\nwhere Rη′(V H\\n0 , C) is the render of a high-resolution voxel\\ngrid V H\\n0\\n∈Rd×S′×S′×S′, S′ \\> S using the learnable ren-\\nderer Rη′ with scene specific parameters η′. Minimizing\\nwith respect to Isuper means that the 3D model must be con-\\nsistent with at least one of the possible super-resolved im-\\nages, drawn from the distribution of super-resolved sam-\\nples, for each view C.\\nPatch remix.\\nIn practice, this approach requires a very\\nlarge number of super resolved images IC to be effective.\\nWe found that we can significantly improve the statistical\\nefficiency by performing the minimization at the level of\\nindividual patches. Namely, we produce a stack of only\\n\\n--- Page 6 ---\\nπ-GAN\\nDreamFusion\\nHoloDiffusion\\nHoloDiffusion*\\nHoloFusion\\n0°\\n36°\\n72°\\n108°\\n144°\\n180°\\nFigure 4: Generated 3D samples visualized from a mov-\\ning camera. π-GAN and HoloDiffusion∗fail to produce\\n3D view consistent samples, while DreamFusion suffers\\nfrom the “Janus” problem (multiple heads).\\nK = |IC| = 5 super resolved images and perform the min-\\nimization in Eq. (8) at the level of small 16 × 16 patches\\nindependently (effectively allowing super-resolved images\\nto ‘remix’ as needed to fit the generated view Rη′(V H\\n0 , C)).\\nDistillation details.\\nLdistil is optimized independently for\\neach generated scene with Adam (lr=2·10−4) for 25K steps\\nuntil convergence.\\nWhile η′ is initialized using the pre-\\ntrained multi-sequence weights η, V H\\n0\\nis initialized by tri-\\nlinearly upsampling the low-resolution volume V0 output\\nby HoloDiffusion. Cameras C are sampled at uniform az-\\nimuths with elevation fixed at object’s equator.\\nMSE\\nSDS\\nOurs\\nFigure 5: Fusing views. Our patch-remix (Sec. 3.3.2) com-\\npared to the SDS and MSE distillation. MSE has “floaters”\\nand viewpoint inconsistencies, SDS over-smooths the tex-\\nture. Ours is robust and produces superior quality.\\n4. Experiments\\nWe begin with a description of the experiments con-\\nducted in Sec. 4.1, followed by an analysis and discussion\\nof the results in Sec. 4.2.\\n4.1. Details\\nDataset.\\nWe experiment on the challenging large-scale\\nCo3Dv2 [30] dataset which is a popular choice for a real-\\nworld 3D reconstruction benchmark. More specifically, 4\\ncategories are selected, Apple, Hydrant, TeddyBear,\\nand Donut, with 500 3D-scenes per category for training.\\nEach 3D scene contains ∼200 images of the object of in-\\nterest along with poses of their corresponding cameras.\\nBaselines.\\nWe use two sets of baselines for comparison\\n(Tab. 1):\\n(i) general 3D generative modeling baselines\\nand (ii) diffusion distillation based baselines. π-GAN [5],\\nEG3D [4], GET3D [9], and HoloDiffusion [18] are consid-\\nered as the 3D generative baselines. Along with HoloDiffu-\\nsion, we also test the super-resolution integrated model (de-\\nscribed in Sec. 3.3.1) HoloDiffusion∗. For the distillation-\\nbased baselines, we consider the open-source implementa-\\ntion of DreamFusion [29] titled Stable-DreamFusion [40].\\nFor the latter, scenes are generated by conditioning on\\nprompts comprising names of Co3Dv2 categories extended\\nwith color and style modifier phrases leading to ∼200\\nprompts / 3D shapes per class. More details regarding the\\nprompt creation are in the supplementary.\\nMetrics.\\nWe use FID [15] and KID [3] to compare the\\nquality of our 2D renders, as these are commonly used to\\nassess 2D and 3D generators.\\n4.2. Quantitative and qualitative analysis\\nTab. 1 evaluates quantitatively while Fig. 6 qualitatively.\\nFurthermore, Fig. 4 compares rendering view-consistency.\\nHoloFusion (Ours) yields better FID/KID scores than\\nthe general 3D generative baselines except for π-GAN on\\nApple and Donut classes. However, since π-GAN does\\nnot guarantee view consistency by design, it essentially\\nacts as a 2D image GAN, and thus does better on the\\n2D FID/KID metrics, but it generates significantly view-\\ninconsistent renders (see Fig. 4 and the supplementary).\\nWe observed that the other 3D-GAN baselines, EG3D\\nand GET3D, are prone to collapsing to a single adversarial\\nsample leading to poor FID/KID scores. The latter is prob-\\nably due to the 3D misalignment of the CO3Dv2 sequences\\nacross instances, which makes training harder.\\nHoloFusion also outperforms the text-to-3D Stable-\\nDreamFusion\\non\\nboth\\nFID/KID.\\nStable-DreamFusion\\nyields good shapes, but produces synthetic-looking and\\noverly-smooth textures and thus performs poorly when\\ncompared to the real-world images of Co3Dv2. As evi-\\n\\n--- Page 7 ---\\nApple\\nHydrant\\nTeddyBear\\nDonut\\n4285f4\\nπ-GAN\\nEG3D\\nGET-3D\\nDreamFusion\\nHoloDiffusion\\nHoloDiffusion*\\nHoloFusion\\nFigure 6: 3D samples generated by our HoloFusion compared to π-GAN, EG3D, GET3D, HoloDiffusion, HoloDiffusion∗,\\nand the text-to-3D Stable-DreamFusion.\\n\\n--- Page 8 ---\\nTable 1: FID (↓) and KID (↓) on 4 classes of Co3Dv2 [30]. We compare with 3D generative modeling baselines (rows 1–5);\\nwith an SDS distillation-based Stable-DreamFusion (row 6); and with ablations of our HoloFusion (rows 7–8). The column\\n“VP” denotes whether renders of a method are 3D view-consistent or not.\\nmethod\\nVP\\nApple\\nHydrant\\nTeddyBear\\nDonut\\nMean\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nFID ↓\\nKID ↓\\nπ-GAN [5]\\n✗\\n49.3\\n0.042\\n92.1\\n0.080\\n125.8\\n0.118\\n99.4\\n0.069\\n91.7\\n0.077\\nEG3D [4]\\n✓\\n170.5\\n0.203\\n229.5\\n0.253\\n236.1\\n0.239\\n222.3\\n0.237\\n214.6\\n0.233\\nGET3D [9]\\n✓\\n179.1\\n0.190\\n303.3\\n0.380\\n244.5\\n0.280\\n209.9\\n0.230\\n234.2\\n0.270\\nHoloDiffusion [18]\\n✓\\n94.5\\n0.095\\n100.5\\n0.079\\n109.2\\n0.106\\n115.4\\n0.085\\n122.5\\n0.102\\nHoloDiffusion∗\\n✗\\n55.9\\n0.045\\n62.6\\n0.045\\n116.6\\n0.101\\n99.6\\n0.079\\n83.7\\n0.068\\nStable-DreamFusion [40]\\n✓\\n139.0\\n0.104\\n185.2\\n0.132\\n183.4\\n0.125\\n169.3\\n0.114\\n169.2\\n0.119\\nHoloFusion (MSE)\\n✗\\n72.7\\n0.067\\n62.2\\n0.045\\n87.2\\n0.076\\n109.0\\n0.099\\n82.8\\n0.072\\nHoloFusion (SDS)\\n✓\\n123.0\\n0.105\\n77.1\\n0.058\\n117.8\\n0.090\\n142.8\\n0.087\\n115.2\\n0.085\\nHoloFusion (Ours)\\n✓\\n69.2\\n0.063\\n66.8\\n0.047\\n87.6\\n0.075\\n109.7\\n0.098\\n83.3\\n0.071\\nTeddyBear\\nHydrant\\nApple\\nDonut\\nNormalized std\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFigure 7: Heatmaps illustrating the per-pixel color variance\\nof K = 10 hypothesis produced by the upsampler Dβ.\\nSome samples contain artifacts around the object bound-\\naries which correspond to the high-variance regions in the\\nfigure. Our top-K patch-remix increases robustness by al-\\nlowing the loss to discard such artifacts during distillation.\\ndent from the TeddyBear samples, the method also suf-\\nfers from the “Janus” issue.\\nCompared to HoloDiffusion, we improve the FID/KID\\nscores by a significant margin, mainly due to the more\\nphoto-realistic renders that include high-frequency details.\\nAlthough the 2D Diffusion upsampler of HoloDiffusion∗\\nproduces renders with the highest amount of details yield-\\ning scores similar to ours, they are not 3D view-consistent\\n(as apparent from Fig. 4 and as explained in Sec. 3.3).\\nAblations.\\nIn Tab. 1 and in Fig. 5 we ablate components\\nof our HoloFusion to verify their contribution.\\nThe first variant, HoloFusion (SDS), replaces the Top-\\nk patch-remixed distillation loss with the score distillation\\nsampling (SDS) gradient as proposed in [29]. As apparent\\nfrom Fig. 5 and from the lower scores, SDS washes out all\\nthe high-frequency details in the textures.\\nSecondly, HoloFusion (MSE) reduces the number of up-\\nsampling hypotheses I to the minimum of |I| = 1. Even\\nthough this slightly improves the 2D metrics, as can be seen\\nfrom Fig. 5, the samples lack view-consistency and intro-\\nduce “floaters”. In Fig. 7 we further illustrate the variability\\nof the upsampling hypotheses.\\n5. Conclusion\\nWe have presented a hybrid diffusion-based method that\\ncan generate high-quality 3D neural radiance fields of real-\\nlife object categories.\\nOur method starts by producing\\ncoarse 3D models whose renders are independently super-\\nresolved, and finally consolidated using a robust distillation\\nprocess. We evaluated our method on the Co3D v2 dataset\\nand presented 3D-consistent, diverse, and high-quality re-\\nsults superior to all competing baselines.\\nOur method suffers from limitations that can be ad-\\ndressed in future work. First, our method is slow to sample\\nfrom as the sampling process takes about 30 mins for each\\ngeneration, because it is still a distillation-based method.\\nAn interesting extension would be to train another network\\nto directly distill a set of super-resolved images, without\\nrequiring explicit optimization during inference. Second,\\nwe do not produce an explicit surface representation (e.g., a\\nmesh), which could be done by integrating a differentiable\\nmesh render in the loop as done in some prior work.\\n6. Acknowledgements\\nAnimesh and Niloy were partially funded by the Euro-\\npean Union’s Horizon 2020 research and innovation pro-'}\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:03.581\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - \u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:05.056\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 4: Extract the core mathematical and scientific concepts from the full text focusing on generalizable topics for a learning pathway.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:15:18.152\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking LLM Tool with args: {'task': 'Extract the core mathematical and scientific concepts from the full text focusing on generalizable topics for a learning pathway.'}\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:28.260\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - The paper \"HoloFusion: Towards Photo-realistic 3D Generative Modeling\" introduces several core mathematical and scientific concepts that are crucial for understanding and advancing in the field of 3D generative modeling. Here are the key concepts:\n",
      "\n",
      "1. **Diffusion Models**: These are probabilistic models used for generating data by reversing a diffusion process. The paper discusses Denoising Diffusion Probabilistic Models (DDPMs), which are used to model the probability distribution of data through a series of latent variables.\n",
      "\n",
      "2. **3D Generative Modeling**: The process of creating 3D models from data, which in this case, is achieved using diffusion models. The paper highlights the challenges of translating the success of 2D diffusion models to 3D.\n",
      "\n",
      "3. **Super-resolution Networks**: These networks are used to enhance the resolution of images. In HoloFusion, a super-resolution network is integrated to improve the detail and fidelity of 3D models by upscaling 2D renders.\n",
      "\n",
      "4. **View Consistency**: Ensuring that different views of a 3D object are consistent with each other. This is a significant challenge in 3D modeling, and HoloFusion addresses it by distilling super-resolved images into a coherent 3D representation.\n",
      "\n",
      "5. **Implicit 3D Representation**: A method of representing 3D objects not by explicit geometry (like meshes) but through functions or fields that define the object's properties at any point in space.\n",
      "\n",
      "6. **Photometric Reconstruction Loss**: A loss function used to train models by comparing the rendered images of 3D models to real images, ensuring that the generated models are photorealistic.\n",
      "\n",
      "7. **Neural Rendering**: A technique that uses neural networks to render images from 3D models, allowing for differentiable rendering processes that can be integrated into learning frameworks.\n",
      "\n",
      "8. **Distillation Techniques**: Methods used to refine and consolidate information from multiple sources or views into a single, coherent model. HoloFusion uses a novel distillation technique to combine super-resolved views into a high-quality 3D model.\n",
      "\n",
      "9. **Voxel Grids and Trilinear Interpolation**: Voxel grids are used to represent 3D space in a discrete manner, and trilinear interpolation is a method to estimate values within the grid, which is crucial for rendering and processing 3D data.\n",
      "\n",
      "10. **Generative Adversarial Networks (GANs)**: Although not the primary focus, the paper references GANs as a related technology in 3D generative modeling, highlighting the differences and potential integrations with diffusion models.\n",
      "\n",
      "These concepts form a foundation for understanding advanced techniques in 3D generative modeling and can be part of a learning pathway for those interested in computer graphics, machine learning, and artificial intelligence.\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:29.797\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 5: Select the topics from the extracted core concepts using the provided topic selection service.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:15:34.613\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Topic Selector Tool with args: {'raw_topics': ['Diffusion Models', '3D Generative Modeling', 'Super-resolution Networks', 'View Consistency', 'Implicit 3D Representation', 'Photometric Reconstruction Loss', 'Neural Rendering', 'Distillation Techniques', 'Voxel Grids and Trilinear Interpolation', 'Generative Adversarial Networks (GANs)'], 'selected_indices': None}\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:37.580\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [{\"id\": \"clar-2fcb0c3e-251f-4a40-a0f4-756085f6e365\", \"plan_run_id\": \"prun-903a1d6a-ffbe-408a-9e81-853bdb094723\", \"category\": \"Input\", \"response\": null, \"step\": null, \"user_guidance\": \"Please enter the numbers of the topics you'd like to learn about, separated by commas (e.g. 1, 3, 5):\\n\\n1. Diffusion Models\\n2. 3D Generative Modeling\\n3. Super-resolution Networks\\n4. View Consistency\\n5. Implicit 3D Representation\\n6. Photometric Reconstruction Loss\\n7. Neural Rendering\\n8. Distillation Techniques\\n9. Voxel Grids and Trilinear Interpolation\\n10. Generative Adversarial Networks (GANs)\", \"resolved\": false, \"argument_name\": \"selected_indices\"}]\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:37.582\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.portia\u001b[0m:\u001b[38;5;87m_raise_clarifications\u001b[0m:\u001b[38;5;87m791\u001b[0m - \u001b[1mClarification requested - category: ClarificationCategory.INPUT, user_guidance: Please enter the numbers of the topics you'd like to learn about, separated by commas (e.g. 1, 3, 5):\n",
      "\n",
      "1. Diffusion Models\n",
      "2. 3D Generative Modeling\n",
      "3. Super-resolution Networks\n",
      "4. View Consistency\n",
      "5. Implicit 3D Representation\n",
      "6. Photometric Reconstruction Loss\n",
      "7. Neural Rendering\n",
      "8. Distillation Techniques\n",
      "9. Voxel Grids and Trilinear Interpolation\n",
      "10. Generative Adversarial Networks (GANs).\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[38;5;87mPlease enter the numbers of the topics you'd like to learn about, separated by commas (e.g. 1, 3, 5):\n",
      "\n",
      "1. Diffusion Models\n",
      "2. 3D Generative Modeling\n",
      "3. Super-resolution Networks\n",
      "4. View Consistency\n",
      "5. Implicit 3D Representation\n",
      "6. Photometric Reconstruction Loss\n",
      "7. Neural Rendering\n",
      "8. Distillation Techniques\n",
      "9. Voxel Grids and Trilinear Interpolation\n",
      "10. Generative Adversarial Networks (GANs)\n",
      "Please enter a value\u001b[0m:\u001b[32m2025-04-13 06:15:45.669\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.portia\u001b[0m:\u001b[38;5;87mresolve_clarification\u001b[0m:\u001b[38;5;87m366\u001b[0m - \u001b[1mClarification resolved with response: 1, 2, 3\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:50.277\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[37mportia.portia\u001b[0m:\u001b[37mwait_for_ready\u001b[0m:\u001b[37m477\u001b[0m - \u001b[1mRun prun-903a1d6a-ffbe-408a-9e81-853bdb094723 is ready to resume\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:51.692\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m528\u001b[0m - \u001b[1mPlan Run State is updated to PlanRunState.IN_PROGRESS. View in your Portia AI dashboard: https://app.portialabs.ai/dashboard/plan-runs?plan_run_id=prun-903a1d6a-ffbe-408a-9e81-853bdb094723\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:51.695\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 5: Select the topics from the extracted core concepts using the provided topic selection service.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:15:56.117\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Topic Selector Tool with args: {'raw_topics': ['Diffusion Models', '3D Generative Modeling', 'Super-resolution Networks', 'View Consistency', 'Implicit 3D Representation', 'Photometric Reconstruction Loss', 'Neural Rendering', 'Distillation Techniques', 'Voxel Grids and Trilinear Interpolation', 'Generative Adversarial Networks (GANs)'], 'selected_indices': '1, 2, 3'}\u001b[0m\n",
      "\u001b[32m2025-04-13 06:15:59.083\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - [\"Diffusion Models\", \"3D Generative Modeling\", \"Super-resolution Networks\"]\u001b[0m\n",
      "\u001b[32m2025-04-13 06:16:00.518\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 6: Create pages for the selected topics in the knowledge management tool.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n",
      "\u001b[32m2025-04-13 06:16:03.223\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;87mportia.tool_wrapper\u001b[0m:\u001b[38;5;87mrun\u001b[0m:\u001b[38;5;87m115\u001b[0m - \u001b[1mInvoking Notion Tool with args: {'topics': ['Diffusion Models', '3D Generative Modeling', 'Super-resolution Networks']}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error in Loguru Handler #2 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=350, microseconds=803638), 'exception': None, 'extra': {}, 'file': (name='portia.py', path='/opt/anaconda3/envs/RA/lib/python3.11/site-packages/portia/portia.py'), 'function': '_execute_plan_run', 'level': (name='INFO', no=20, icon='ℹ️'), 'line': 588, 'message': 'Step output - [{\"topic\": \"Diffusion Models\", \"page_id\": \"1d46ccbb-ecba-816d-b07f-ffa30847e7ff\", \"content\": \"[[Introduction]]\\\\nDiffusion models are a class of probabilistic models used to describe the way particles, information, or phenomena spread over time and space. These models are widely applicable in fields such as physics, biology, and finance, providing insights into processes like heat distribution, population dynamics, and market trends.\\\\n\\\\n[[Key Definitions]]\\\\n- **Diffusion Process**: A stochastic process that describes the random movement of particles in a medium, often modeled by differential equations.\\\\n- **Brownian Motion**: A continuous-time stochastic process that serves as a mathematical model for describing random motion, often used as the basis for diffusion models.\\\\n- **Fick\\'s Laws of Diffusion**: Fundamental principles that describe the diffusion process, including the relationship between diffusion flux and concentration gradient.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Fick\\'s First Law**\\\\n\\\\\\\\( J = -D \\\\\\\\frac{dC}{dx} \\\\\\\\)\\\\nwhere \\\\\\\\( J \\\\\\\\) is the diffusion flux, \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, and \\\\\\\\( \\\\\\\\frac{dC}{dx} \\\\\\\\) is the concentration gradient.\\\\n\\\\n- **Fick\\'s Second Law**\\\\n\\\\\\\\( \\\\\\\\frac{\\\\\\\\partial C}{\\\\\\\\partial t} = D \\\\\\\\frac{\\\\\\\\partial^2 C}{\\\\\\\\partial x^2} \\\\\\\\)\\\\nwhich describes how diffusion causes the concentration \\\\\\\\( C \\\\\\\\) to change over time.\\\\n\\\\n- **Einstein\\'s Relation**\\\\n\\\\\\\\( D = \\\\\\\\frac{k_B T}{\\\\\\\\gamma} \\\\\\\\)\\\\nwhere \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, \\\\\\\\( k_B \\\\\\\\) is Boltzmann\\'s constant, \\\\\\\\( T \\\\\\\\) is the absolute temperature, and \\\\\\\\( \\\\\\\\gamma \\\\\\\\) is the friction coefficient.\\\\n\\\\n[[Examples]]\\\\n1. **Heat Distribution in a Metal Rod**: Consider a metal rod heated at one end. Over time, the heat will diffuse along the rod, moving from regions of higher temperature to lower temperature, until thermal equilibrium is reached. This process can be described using diffusion models, specifically Fick\\'s laws.\\\\n\\\\n2. **Spread of a Perfume in a Room**: When perfume is sprayed in a room, the fragrance molecules diffuse through the air. Initially concentrated near the source, the molecules spread out over time, eventually filling the room evenly. This diffusion process can be modeled mathematically to predict how quickly the scent will spread.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do diffusion models help in understanding the spread of pollutants in the environment?\\\\n2. In what ways can diffusion models be applied to financial markets?\\\\n3. How does temperature affect the diffusion coefficient in Einstein\\'s relation?\\\\n4. What are the limitations of using Fick\\'s laws for modeling diffusion in complex systems?\\\\n5. How might diffusion models be used to simulate the spread of diseases in a population?\"}, {\"topic\": \"3D Generative Modeling\", \"page_id\": \"1d46ccbb-ecba-81a6-a796-e6a7d3e55e48\", \"content\": \"[[Introduction]]\\\\n3D Generative Modeling involves creating three-dimensional models using algorithms that can learn patterns from data. This process is widely used in fields such as computer graphics, virtual reality, and robotics. The goal is to generate realistic and complex 3D shapes or environments that can be used in various applications.\\\\n\\\\n[[Key Definitions]]\\\\n- **3D Model**: A representation of a three-dimensional object using a collection of points in 3D space, connected by various geometric entities such as triangles, lines, curved surfaces, etc.\\\\n- **Generative Adversarial Network (GAN)**: A class of machine learning frameworks where two neural networks contest with each other in a game. In the context of 3D modeling, GANs can be used to generate new 3D shapes.\\\\n- **Voxel**: A volumetric pixel, representing a value on a regular grid in three-dimensional space, often used in 3D modeling to represent the structure of a 3D object.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Loss Function for GANs**\\\\n  \\\\\\\\( \\\\\\\\min_G \\\\\\\\max_D V(D, G) = \\\\\\\\mathbb{E}_{x \\\\\\\\sim p_{\\\\\\\\text{data}}(x)}[\\\\\\\\log D(x)] + \\\\\\\\mathbb{E}_{z \\\\\\\\sim p_z(z)}[\\\\\\\\log(1 - D(G(z)))] \\\\\\\\)\\\\n\\\\n- **Transformation Matrix for 3D Rotation**\\\\n  \\\\\\\\( R = \\\\\\\\begin{bmatrix} \\\\\\\\cos \\\\\\\\theta & -\\\\\\\\sin \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ \\\\\\\\sin \\\\\\\\theta & \\\\\\\\cos \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ 0 & 0 & 1 \\\\\\\\end{bmatrix} \\\\\\\\)\\\\n\\\\n[[Examples]]\\\\n1. **Virtual Reality Environments**: In VR, 3D generative models are used to create immersive environments. For instance, a generative model can create a virtual forest with realistic trees and terrain, enhancing the user\\'s experience.\\\\n\\\\n2. **Robotics Simulation**: In robotics, 3D models are used to simulate environments where robots operate. A generative model can create different types of terrains and obstacles, allowing robots to be tested in varied conditions before real-world deployment.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do generative models differ from traditional 3D modeling techniques?\\\\n2. What are the advantages of using GANs in 3D modeling?\\\\n3. How can voxel representation be beneficial in 3D generative modeling?\\\\n4. What challenges might arise when using 3D generative models in real-time applications?\\\\n5. How does the transformation matrix help in manipulating 3D models?\"}, {\"topic\": \"Super-resolution Networks\", \"page_id\": \"1d46ccbb-ecba-8184-aa27-deb9a34c8862\", \"content\": \"[[Introduction]]\\\\nSuper-resolution networks are a class of deep learning models designed to enhance the resolution of images. These networks aim to reconstruct high-resolution images from their low-resolution counterparts by learning complex mappings between the two. This technology has significant applications in fields such as medical imaging, satellite imagery, and consumer electronics.\\\\n\\\\n[[Key Definitions]]\\\\n- **Super-resolution**: The process of enhancing the resolution of an image, increasing its pixel count and detail level.\\\\n- **Convolutional Neural Network (CNN)**: A type of deep learning model particularly effective for image processing tasks, including super-resolution.\\\\n- **Upsampling**: A technique used in image processing to increase the size of an image by adding additional pixels, often used in super-resolution networks.\\\\n\\\\n[[Relevant Formulas]]\\\\n**Mean Squared Error (MSE)**\\\\n\\\\\\\\( \\\\\\\\text{MSE} = \\\\\\\\frac{1}{n} \\\\\\\\sum_{i=1}^{n} (y_i - \\\\\\\\hat{y}_i)^2 \\\\\\\\)\\\\n\\\\n**Peak Signal-to-Noise Ratio (PSNR)**\\\\n\\\\\\\\( \\\\\\\\text{PSNR} = 20 \\\\\\\\cdot \\\\\\\\log_{10} \\\\\\\\left( \\\\\\\\frac{\\\\\\\\text{MAX}_I}{\\\\\\\\sqrt{\\\\\\\\text{MSE}}} \\\\\\\\right) \\\\\\\\)\\\\n\\\\n**Bicubic Interpolation Formula**\\\\n\\\\\\\\( f(x, y) = \\\\\\\\sum_{i=-1}^{2} \\\\\\\\sum_{j=-1}^{2} a_{ij} \\\\\\\\cdot B(u) \\\\\\\\cdot B(v) \\\\\\\\)\\\\nwhere \\\\\\\\(u = x - i\\\\\\\\) and \\\\\\\\(v = y - j\\\\\\\\), and \\\\\\\\(B\\\\\\\\) is the bicubic kernel function.\\\\n\\\\n[[Examples]]\\\\n1. **Medical Imaging**: In medical diagnostics, super-resolution networks can enhance MRI or CT scan images, allowing doctors to observe finer details that are crucial for accurate diagnosis and treatment planning.\\\\n2. **Satellite Imagery**: Super-resolution networks are used to improve the resolution of satellite images, providing clearer and more detailed views of Earth\\'s surface, which is essential for urban planning, agriculture, and environmental monitoring.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do super-resolution networks differ from traditional image upscaling methods like bicubic interpolation?\\\\n2. What are the potential ethical considerations when using super-resolution networks in surveillance?\\\\n3. How does the choice of loss function affect the performance of a super-resolution network?\\\\n4. In what ways can super-resolution networks be improved to handle real-time video processing?\\\\n5. What are the limitations of current super-resolution networks, and how might future research address these challenges?\"}]', 'module': 'portia', 'name': 'portia.portia', 'process': (id=30587, name='MainProcess'), 'thread': (id=8453638208, name='MainThread'), 'time': datetime(2025, 4, 13, 6, 16, 37, 560095, tzinfo=datetime.timezone(datetime.timedelta(seconds=3600), 'BST'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 164, in emit\n",
      "    _, precomputed_format = self._memoize_dynamic_format(dynamic_format, ansi_level)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 14, in prepare_colored_format\n",
      "    colored = Colorizer.prepare_format(format_)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 367, in prepare_format\n",
      "    tokens, messages_color_tokens = Colorizer._parse_without_formatting(string)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 455, in _parse_without_formatting\n",
      "    for literal_text, field_name, format_spec, conversion in formatter.parse(string):\n",
      "ValueError: Single '}' encountered in format string\n",
      "--- End of logging error ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 06:16:39.028\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 7: Find relevant videos for each topic based on the created pages.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error in Loguru Handler #2 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=394, microseconds=349615), 'exception': None, 'extra': {}, 'file': (name='tool_wrapper.py', path='/opt/anaconda3/envs/RA/lib/python3.11/site-packages/portia/tool_wrapper.py'), 'function': 'run', 'level': (name='INFO', no=20, icon='ℹ️'), 'line': 115, 'message': 'Invoking YouTube Tool with args: {\\'topics\\': [{\\'topic\\': \\'Diffusion Models\\', \\'page_id\\': \\'1d46ccbb-ecba-816d-b07f-ffa30847e7ff\\', \\'content\\': \"[[Introduction]]\\\\nDiffusion models are a class of probabilistic models used to describe the way particles, information, or phenomena spread over time and space. These models are widely applicable in fields such as physics, biology, and finance, providing insights into processes like heat distribution, population dynamics, and market trends.\\\\n\\\\n[[Key Definitions]]\\\\n- **Diffusion Process**: A stochastic process that describes the random movement of particles in a medium, often modeled by differential equations.\\\\n- **Brownian Motion**: A continuous-time stochastic process that serves as a mathematical model for describing random motion, often used as the basis for diffusion models.\\\\n- **Fick\\'s Laws of Diffusion**: Fundamental principles that describe the diffusion process, including the relationship between diffusion flux and concentration gradient.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Fick\\'s First Law**\\\\n\\\\\\\\( J = -D \\\\\\\\frac{dC}{dx} \\\\\\\\)\\\\nwhere \\\\\\\\( J \\\\\\\\) is the diffusion flux, \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, and \\\\\\\\( \\\\\\\\frac{dC}{dx} \\\\\\\\) is the concentration gradient.\\\\n\\\\n- **Fick\\'s Second Law**\\\\n\\\\\\\\( \\\\\\\\frac{\\\\\\\\partial C}{\\\\\\\\partial t} = D \\\\\\\\frac{\\\\\\\\partial^2 C}{\\\\\\\\partial x^2} \\\\\\\\)\\\\nwhich describes how diffusion causes the concentration \\\\\\\\( C \\\\\\\\) to change over time.\\\\n\\\\n- **Einstein\\'s Relation**\\\\n\\\\\\\\( D = \\\\\\\\frac{k_B T}{\\\\\\\\gamma} \\\\\\\\)\\\\nwhere \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, \\\\\\\\( k_B \\\\\\\\) is Boltzmann\\'s constant, \\\\\\\\( T \\\\\\\\) is the absolute temperature, and \\\\\\\\( \\\\\\\\gamma \\\\\\\\) is the friction coefficient.\\\\n\\\\n[[Examples]]\\\\n1. **Heat Distribution in a Metal Rod**: Consider a metal rod heated at one end. Over time, the heat will diffuse along the rod, moving from regions of higher temperature to lower temperature, until thermal equilibrium is reached. This process can be described using diffusion models, specifically Fick\\'s laws.\\\\n\\\\n2. **Spread of a Perfume in a Room**: When perfume is sprayed in a room, the fragrance molecules diffuse through the air. Initially concentrated near the source, the molecules spread out over time, eventually filling the room evenly. This diffusion process can be modeled mathematically to predict how quickly the scent will spread.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do diffusion models help in understanding the spread of pollutants in the environment?\\\\n2. In what ways can diffusion models be applied to financial markets?\\\\n3. How does temperature affect the diffusion coefficient in Einstein\\'s relation?\\\\n4. What are the limitations of using Fick\\'s laws for modeling diffusion in complex systems?\\\\n5. How might diffusion models be used to simulate the spread of diseases in a population?\"}, {\\'topic\\': \\'3D Generative Modeling\\', \\'page_id\\': \\'1d46ccbb-ecba-81a6-a796-e6a7d3e55e48\\', \\'content\\': \"[[Introduction]]\\\\n3D Generative Modeling involves creating three-dimensional models using algorithms that can learn patterns from data. This process is widely used in fields such as computer graphics, virtual reality, and robotics. The goal is to generate realistic and complex 3D shapes or environments that can be used in various applications.\\\\n\\\\n[[Key Definitions]]\\\\n- **3D Model**: A representation of a three-dimensional object using a collection of points in 3D space, connected by various geometric entities such as triangles, lines, curved surfaces, etc.\\\\n- **Generative Adversarial Network (GAN)**: A class of machine learning frameworks where two neural networks contest with each other in a game. In the context of 3D modeling, GANs can be used to generate new 3D shapes.\\\\n- **Voxel**: A volumetric pixel, representing a value on a regular grid in three-dimensional space, often used in 3D modeling to represent the structure of a 3D object.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Loss Function for GANs**\\\\n  \\\\\\\\( \\\\\\\\min_G \\\\\\\\max_D V(D, G) = \\\\\\\\mathbb{E}_{x \\\\\\\\sim p_{\\\\\\\\text{data}}(x)}[\\\\\\\\log D(x)] + \\\\\\\\mathbb{E}_{z \\\\\\\\sim p_z(z)}[\\\\\\\\log(1 - D(G(z)))] \\\\\\\\)\\\\n\\\\n- **Transformation Matrix for 3D Rotation**\\\\n  \\\\\\\\( R = \\\\\\\\begin{bmatrix} \\\\\\\\cos \\\\\\\\theta & -\\\\\\\\sin \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ \\\\\\\\sin \\\\\\\\theta & \\\\\\\\cos \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ 0 & 0 & 1 \\\\\\\\end{bmatrix} \\\\\\\\)\\\\n\\\\n[[Examples]]\\\\n1. **Virtual Reality Environments**: In VR, 3D generative models are used to create immersive environments. For instance, a generative model can create a virtual forest with realistic trees and terrain, enhancing the user\\'s experience.\\\\n\\\\n2. **Robotics Simulation**: In robotics, 3D models are used to simulate environments where robots operate. A generative model can create different types of terrains and obstacles, allowing robots to be tested in varied conditions before real-world deployment.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do generative models differ from traditional 3D modeling techniques?\\\\n2. What are the advantages of using GANs in 3D modeling?\\\\n3. How can voxel representation be beneficial in 3D generative modeling?\\\\n4. What challenges might arise when using 3D generative models in real-time applications?\\\\n5. How does the transformation matrix help in manipulating 3D models?\"}, {\\'topic\\': \\'Super-resolution Networks\\', \\'page_id\\': \\'1d46ccbb-ecba-8184-aa27-deb9a34c8862\\', \\'content\\': \"[[Introduction]]\\\\nSuper-resolution networks are a class of deep learning models designed to enhance the resolution of images. These networks aim to reconstruct high-resolution images from their low-resolution counterparts by learning complex mappings between the two. This technology has significant applications in fields such as medical imaging, satellite imagery, and consumer electronics.\\\\n\\\\n[[Key Definitions]]\\\\n- **Super-resolution**: The process of enhancing the resolution of an image, increasing its pixel count and detail level.\\\\n- **Convolutional Neural Network (CNN)**: A type of deep learning model particularly effective for image processing tasks, including super-resolution.\\\\n- **Upsampling**: A technique used in image processing to increase the size of an image by adding additional pixels, often used in super-resolution networks.\\\\n\\\\n[[Relevant Formulas]]\\\\n**Mean Squared Error (MSE)**\\\\n\\\\\\\\( \\\\\\\\text{MSE} = \\\\\\\\frac{1}{n} \\\\\\\\sum_{i=1}^{n} (y_i - \\\\\\\\hat{y}_i)^2 \\\\\\\\)\\\\n\\\\n**Peak Signal-to-Noise Ratio (PSNR)**\\\\n\\\\\\\\( \\\\\\\\text{PSNR} = 20 \\\\\\\\cdot \\\\\\\\log_{10} \\\\\\\\left( \\\\\\\\frac{\\\\\\\\text{MAX}_I}{\\\\\\\\sqrt{\\\\\\\\text{MSE}}} \\\\\\\\right) \\\\\\\\)\\\\n\\\\n**Bicubic Interpolation Formula**\\\\n\\\\\\\\( f(x, y) = \\\\\\\\sum_{i=-1}^{2} \\\\\\\\sum_{j=-1}^{2} a_{ij} \\\\\\\\cdot B(u) \\\\\\\\cdot B(v) \\\\\\\\)\\\\nwhere \\\\\\\\(u = x - i\\\\\\\\) and \\\\\\\\(v = y - j\\\\\\\\), and \\\\\\\\(B\\\\\\\\) is the bicubic kernel function.\\\\n\\\\n[[Examples]]\\\\n1. **Medical Imaging**: In medical diagnostics, super-resolution networks can enhance MRI or CT scan images, allowing doctors to observe finer details that are crucial for accurate diagnosis and treatment planning.\\\\n2. **Satellite Imagery**: Super-resolution networks are used to improve the resolution of satellite images, providing clearer and more detailed views of Earth\\'s surface, which is essential for urban planning, agriculture, and environmental monitoring.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do super-resolution networks differ from traditional image upscaling methods like bicubic interpolation?\\\\n2. What are the potential ethical considerations when using super-resolution networks in surveillance?\\\\n3. How does the choice of loss function affect the performance of a super-resolution network?\\\\n4. In what ways can super-resolution networks be improved to handle real-time video processing?\\\\n5. What are the limitations of current super-resolution networks, and how might future research address these challenges?\"}]}', 'module': 'tool_wrapper', 'name': 'portia.tool_wrapper', 'process': (id=30587, name='MainProcess'), 'thread': (id=14112813056, name='ThreadPoolExecutor-19_0'), 'time': datetime(2025, 4, 13, 6, 17, 21, 106072, tzinfo=datetime.timezone(datetime.timedelta(seconds=3600), 'BST'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 164, in emit\n",
      "    _, precomputed_format = self._memoize_dynamic_format(dynamic_format, ansi_level)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 14, in prepare_colored_format\n",
      "    colored = Colorizer.prepare_format(format_)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 367, in prepare_format\n",
      "    tokens, messages_color_tokens = Colorizer._parse_without_formatting(string)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 455, in _parse_without_formatting\n",
      "    for literal_text, field_name, format_spec, conversion in formatter.parse(string):\n",
      "ValueError: Single '}' encountered in format string\n",
      "--- End of logging error ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 06:17:27.851\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - \u001b[0m\n",
      "\u001b[32m2025-04-13 06:17:29.411\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 8: Identify recommended resources for each topic linked to the created pages.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error in Loguru Handler #2 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=434, microseconds=498893), 'exception': None, 'extra': {}, 'file': (name='tool_wrapper.py', path='/opt/anaconda3/envs/RA/lib/python3.11/site-packages/portia/tool_wrapper.py'), 'function': 'run', 'level': (name='INFO', no=20, icon='ℹ️'), 'line': 115, 'message': 'Invoking RecRead Tool with args: {\\'topics\\': [{\\'topic\\': \\'Diffusion Models\\', \\'page_id\\': \\'1d46ccbb-ecba-816d-b07f-ffa30847e7ff\\', \\'content\\': \"[[Introduction]]\\\\nDiffusion models are a class of probabilistic models used to describe the way particles, information, or phenomena spread over time and space. These models are widely applicable in fields such as physics, biology, and finance, providing insights into processes like heat distribution, population dynamics, and market trends.\\\\n\\\\n[[Key Definitions]]\\\\n- **Diffusion Process**: A stochastic process that describes the random movement of particles in a medium, often modeled by differential equations.\\\\n- **Brownian Motion**: A continuous-time stochastic process that serves as a mathematical model for describing random motion, often used as the basis for diffusion models.\\\\n- **Fick\\'s Laws of Diffusion**: Fundamental principles that describe the diffusion process, including the relationship between diffusion flux and concentration gradient.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Fick\\'s First Law**\\\\n\\\\\\\\( J = -D \\\\\\\\frac{dC}{dx} \\\\\\\\)\\\\nwhere \\\\\\\\( J \\\\\\\\) is the diffusion flux, \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, and \\\\\\\\( \\\\\\\\frac{dC}{dx} \\\\\\\\) is the concentration gradient.\\\\n\\\\n- **Fick\\'s Second Law**\\\\n\\\\\\\\( \\\\\\\\frac{\\\\\\\\partial C}{\\\\\\\\partial t} = D \\\\\\\\frac{\\\\\\\\partial^2 C}{\\\\\\\\partial x^2} \\\\\\\\)\\\\nwhich describes how diffusion causes the concentration \\\\\\\\( C \\\\\\\\) to change over time.\\\\n\\\\n- **Einstein\\'s Relation**\\\\n\\\\\\\\( D = \\\\\\\\frac{k_B T}{\\\\\\\\gamma} \\\\\\\\)\\\\nwhere \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, \\\\\\\\( k_B \\\\\\\\) is Boltzmann\\'s constant, \\\\\\\\( T \\\\\\\\) is the absolute temperature, and \\\\\\\\( \\\\\\\\gamma \\\\\\\\) is the friction coefficient.\\\\n\\\\n[[Examples]]\\\\n1. **Heat Distribution in a Metal Rod**: Consider a metal rod heated at one end. Over time, the heat will diffuse along the rod, moving from regions of higher temperature to lower temperature, until thermal equilibrium is reached. This process can be described using diffusion models, specifically Fick\\'s laws.\\\\n\\\\n2. **Spread of a Perfume in a Room**: When perfume is sprayed in a room, the fragrance molecules diffuse through the air. Initially concentrated near the source, the molecules spread out over time, eventually filling the room evenly. This diffusion process can be modeled mathematically to predict how quickly the scent will spread.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do diffusion models help in understanding the spread of pollutants in the environment?\\\\n2. In what ways can diffusion models be applied to financial markets?\\\\n3. How does temperature affect the diffusion coefficient in Einstein\\'s relation?\\\\n4. What are the limitations of using Fick\\'s laws for modeling diffusion in complex systems?\\\\n5. How might diffusion models be used to simulate the spread of diseases in a population?\"}, {\\'topic\\': \\'3D Generative Modeling\\', \\'page_id\\': \\'1d46ccbb-ecba-81a6-a796-e6a7d3e55e48\\', \\'content\\': \"[[Introduction]]\\\\n3D Generative Modeling involves creating three-dimensional models using algorithms that can learn patterns from data. This process is widely used in fields such as computer graphics, virtual reality, and robotics. The goal is to generate realistic and complex 3D shapes or environments that can be used in various applications.\\\\n\\\\n[[Key Definitions]]\\\\n- **3D Model**: A representation of a three-dimensional object using a collection of points in 3D space, connected by various geometric entities such as triangles, lines, curved surfaces, etc.\\\\n- **Generative Adversarial Network (GAN)**: A class of machine learning frameworks where two neural networks contest with each other in a game. In the context of 3D modeling, GANs can be used to generate new 3D shapes.\\\\n- **Voxel**: A volumetric pixel, representing a value on a regular grid in three-dimensional space, often used in 3D modeling to represent the structure of a 3D object.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Loss Function for GANs**\\\\n  \\\\\\\\( \\\\\\\\min_G \\\\\\\\max_D V(D, G) = \\\\\\\\mathbb{E}_{x \\\\\\\\sim p_{\\\\\\\\text{data}}(x)}[\\\\\\\\log D(x)] + \\\\\\\\mathbb{E}_{z \\\\\\\\sim p_z(z)}[\\\\\\\\log(1 - D(G(z)))] \\\\\\\\)\\\\n\\\\n- **Transformation Matrix for 3D Rotation**\\\\n  \\\\\\\\( R = \\\\\\\\begin{bmatrix} \\\\\\\\cos \\\\\\\\theta & -\\\\\\\\sin \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ \\\\\\\\sin \\\\\\\\theta & \\\\\\\\cos \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ 0 & 0 & 1 \\\\\\\\end{bmatrix} \\\\\\\\)\\\\n\\\\n[[Examples]]\\\\n1. **Virtual Reality Environments**: In VR, 3D generative models are used to create immersive environments. For instance, a generative model can create a virtual forest with realistic trees and terrain, enhancing the user\\'s experience.\\\\n\\\\n2. **Robotics Simulation**: In robotics, 3D models are used to simulate environments where robots operate. A generative model can create different types of terrains and obstacles, allowing robots to be tested in varied conditions before real-world deployment.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do generative models differ from traditional 3D modeling techniques?\\\\n2. What are the advantages of using GANs in 3D modeling?\\\\n3. How can voxel representation be beneficial in 3D generative modeling?\\\\n4. What challenges might arise when using 3D generative models in real-time applications?\\\\n5. How does the transformation matrix help in manipulating 3D models?\"}, {\\'topic\\': \\'Super-resolution Networks\\', \\'page_id\\': \\'1d46ccbb-ecba-8184-aa27-deb9a34c8862\\', \\'content\\': \"[[Introduction]]\\\\nSuper-resolution networks are a class of deep learning models designed to enhance the resolution of images. These networks aim to reconstruct high-resolution images from their low-resolution counterparts by learning complex mappings between the two. This technology has significant applications in fields such as medical imaging, satellite imagery, and consumer electronics.\\\\n\\\\n[[Key Definitions]]\\\\n- **Super-resolution**: The process of enhancing the resolution of an image, increasing its pixel count and detail level.\\\\n- **Convolutional Neural Network (CNN)**: A type of deep learning model particularly effective for image processing tasks, including super-resolution.\\\\n- **Upsampling**: A technique used in image processing to increase the size of an image by adding additional pixels, often used in super-resolution networks.\\\\n\\\\n[[Relevant Formulas]]\\\\n**Mean Squared Error (MSE)**\\\\n\\\\\\\\( \\\\\\\\text{MSE} = \\\\\\\\frac{1}{n} \\\\\\\\sum_{i=1}^{n} (y_i - \\\\\\\\hat{y}_i)^2 \\\\\\\\)\\\\n\\\\n**Peak Signal-to-Noise Ratio (PSNR)**\\\\n\\\\\\\\( \\\\\\\\text{PSNR} = 20 \\\\\\\\cdot \\\\\\\\log_{10} \\\\\\\\left( \\\\\\\\frac{\\\\\\\\text{MAX}_I}{\\\\\\\\sqrt{\\\\\\\\text{MSE}}} \\\\\\\\right) \\\\\\\\)\\\\n\\\\n**Bicubic Interpolation Formula**\\\\n\\\\\\\\( f(x, y) = \\\\\\\\sum_{i=-1}^{2} \\\\\\\\sum_{j=-1}^{2} a_{ij} \\\\\\\\cdot B(u) \\\\\\\\cdot B(v) \\\\\\\\)\\\\nwhere \\\\\\\\(u = x - i\\\\\\\\) and \\\\\\\\(v = y - j\\\\\\\\), and \\\\\\\\(B\\\\\\\\) is the bicubic kernel function.\\\\n\\\\n[[Examples]]\\\\n1. **Medical Imaging**: In medical diagnostics, super-resolution networks can enhance MRI or CT scan images, allowing doctors to observe finer details that are crucial for accurate diagnosis and treatment planning.\\\\n2. **Satellite Imagery**: Super-resolution networks are used to improve the resolution of satellite images, providing clearer and more detailed views of Earth\\'s surface, which is essential for urban planning, agriculture, and environmental monitoring.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do super-resolution networks differ from traditional image upscaling methods like bicubic interpolation?\\\\n2. What are the potential ethical considerations when using super-resolution networks in surveillance?\\\\n3. How does the choice of loss function affect the performance of a super-resolution network?\\\\n4. In what ways can super-resolution networks be improved to handle real-time video processing?\\\\n5. What are the limitations of current super-resolution networks, and how might future research address these challenges?\"}]}', 'module': 'tool_wrapper', 'name': 'portia.tool_wrapper', 'process': (id=30587, name='MainProcess'), 'thread': (id=14112813056, name='ThreadPoolExecutor-21_0'), 'time': datetime(2025, 4, 13, 6, 18, 1, 255350, tzinfo=datetime.timezone(datetime.timedelta(seconds=3600), 'BST'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 164, in emit\n",
      "    _, precomputed_format = self._memoize_dynamic_format(dynamic_format, ansi_level)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 14, in prepare_colored_format\n",
      "    colored = Colorizer.prepare_format(format_)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 367, in prepare_format\n",
      "    tokens, messages_color_tokens = Colorizer._parse_without_formatting(string)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 455, in _parse_without_formatting\n",
      "    for literal_text, field_name, format_spec, conversion in formatter.parse(string):\n",
      "ValueError: Single '}' encountered in format string\n",
      "--- End of logging error ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: Diffusion Models\n",
      "Processing topic: 3D Generative Modeling\n",
      "Processing topic: Super-resolution Networks\n",
      "\u001b[32m2025-04-13 06:18:09.759\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - ✅ Recommended Reading added to Notion pages successfully.\u001b[0m\n",
      "\u001b[32m2025-04-13 06:18:11.521\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m551\u001b[0m - \u001b[1mExecuting step 9: Generate quizzes for each topic based on the created pages.\u001b[0m | {'plan': 'plan-9a8e30db-a7b9-4912-8527-8ce10756caf1', 'plan_run': 'prun-903a1d6a-ffbe-408a-9e81-853bdb094723'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error in Loguru Handler #2 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=476, microseconds=482524), 'exception': None, 'extra': {}, 'file': (name='tool_wrapper.py', path='/opt/anaconda3/envs/RA/lib/python3.11/site-packages/portia/tool_wrapper.py'), 'function': 'run', 'level': (name='INFO', no=20, icon='ℹ️'), 'line': 115, 'message': 'Invoking Quiz Tool with args: {\\'topics\\': [{\\'topic\\': \\'Diffusion Models\\', \\'page_id\\': \\'1d46ccbb-ecba-816d-b07f-ffa30847e7ff\\', \\'content\\': \"[[Introduction]]\\\\nDiffusion models are a class of probabilistic models used to describe the way particles, information, or phenomena spread over time and space. These models are widely applicable in fields such as physics, biology, and finance, providing insights into processes like heat distribution, population dynamics, and market trends.\\\\n\\\\n[[Key Definitions]]\\\\n- **Diffusion Process**: A stochastic process that describes the random movement of particles in a medium, often modeled by differential equations.\\\\n- **Brownian Motion**: A continuous-time stochastic process that serves as a mathematical model for describing random motion, often used as the basis for diffusion models.\\\\n- **Fick\\'s Laws of Diffusion**: Fundamental principles that describe the diffusion process, including the relationship between diffusion flux and concentration gradient.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Fick\\'s First Law**\\\\n\\\\\\\\( J = -D \\\\\\\\frac{dC}{dx} \\\\\\\\)\\\\nwhere \\\\\\\\( J \\\\\\\\) is the diffusion flux, \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, and \\\\\\\\( \\\\\\\\frac{dC}{dx} \\\\\\\\) is the concentration gradient.\\\\n\\\\n- **Fick\\'s Second Law**\\\\n\\\\\\\\( \\\\\\\\frac{\\\\\\\\partial C}{\\\\\\\\partial t} = D \\\\\\\\frac{\\\\\\\\partial^2 C}{\\\\\\\\partial x^2} \\\\\\\\)\\\\nwhich describes how diffusion causes the concentration \\\\\\\\( C \\\\\\\\) to change over time.\\\\n\\\\n- **Einstein\\'s Relation**\\\\n\\\\\\\\( D = \\\\\\\\frac{k_B T}{\\\\\\\\gamma} \\\\\\\\)\\\\nwhere \\\\\\\\( D \\\\\\\\) is the diffusion coefficient, \\\\\\\\( k_B \\\\\\\\) is Boltzmann\\'s constant, \\\\\\\\( T \\\\\\\\) is the absolute temperature, and \\\\\\\\( \\\\\\\\gamma \\\\\\\\) is the friction coefficient.\\\\n\\\\n[[Examples]]\\\\n1. **Heat Distribution in a Metal Rod**: Consider a metal rod heated at one end. Over time, the heat will diffuse along the rod, moving from regions of higher temperature to lower temperature, until thermal equilibrium is reached. This process can be described using diffusion models, specifically Fick\\'s laws.\\\\n\\\\n2. **Spread of a Perfume in a Room**: When perfume is sprayed in a room, the fragrance molecules diffuse through the air. Initially concentrated near the source, the molecules spread out over time, eventually filling the room evenly. This diffusion process can be modeled mathematically to predict how quickly the scent will spread.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do diffusion models help in understanding the spread of pollutants in the environment?\\\\n2. In what ways can diffusion models be applied to financial markets?\\\\n3. How does temperature affect the diffusion coefficient in Einstein\\'s relation?\\\\n4. What are the limitations of using Fick\\'s laws for modeling diffusion in complex systems?\\\\n5. How might diffusion models be used to simulate the spread of diseases in a population?\"}, {\\'topic\\': \\'3D Generative Modeling\\', \\'page_id\\': \\'1d46ccbb-ecba-81a6-a796-e6a7d3e55e48\\', \\'content\\': \"[[Introduction]]\\\\n3D Generative Modeling involves creating three-dimensional models using algorithms that can learn patterns from data. This process is widely used in fields such as computer graphics, virtual reality, and robotics. The goal is to generate realistic and complex 3D shapes or environments that can be used in various applications.\\\\n\\\\n[[Key Definitions]]\\\\n- **3D Model**: A representation of a three-dimensional object using a collection of points in 3D space, connected by various geometric entities such as triangles, lines, curved surfaces, etc.\\\\n- **Generative Adversarial Network (GAN)**: A class of machine learning frameworks where two neural networks contest with each other in a game. In the context of 3D modeling, GANs can be used to generate new 3D shapes.\\\\n- **Voxel**: A volumetric pixel, representing a value on a regular grid in three-dimensional space, often used in 3D modeling to represent the structure of a 3D object.\\\\n\\\\n[[Relevant Formulas]]\\\\n- **Loss Function for GANs**\\\\n  \\\\\\\\( \\\\\\\\min_G \\\\\\\\max_D V(D, G) = \\\\\\\\mathbb{E}_{x \\\\\\\\sim p_{\\\\\\\\text{data}}(x)}[\\\\\\\\log D(x)] + \\\\\\\\mathbb{E}_{z \\\\\\\\sim p_z(z)}[\\\\\\\\log(1 - D(G(z)))] \\\\\\\\)\\\\n\\\\n- **Transformation Matrix for 3D Rotation**\\\\n  \\\\\\\\( R = \\\\\\\\begin{bmatrix} \\\\\\\\cos \\\\\\\\theta & -\\\\\\\\sin \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ \\\\\\\\sin \\\\\\\\theta & \\\\\\\\cos \\\\\\\\theta & 0 \\\\\\\\\\\\\\\\ 0 & 0 & 1 \\\\\\\\end{bmatrix} \\\\\\\\)\\\\n\\\\n[[Examples]]\\\\n1. **Virtual Reality Environments**: In VR, 3D generative models are used to create immersive environments. For instance, a generative model can create a virtual forest with realistic trees and terrain, enhancing the user\\'s experience.\\\\n\\\\n2. **Robotics Simulation**: In robotics, 3D models are used to simulate environments where robots operate. A generative model can create different types of terrains and obstacles, allowing robots to be tested in varied conditions before real-world deployment.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do generative models differ from traditional 3D modeling techniques?\\\\n2. What are the advantages of using GANs in 3D modeling?\\\\n3. How can voxel representation be beneficial in 3D generative modeling?\\\\n4. What challenges might arise when using 3D generative models in real-time applications?\\\\n5. How does the transformation matrix help in manipulating 3D models?\"}, {\\'topic\\': \\'Super-resolution Networks\\', \\'page_id\\': \\'1d46ccbb-ecba-8184-aa27-deb9a34c8862\\', \\'content\\': \"[[Introduction]]\\\\nSuper-resolution networks are a class of deep learning models designed to enhance the resolution of images. These networks aim to reconstruct high-resolution images from their low-resolution counterparts by learning complex mappings between the two. This technology has significant applications in fields such as medical imaging, satellite imagery, and consumer electronics.\\\\n\\\\n[[Key Definitions]]\\\\n- **Super-resolution**: The process of enhancing the resolution of an image, increasing its pixel count and detail level.\\\\n- **Convolutional Neural Network (CNN)**: A type of deep learning model particularly effective for image processing tasks, including super-resolution.\\\\n- **Upsampling**: A technique used in image processing to increase the size of an image by adding additional pixels, often used in super-resolution networks.\\\\n\\\\n[[Relevant Formulas]]\\\\n**Mean Squared Error (MSE)**\\\\n\\\\\\\\( \\\\\\\\text{MSE} = \\\\\\\\frac{1}{n} \\\\\\\\sum_{i=1}^{n} (y_i - \\\\\\\\hat{y}_i)^2 \\\\\\\\)\\\\n\\\\n**Peak Signal-to-Noise Ratio (PSNR)**\\\\n\\\\\\\\( \\\\\\\\text{PSNR} = 20 \\\\\\\\cdot \\\\\\\\log_{10} \\\\\\\\left( \\\\\\\\frac{\\\\\\\\text{MAX}_I}{\\\\\\\\sqrt{\\\\\\\\text{MSE}}} \\\\\\\\right) \\\\\\\\)\\\\n\\\\n**Bicubic Interpolation Formula**\\\\n\\\\\\\\( f(x, y) = \\\\\\\\sum_{i=-1}^{2} \\\\\\\\sum_{j=-1}^{2} a_{ij} \\\\\\\\cdot B(u) \\\\\\\\cdot B(v) \\\\\\\\)\\\\nwhere \\\\\\\\(u = x - i\\\\\\\\) and \\\\\\\\(v = y - j\\\\\\\\), and \\\\\\\\(B\\\\\\\\) is the bicubic kernel function.\\\\n\\\\n[[Examples]]\\\\n1. **Medical Imaging**: In medical diagnostics, super-resolution networks can enhance MRI or CT scan images, allowing doctors to observe finer details that are crucial for accurate diagnosis and treatment planning.\\\\n2. **Satellite Imagery**: Super-resolution networks are used to improve the resolution of satellite images, providing clearer and more detailed views of Earth\\'s surface, which is essential for urban planning, agriculture, and environmental monitoring.\\\\n\\\\n[[Reflective Questions]]\\\\n1. How do super-resolution networks differ from traditional image upscaling methods like bicubic interpolation?\\\\n2. What are the potential ethical considerations when using super-resolution networks in surveillance?\\\\n3. How does the choice of loss function affect the performance of a super-resolution network?\\\\n4. In what ways can super-resolution networks be improved to handle real-time video processing?\\\\n5. What are the limitations of current super-resolution networks, and how might future research address these challenges?\"}]}', 'module': 'tool_wrapper', 'name': 'portia.tool_wrapper', 'process': (id=30587, name='MainProcess'), 'thread': (id=14112813056, name='ThreadPoolExecutor-23_0'), 'time': datetime(2025, 4, 13, 6, 18, 43, 238981, tzinfo=datetime.timezone(datetime.timedelta(seconds=3600), 'BST'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 164, in emit\n",
      "    _, precomputed_format = self._memoize_dynamic_format(dynamic_format, ansi_level)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_handler.py\", line 14, in prepare_colored_format\n",
      "    colored = Colorizer.prepare_format(format_)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 367, in prepare_format\n",
      "    tokens, messages_color_tokens = Colorizer._parse_without_formatting(string)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/RA/lib/python3.11/site-packages/loguru/_colorizer.py\", line 455, in _parse_without_formatting\n",
      "    for literal_text, field_name, format_spec, conversion in formatter.parse(string):\n",
      "ValueError: Single '}' encountered in format string\n",
      "--- End of logging error ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 06:19:14.577\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[38;5;129mportia.portia\u001b[0m:\u001b[38;5;129m_execute_plan_run\u001b[0m:\u001b[38;5;129m588\u001b[0m - \u001b[1mStep output - Quizzes created successfully!\u001b[0m\n",
      "\u001b[32m2025-04-13 06:19:20.106\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[37mportia.portia\u001b[0m:\u001b[37m_log_final_output\u001b[0m:\u001b[37m618\u001b[0m - \u001b[1mFinal output: The research assistant executed a series of tasks to explore the topic of Holodiffusion, focusing on the paper \"HoloFusion: Towards Photo-realistic 3D Generative Modeling.\" The tasks included downloading the paper, extracting its full text, and identifying core mathematical and scientific concepts such as Diffusion Models, 3D Generative Modeling, and Super-resolution Networks. These topics were selected for further exploration, with Notion pages created and populated with content. Recommended readings and quizzes were also generated, although no videos were found. The process emphasized the integration of super-resolution networks and view consistency in 3D modeling.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "constraints = []\n",
    "# topic = input(\"What topic are you interested in covering today?\")\n",
    "# number_of_papers = int(input(\"How many papers do you want to download?\"))\n",
    "topic = \"Holodiffusion\"\n",
    "video_preference = True\n",
    "rec_reading_preference = True\n",
    "quiz_preference = True\n",
    "\n",
    "# Define the path to your papers folder\n",
    "papers_folder = \"papers\"\n",
    "\n",
    "# Ensure the papers folder exists\n",
    "if not os.path.exists(papers_folder):\n",
    "    os.makedirs(papers_folder)\n",
    "else:\n",
    "    # If it exists, clear its contents\n",
    "    for filename in os.listdir(papers_folder):\n",
    "        file_path = os.path.join(papers_folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # remove file or symlink\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # remove directory\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "\n",
    "task = (\n",
    "            lambda : f\"\"\"You are a research assistant running these tasks: \n",
    "                      - Find and download 1 paper on the topic of {topic} using the ArXivTool. \n",
    "                      - Run the PDFReaderTool to extract the full text from the pdfs in the local folder.\n",
    "                      - Use PSTool to create and populate the Page Summary subpage.\n",
    "                      - From the text, extract the core mathematical and scientific concepts required \n",
    "                        to understand the paper. Focus only on generalizable topics that could be included \n",
    "                        in a learning pathway or curriculum—avoid content specific to the study's location, \n",
    "                        data, or outcomes. List only the overarching topics, with no explanations or extra text.\n",
    "                      - Then use the TopicSelectorTool on these topics. \n",
    "                      - Then use the Notion Tool to create Notion pages for these topics.\n",
    "\n",
    "                      - {video_preference * \"Use the YouTubeTool to find videos on each topic.\"}\n",
    "                      - {rec_reading_preference * \"Use the RecReadTool to find resources on each topic.\"}\n",
    "                      - {quiz_preference * \"Use the QuizTool to create quizzes on each topic.\"}\n",
    "                      \n",
    "                      \n",
    "                        Take into account these constraints: {constraints}\n",
    "                      \"\"\"\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# Iterate on the plan with the user until they are happy with it\n",
    "with execution_context(end_user_id=\"learning_enthusiast\",):\n",
    "    plan = portia.plan(task())\n",
    "    print(\"\\nHere are the steps in the generated plan:\")\n",
    "    [print(step.model_dump_json(indent=2)) for step in plan.steps]\n",
    "    ready_to_proceed = False\n",
    "    while not ready_to_proceed:\n",
    "        user_input = input(\"Are you happy with the plan? (y/n):\\n\")\n",
    "        if user_input == \"y\":\n",
    "            ready_to_proceed = True\n",
    "        else:\n",
    "            user_input = input(\"Any additional guidance for the planner?:\\n\")\n",
    "            constraints.append(user_input)\n",
    "            plan = portia.plan(task())\n",
    "            print(\"\\nHere are the updated steps in the plan:\")\n",
    "            [print(step.model_dump_json(indent=2)) for step in plan.steps]\n",
    "\n",
    "    # Execute the plan\n",
    "    print(\"\\nThe plan will now be executed. Please wait...\")\n",
    "    run = portia.run_plan(plan)\n",
    "    \n",
    "    if run.state != PlanRunState.COMPLETE:\n",
    "        raise Exception(\n",
    "            f\"Plan run failed with state {run.state}. Check logs for details.\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
